#!/usr/bin/python
#/******************************************************************************
# *                   llsf.py
# *
# * This file is part of the Castor project.
# * See http://castor.web.cern.ch/castor
# *
# * Copyright (C) 2003  CERN
# * This program is free software; you can redistribute it and/or
# * modify it under the terms of the GNU General Public License
# * as published by the Free Software Foundation; either version 2
# * of the License, or (at your option) any later version.
# * This program is distributed in the hope that it will be useful,
# * but WITHOUT ANY WARRANTY; without even the implied warranty of
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# * GNU General Public License for more details.
# * You should have received a copy of the GNU General Public License
# * along with this program; if not, write to the Free Software
# * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
# *
# * @(#)$RCSfile: castor_tools.py,v $ $Revision: 1.9 $ $Release$ $Date: 2009/03/23 15:47:41 $ $Author: sponcec3 $
# *
# * low latency scheduling facility of the CASTOR project
# *
# * @author Castor Dev team, castor-dev@cern.ch
# *****************************************************************************/

import sys
import getopt
import rpyc
import time
import threading
import socket
import daemon
import castor_tools, connectionpool, dispatcher, serverqueue, aborter, synchronizer
import dlf
from llsfddlf import msgs

# usage function
def usage(exitCode):
  print 'Usage : ' + sys.argv[0] + ' [-h|--help] [-f|--foreground] [-n|--nbworkers <nb worker threads>]'
  sys.exit(exitCode)

# first parse the options
daemonize = True
nbWorkers = 5
try:
  options, args = getopt.getopt(sys.argv[1:], 'hvfn:', ['help', 'foreground', 'nbworkers'])
except Exception, e:
  print e
  usage(1)
for f, v in options:
  if f == '-h' or f == '--help':
    usage(0)
  elif f == '-f' or f == '--foreground':
    daemonize = False
  elif f == '-n' or f == '--nbworkers':
    nbWorkers = int(v)
  else:
    print "unknown option : " + f
    usage(1)

# If any arg, complain and stop
if len(args) != 0:
  print "Unknown arguments : " + ' '.join(args) + "\n"
  usage(1)

class DiskServerListCache:
  '''cache for the list of diskservers, organized by service class.'''

  # class variable
  _diskServerList = None

  def _rebuild(self):
    '''rebuild the cache of diskservers/diskpools'''
    # query the stager database
    stconn = castor_tools.connectToStager()
    stcur = stconn.cursor()
    stcur.arraysize = 50
    stDiskServers = '''SELECT UNIQUE DiskServer.name, DiskPool.name
                         FROM FileSystem, DiskServer, DiskPool
                        WHERE FileSystem.diskServer = DiskServer.id
                          AND FileSystem.diskPool = DiskPool.id'''
    stcur.execute(stDiskServers)
    rows = stcur.fetchall()
      # build up the list
    self._diskServerList = {}
    self._diskServerSet = set()
    for diskserver, diskPool in rows:
      if diskPool not in self._diskServerList : self._diskServerList[diskPool] = []
      self._diskServerList[diskPool].append(diskserver)
      self._diskServerSet.add(diskserver)
    castor_tools.disconnectDB(stconn)

  def getlist(self):
    '''returns the list of diskservers from the cache, clustered by diskpool.
    Builds this list form the DB if needed'''
    if self._diskServerList == None: self._rebuild()
    return self._diskServerList

  def getset(self):
    '''returns the set of diskservers from the cache, or builds it from the DB if needed'''
    if self._diskServerSet == None: self._rebuild()
    return self._diskServerSet

  def clear(self):
    '''clears the cache'''
    self._diskServerList = None
    self._diskServerSet = None

class SchedulerService(rpyc.Service):
  '''This service is responsible for answering all requests made to the scheduler.
  There are mainly 2 kinds : monitoring requests and callbacks from the diskservers'''

  def dispatch(self, func, diskPool):
    '''gather the results of the call to func for all diskServers'''
    res = {}
    dslist = diskServerList.getlist()
    if diskPool != None: dslist = {diskPool : dslist[diskPool]}
    for diskPool in dslist:
      res[diskPool] = {}
      for ds in dslist[diskPool]:
        try:
          # call the function on the appropriate diskserver
          res[diskPool][ds] = getattr(connections,func)(ds)
        except Exception, e:
          # 'Could not contact diskserver' message
          dlf.writenotice(msgs.COULDNOTCONTACTDS, function=func, diskserver=ds,
                          errortype=str(e.__class__), error=str(e))
          pass
    return res

  def exposed_bqueues(self, diskPool=None):
    '''bqueues lists the number of running and pending jobs per queue'''
    dlf.writedebug(msgs.BQUEUESCALLED, diskpool=diskPool)
    nbs = self.dispatch('nbJobs', diskPool)
    res = []
    for diskPool in nbs:
      if len(nbs[diskPool]) > 0:
        res.append(tuple([diskPool] + map(sum,zip(*nbs[diskPool].values()))))
      else:
        res.append((diskPool,0,0,0))
    return tuple(res)

  def exposed_bhosts(self, diskPool=None):
    '''bhosts lists the hosts in the scheduling facility together with the number of
    running and pending jobs for each of them and their number of slots'''
    dlf.writedebug(msgs.BHOSTSCALLED, diskpool=diskPool)
    # get the raw data as a dictionnary of dictionnaries (svclass then diskserver level)
    hosts = self.dispatch('nbJobs', diskPool)
    # flatten it to a list of (diskserver, (values))
    hostslist = reduce(lambda x,y:x+y, [diskserver.items() for diskserver in hosts.values()])
    # return a tuple of (diskserver, value1, value2, ...)
    return tuple(tuple([host[0]]+list(host[1])) for host in hostslist)

  def exposed_badmin(self, diskPool=None):
    '''badmin reconfigures the scheduling facility'''
    dlf.writedebug(msgs.BADMINCALLED, diskpool=diskPool)
    # reset all known diskservers
    self.dispatch('badmin', diskPool)
    # and reset list of known diskServers
    diskServerList.clear()

  def exposed_bjobs(self, diskPool=None):
    '''bqueues lists the number of running and pending jobs per queue'''
    dlf.writedebug(msgs.BJOBSCALLED, diskpool=diskPool)
    jobs = self.dispatch('bjobs', diskPool)
    res = []
    for diskPool in jobs:
      for ds in jobs[diskPool]:
        for jobid, scheduler, user, status, jobtype, arrivalTime, startTime in jobs[diskPool][ds]:
          res.append((jobid, scheduler, user, status, diskPool, ds, jobtype, arrivalTime, startTime))
    return tuple(res)

  def exposed_bkill(self, jobids):
    '''bkill removes jobs from all queues on all servers and diskservers'''
    for jobid in jobids:
      dlf.writedebug(msgs.BKILLCALLED, subreqid=jobid)
    # call the internal method on all schedulers (including ourselves)
    for scheduler in configuration['DiskManager']['ServerHosts'].split():
      connections.bkillinternal(scheduler, jobids)

  def exposed_bkillinternal(self, jobids):
    '''bkillinternal removes jobs from the queues handled by this scheduler (server queue and diskserver's)'''
    for jobid in jobids:
      dlf.writedebug(msgs.BKILLINTERNALCALLED, subreqid=jobid)
    queuingJobs.remove(jobids)

  def exposed_jobsKilled(self, jobs):
    '''informs the stager that these jobs have been killed'''
    for jobid, fileid, errno, errmsg in jobs:
      dlf.writedebug(msgs.JOBSKILLEDCALLED, subreqid=jobid, fileid=fileid, errno=errno, errmsg=errmsg)
    # only inform the stager
    try:
      stcur = dispatcher.dbConnection().cursor()
      stcur.arraysize = 50
      jobids, fileids, errnos, errmsgs = map(list,zip(*jobs))
      stcur.execute("BEGIN jobFailedSafe(:1, :2, :3); END;", [jobids, errnos, errmsgs])
      stcur.close()
    except Exception, e:
      for jobid, fileid, errno, errmsg in jobs:
        # "Exception caught while failing job" message
        dlf.writeerr(msgs.FAILJOBEXCEPTION, subreqid=jobid, fileid=fileid, type=str(e.__class__), msg=str(e))

  def exposed_jobsCanceled(self, machine, jobs):
    '''cancels jobs in the queues and informs the stager in case there is no
    remaining machines where it could run'''
    for jobid, fileid, rc, s in jobs:
      dlf.writedebug(msgs.JOBSCANCELEDCALLED, diskserver=machine,
                     subreqid=jobid, fileid=fileid, errcode=rc, errmsg=s)
    # first cancel jobs in the queue
    jobskilled = queuingJobs.jobsCanceled(machine, jobs)
    # for those that are really over, inform the stager
    if jobskilled:
      self.exposed_jobsKilled(jobskilled)

  def exposed_jobStarting(self, diskserver, jobid, jobtype):
    '''called when a job started on a given diskserver'''
    dlf.writedebug(msgs.JOBSTARTINGCALLED, diskserver=diskserver, subreqid=jobid, jobtype=jobtype)
    machinesToInform = queuingJobs.jobStarting(diskserver, jobid, jobtype)

  def exposed_getQueuingJobs(self, diskserver):
    '''called on the (re)start of the diskManager to rebuild the queue of jobs'''
    dlf.writedebug(msgs.GETQUEUINGJOBSCALLED, diskserver=diskserver)
    return tuple(queuingJobs.listQueuingJobs(diskserver))

  def exposed_getRunningD2dSourceJobs(self, diskserver):
    '''called on the (re)start of the diskManager to rebuild the list of running d2dsource jobs'''
    dlf.writedebug(msgs.GETRUNNINGD2DSOURCEJOBSCALLED, diskserver=diskserver)
    return tuple(queuingJobs.listRunningD2dSources(diskserver))

  def exposed_d2dend(self, jobid):
    '''called when a d2d is over. Responsible for informing the source of the d2d'''
    dlf.writedebug(msgs.D2DENDCALLED, subreqid=jobid)
    queuingJobs.d2dend(jobid)

def initQueues():
  '''initializes the queue of pending jobs by rebuilding it from the local queues of each diskserver'''
  dslist = diskServerList.getlist()
  for diskPool in dslist:
    for ds in dslist[diskPool]:
      try:
        # first list the running d2dsource jobs
        for jobid, job, arrivaltime in connections.getConnection(ds).getRunningD2dSource(socket.gethostname()):
          queuingJobs.putRunningD2dSource(ds, jobid, job, arrivaltime)
        # then list queuing jobs
        for jobid, job, jobtype, arrivaltime in connections.getConnection(ds).getQueuingJobs(socket.gethostname()):
          queuingJobs.put(ds, jobid, job, arrivaltime, jobtype)
      except Exception, e:
        # we could not connect. No problem, the scheduler is probably not running, so no queue to retrieve
        # 'No queue could be retrieved' message
        dlf.writenotice(msgs.NOQUEUERETRIEVED, diskserver=ds, error=str(e))

import signal
def handler(signum, frame):
  '''signal handler for the llsf daemon, only logging and calling sys.exit'''
  # 'Received signal' message
  dlf.write(msgs.SIGNALRECEIVED, signal=signum)
  sys.exit()
signal.signal(signal.SIGTERM, handler)
signal.signal(signal.SIGINT, handler)

# Note that from python 2.5 on, the daemonization should use the "with" statement :
# with daemon.DaemonContext():
#   rest of the code
if daemonize:
  context = daemon.DaemonContext()
  context.__enter__()
try:
  try :
    # get configuration
    configuration = castor_tools.castorConf()
    # setup logging
    dlf.init('llsfd')
    # global cache on the list of diskservers
    diskServerList = DiskServerListCache()
    # create a connection pool for connections to the DiskServers
    connections = connectionpool.ConnectionPool()
    # a dictionnary holding the list of jobs submitted by us and still queuing for each diskserver
    queuingJobs = serverqueue.ServerQueue(connections)
    # initialize the queues of pending and running jobs using the knowledge of the diskservers
    initQueues()
    # defines a service listening for monitoring queries and answering them
    from rpyc.utils.server import ThreadedServer
    llsfd = ThreadedServer(SchedulerService,
                           port = configuration.getValue('Scheduler', 'Port', 15011, int),
                           auto_register=False)
    llsfd.daemon = True
    # launch a processing thread that will regularly check if we need to schedule new jobs
    # and dispatch them on the relevant diskservers
    maxNbJobsScheduledPerSecond = configuration.getValue('Sched', 'MaxNbJobsScheduledPerSecond', None, int)
    dispatch = dispatcher.Dispatcher(connections, queuingJobs, maxNbJobsScheduledPerSecond, nbWorkers)
    dispatch.setName('Dispatcher')
    dispatch.setDaemon(True)
    dispatch.start()
    # launch a processing thread that will regularly check if there are jobs to abort
    abort = aborter.Aborter(connections)
    abort.setName('Aborter')
    abort.setDaemon(True)
    abort.start()
    # launch a processing thread that will regularly synchronize the stager DB with the running
    # jobs, meaning that it will check that jobs running for already long according to the DB
    # are effectively still running. If they are not, is will update the DB accordingly
    synchro = synchronizer.Synchronizer(connections, diskServerList)
    synchro.setName('Synchronize')
    synchro.setDaemon(True)
    synchro.start()
    # starts listening
    llsfd.start()
  except SystemExit:
    # we are exiting, fine
    pass
  except Exception, e:
    # 'Caught unexpected exception, exiting' message
    print 'Caught unexpected exception, exiting : (' + str(e.__class__) + ') : ' + str(e)
    dlf.writeemerg(msgs.UNEXPECTEDEXCEPTION, type=str(e.__class__), msg=str(e))
    raise
finally:
  # clean up all connections and threads that may be running
  try:
    connections.closeall()
  except:
    pass
  try:
    dispatch.stop()
    dispatch.join()
  except:
    pass
  try:
    abort.stop()
    abort.join()
  except:
    pass
  try:
    synchro.stop()
    synchro.join()
  except:
    pass
  try:
    llsfd.close()
  except:
    pass
  # if we are a daemon, call __exit__
  if daemonize:
    try:
      context.__exit__(None, None, None)
    except:
      pass
