#!/usr/bin/python
#/******************************************************************************
# *                   transfermanagerd.py
# *
# * This file is part of the Castor project.
# * See http://castor.web.cern.ch/castor
# *
# * Copyright (C) 2003  CERN
# * This program is free software; you can redistribute it and/or
# * modify it under the terms of the GNU General Public License
# * as published by the Free Software Foundation; either version 2
# * of the License, or (at your option) any later version.
# * This program is distributed in the hope that it will be useful,
# * but WITHOUT ANY WARRANTY; without even the implied warranty of
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# * GNU General Public License for more details.
# * You should have received a copy of the GNU General Public License
# * along with this program; if not, write to the Free Software
# * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
# *
# * @(#)$RCSfile: castor_tools.py,v $ $Revision: 1.9 $ $Release$ $Date: 2009/03/23 15:47:41 $ $Author: sponcec3 $
# *
# * transfer manager of the CASTOR project
# *
# * @author Castor Dev team, castor-dev@cern.ch
# *****************************************************************************/

import sys
import getopt
import rpyc
import time
import threading
import socket
import daemon
import castor_tools, connectionpool, dispatcher, serverqueue, aborter, synchronizer
import dlf
from transfermanagerdlf import msgs

# usage function
def usage(exitCode):
  print 'Usage : ' + sys.argv[0] + ' [-h|--help] [-f|--foreground] [-n|--nbworkers <nb worker threads>]'
  sys.exit(exitCode)

# first parse the options
daemonize = True
nbWorkers = 5
try:
  options, args = getopt.getopt(sys.argv[1:], 'hvfn:', ['help', 'foreground', 'nbworkers'])
except Exception, e:
  print e
  usage(1)
for f, v in options:
  if f == '-h' or f == '--help':
    usage(0)
  elif f == '-f' or f == '--foreground':
    daemonize = False
  elif f == '-n' or f == '--nbworkers':
    nbWorkers = int(v)
  else:
    print "unknown option : " + f
    usage(1)

# If any arg, complain and stop
if len(args) != 0:
  print "Unknown arguments : " + ' '.join(args) + "\n"
  usage(1)

class DiskServerListCache:
  '''cache for the list of diskservers.
  Automatically refreshes the list regularly, by default every 5mn.
  Set the refresh delay to 0 to always refresh (not recommended) and
  to a negative number to never refresh'''

  def __init__(self, refreshDelay=300):
    '''constructor'''
    # a dictionnary of diskpools with the list of diskservers in each of them
    self.diskPool2DiskServer = None
    # a dictionnary of diskservers with the associated pool for each of them
    self.diskServer2DiskPool = None
    self.refreshDelay = refreshDelay
    self.lastRefresh = 0
    # build the cache right now
    self.refresh()

  def refresh(self):
    '''refreshes the cache of diskservers/diskpools'''
    try:
      # query the stager database
      stconn = castor_tools.connectToStager()
      try:
        stcur = stconn.cursor()
        stcur.arraysize = 50
        stDiskServers = '''SELECT UNIQUE DiskServer.name, DiskPool.name
                             FROM FileSystem, DiskServer, DiskPool
                            WHERE FileSystem.diskServer = DiskServer.id
                              AND FileSystem.diskPool = DiskPool.id'''
        stcur.execute(stDiskServers)
        rows = stcur.fetchall()
        # build up the list of new ones
        self.diskPool2DiskServer = {}
        self.diskServer2DiskPool = {}
        for diskserver, diskPool in rows:
          if diskPool not in self.diskPool2DiskServer : self.diskPool2DiskServer[diskPool] = []
          self.diskPool2DiskServer[diskPool].append(diskserver)
          self.diskServer2DiskPool[diskserver] = diskPool
        self.lastRefresh = time.time()
      finally:
        try:
          castor_tools.disconnectDB(stconn)
        except:
          pass
    except Exception, e:
      # 'failed to refresh list of diskservers, kept old list' message
      dlf.writewarning(msgs.DSREFRESHFAILED, errortype=str(e.__class__), error=str(e))

  def getlist(self):
    '''returns the list of diskservers from the cache, clustered by diskpool.
    Builds this list form the DB if needed'''
    # check whether we need to refresh our data first
    if self.refreshDelay >= 0 and time.time() > self.lastRefresh + self.refreshDelay:
      self.refresh()
    # return the list
    return self.diskPool2DiskServer

  def getset(self):
    '''returns the set of diskservers from the cache, or builds it from the DB if needed'''
    # check whether we need to refresh our data first
    if self.refreshDelay >= 0 and time.time() > self.lastRefresh + self.refreshDelay:
      self.refresh()
    # return the set
    return self.diskServer2DiskPool.keys()

  def getDiskServerPool(self, diskserver):
    '''returns the pool in which a given diskserver lives'''
    return self.diskServer2DiskPool[diskserver]

def extendedSum(t):
  '''behaves like sum for tuples of numbers but is also able to handle a special case of
  tuples of tuples where each item has this format :
    (('key1', value), ...)
  In such a case, the resulting tuple will contain every key present in any of the original tuples
  and for each key, the sum of the values for this key in the different tuples'''
  if len(t) == 0: return 0
  if isinstance(t[0], int): return sum(t)
  dt = map(dict,t)
  keys = reduce(lambda a,b : a|set(b), [set()] + dt)
  return tuple((n, sum([d.get(n,0) for d in dt])) for n in keys)

class TransferManagerService(rpyc.Service):
  '''This service is responsible for answering all requests made to the scheduler.
  There are mainly 2 kinds : monitoring requests and callbacks from the diskservers'''

  def __init__(self, conn):
    '''constructor'''
    rpyc.Service.__init__(self, conn)
    # connection to the stager DB
    self.stagerConnection = None

  def dbConnection(self): 
    '''returns a connection to the stager DB.
    The connection is cached and reconnections are handled'''
    if self.stagerConnection == None:
      self.stagerConnection = castor_tools.connectToStager()
      self.stagerConnection.autocommit = True
    return self.stagerConnection

  def dispatch(self, func, diskPool, *args):
    '''gather the results of the call to func for all diskServers'''
    res = {}
    dslist = diskServerList.getlist()
    if diskPool != None:
      if diskPool in dslist:
        dslist = {diskPool : dslist[diskPool]}
      else:
        dslist = {}
    for diskPool in dslist:
      res[diskPool] = {}
      for ds in dslist[diskPool]:
        try:
          # call the function on the appropriate diskserver
          res[diskPool][ds] = getattr(connections,func)(ds, *args)
        except Exception, e:
          # 'Could not contact diskserver' message
          dlf.writenotice(msgs.COULDNOTCONTACTDS, function=func, diskserver=ds,
                          errortype=str(e.__class__), error=str(e))
          pass
    return res

  def exposed_nbTransfersPerPool(self, diskPool=None, user=None):
    '''returns the number of unique transfers pending on the pool given (or all pools)
    for the given user (or all users). Only tansfers handled by this transfer manager
    are reported'''
    return tuple(queueingTransfers.nbTransfersPerPool(diskServerList, diskPool, user).items())

  def _computeNbTransfersPerPool(self, diskPool=None, user=None):
    '''internal method to compute the number of unique transfers pending per pool
    accross all transfer managers. For this purpose, it contacts all running transfer
    managers and sums up the results. Results can be restricted to a given pool and a
    given user'''
    nbTransfersPerPool = {}
    for scheduler in configuration['DiskManager']['ServerHosts'].split():
      for diskpool, n in connections.nbTransfersPerPool(scheduler, diskPool, user):
        if diskpool not in nbTransfersPerPool: nbTransfersPerPool[diskpool] = 0
        nbTransfersPerPool[diskpool] = nbTransfersPerPool[diskpool] + n
    return nbTransfersPerPool

  def exposed_summarizeTransfersPerPool(self, diskPool=None, user=None, detailed=False):
    '''summarizes the number of running and pending transfers per diskpool for a given diskpool and user.
    Returns a tuple of tuples, with the diskpool name as first item, then a list of data that depends
    on the status of the diskpool and on the value of the the detailed parameter.
    If the diskpool is unknown or has no hosts in it, then only the diskpool name is present in each tuple.
    Else the number of unique transfers in the pool plus all values returned by each diskmanagerd
    summed up by pool are added to each tuple after the disk pool name. This last set varies depending
    on the detailed parameter.
    The exact format of the returned tuple for each non empty and existing pool if detailed is False is :
     (diskpoolName, nbUniqueQueueingTransfers, nbslots,
                    nbQueueingTransfers, nbQueueingSlots,
                    nbRunningTransfers, nbRunningSlots)
    If detailed is True, then it is :
     (diskpoolName, nbUniqueQueueingTransfers, nbslots,
                    nbQueueingTransfers, (('proto1', nbQueueingTransfersForProto1), ...),
                    nbQueueingSlots, (('proto1', nbQueueingSlotsForProto1), ...),
                    nbRunningTransfers, (('proto1', nbRunningTransfersForProto1), ...),
                    nbRunningSlots, (('proto1', nbRunningSlotsForProto1), ...))'''
    dlf.writedebug(msgs.SUMMARIZETRANSFERSPERPOOLCALLED, diskpool=diskPool, user=user, detailed=detailed)
    # get the raw data per diskserveras a dictionnary of dictionnaries (svclass then diskserver level)
    nbs = self.dispatch('summarizeTransfers', diskPool, user, detailed)
    # get the number of unique transfers running per diskpool
    nbtransfersPerPool = self._computeNbTransfersPerPool(diskPool, user)
    # flatten all data to a list of (diskpool, (values))
    res = []
    for diskPool in nbs:
      if len(nbs[diskPool]) > 0:
        data = map(extendedSum,zip(*nbs[diskPool].values()))
        if diskPool not in nbtransfersPerPool: nbtransfersPerPool[diskPool] = 0
        res.append(tuple([diskPool, nbtransfersPerPool[diskPool]] + data))
      else:
        res.append((diskPool,))
    return tuple(res)

  def exposed_summarizeTransfersPerHost(self, diskPool=None, user=None, detailed=False):
    '''summarizes the number of running and pending transfers per host for a given diskpool and user.
    Returns a tuple of tuples, with the host name as first item, then a list of data that depends
    on the value of the detailed parameter.
    The exact format of the returned tuple for each host if detailed is False is :
     (hostName, nbslots, nbQueueingTransfers, nbQueueingSlots, nbRunningTransfers, nbRunningSlots)
    If detailed is True, then it is :
     (hostName, nbslots, nbQueueingTransfers, (('proto1', nbQueueingTransfersForProto1), ...),
                         nbQueueingSlots, (('proto1', nbQueueingSlotsForProto1), ...),
                         nbRunningTransfers, (('proto1', nbRunningTransfersForProto1), ...),
                         nbRunningSlots, (('proto1', nbRunningSlotsForProto1), ...)) '''
    dlf.writedebug(msgs.SUMMARIZETRANSFERSPERHOSTCALLED, diskpool=diskPool, user=user, detailed=detailed)
    # get the raw data as a dictionnary of dictionnaries (svclass then diskserver level)
    hosts = self.dispatch('summarizeTransfers', diskPool, user, detailed)
    # flatten it to a list of (diskserver, (values))
    if hosts :
      hostslist = reduce(lambda x,y:x+y, [diskserver.items() for diskserver in hosts.values()])
    else:
      hostslist = []
    # return a tuple of (diskserver, value1, value2, ...)
    return tuple(tuple([host[0]]+list(host[1])) for host in hostslist)

  def exposed_listTransfers(self, diskPool=None, user=None):
    '''lists the running and pending transfers per diskpool for a given diskpool and user'''
    dlf.writedebug(msgs.LISTTRANSFERSCALLED, diskpool=diskPool)
    transfers = self.dispatch('listTransfers', diskPool, user)
    res = []
    for diskPool in transfers:
      for ds in transfers[diskPool]:
        for transferid, scheduler, user, status, transfertype, arrivalTime, startTime in transfers[diskPool][ds]:
          res.append((transferid, scheduler, user, status, diskPool, ds, transfertype, arrivalTime, startTime))
    return tuple(res)

  def _killtransfers(self, method, arg):
    '''internal method handling transfer killing. This is called from
    exposed_killalltransfers/killtransfers with the proper internal method and argument.
    It calls it on all schedulers and then handles the update of the stager DB'''
    # call the internal method on all schedulers (including ourselves)
    killedtransfers = []
    killedtransferids = []
    for scheduler in configuration['DiskManager']['ServerHosts'].split():
      for transferid, fileid in getattr(connections,method)(scheduler, arg):
        # for the transfers actually killed (and only once per tranfer
        if transferid not in killedtransferids:
          # Remember to fail the transfer in the stager DB with ESTKILLED error code
          killedtransferids.append(transferid)
          killedtransfers.append([transferid, fileid, 1713, 'Transfer killed by admin'])
    # update stagerDB
    self.exposed_transfersKilled(killedtransfers)    

  def exposed_killalltransfers(self, user=None):
    '''killalltransfers removes all transfers from all queues on all servers and diskservers'''
    dlf.writedebug(msgs.KILLALLTRANSFERSCALLED, user=user)
    # call the internal method
    self._killtransfers('killalltransfersinternal', user)

  def exposed_killtransfers(self, transferids):
    '''killtransfers removes given transfers from all queues on all servers and diskservers'''
    for transferid in transferids:
      dlf.writedebug(msgs.KILLTRANSFERSCALLED, subreqid=transferid)
    # call the internal method
    self._killtransfers('killtransfersinternal', transferids)

  def exposed_killalltransfersinternal(self, user):
    '''killalltransfersinternal removes all transfers from the queues handled by this scheduler.
    Return a tuple of (transferid, file) tuples for the transfers actually removed'''
    dlf.writedebug(msgs.KILLALLTRANSFERSINTERNALCALLED, user=user)
    return tuple(queueingTransfers.removeall(user).items())

  def exposed_killtransfersinternal(self, transferIds=None):
    '''killtransfersinternal removes transfers from the queues handled by this scheduler.
    Return a tuple of (transferid, file) tuples for the transfers actually removed'''
    for transferId in transferIds:
      dlf.writedebug(msgs.KILLTRANSFERSINTERNALCALLED, subreqid=transferId)
    return tuple(queueingTransfers.remove(transferIds).items())

  def exposed_transfersKilled(self, transfers):
    '''informs the stager that these transfers have been killed'''
    for transferid, fileid, errno, errmsg in transfers:
      dlf.writedebug(msgs.TRANSFERSKILLEDCALLED, subreqid=transferid, fileid=fileid, errno=errno, errmsg=errmsg)
    # only inform the stager
    try:
      stcur = self.dbConnection().cursor()
      stcur.arraysize = 50
      transferids, fileids, errnos, errmsgs = map(list,zip(*transfers))
      stcur.execute("BEGIN transferFailedSafe(:1, :2, :3); END;", [transferids, errnos, errmsgs])
      stcur.close()
    except Exception, e:
      for transferid, fileid, errno, errmsg in transfers:
        # "Exception caught while failing transfer" message
        dlf.writeerr(msgs.FAILTRANSFEREXCEPTION, subreqid=transferid, fileid=fileid, type=str(e.__class__), msg=str(e))
      # check whether we should reconnect to DB, and do so if needed
      self.dbConnection().checkForReconnection(e)

  def exposed_transfersCanceled(self, machine, transfers):
    '''cancels transfers in the queues and informs the stager in case there is no
    remaining machines where it could run'''
    for transferid, fileid, rc, s in transfers:
      dlf.writedebug(msgs.TRANSFERSCANCELEDCALLED, diskserver=machine,
                     subreqid=transferid, fileid=fileid, errcode=rc, errmsg=s)
    # first cancel transfers in the queue
    transferskilled = queueingTransfers.transfersCanceled(machine, transfers)
    # for those that are really over, inform the stager
    if transferskilled:
      self.exposed_transfersKilled(transferskilled)

  def exposed_transferStarting(self, diskserver, transferid, transfertype):
    '''called when a transfer started on a given diskserver'''
    dlf.writedebug(msgs.TRANSFERSTARTINGCALLED, diskserver=diskserver, subreqid=transferid, transfertype=transfertype)
    machinesToInform = queueingTransfers.transferStarting(diskserver, transferid, transfertype)

  def exposed_getQueueingTransfers(self, diskserver):
    '''called on the (re)start of the diskManager to rebuild the queue of transfers'''
    dlf.writedebug(msgs.GETQUEUEINGTRANSFERSCALLED, diskserver=diskserver)
    return tuple(queueingTransfers.listQueueingTransfers(diskserver))

  def exposed_getRunningD2dSourceTransfers(self, diskserver):
    '''called on the (re)start of the diskManager to rebuild the list of running d2dsrc transfers'''
    dlf.writedebug(msgs.GETRUNNINGD2DSOURCETRANSFERSCALLED, diskserver=diskserver)
    return tuple(queueingTransfers.listRunningD2dSources(diskserver))

  def exposed_d2dend(self, transferid):
    '''called when a d2d is over. Responsible for informing the source of the d2d'''
    dlf.writedebug(msgs.D2DENDCALLED, subreqid=transferid)
    queueingTransfers.d2dend(transferid)

  def exposed_drain(self):
    '''when called, moves the daemon to drain mode, where only ongoing transfers are handled
    but no new work is taken'''
    dlf.writedebug(msgs.DRAINCALLED)
    drain()

def initQueues():
  '''initializes the queue of pending transfers by rebuilding it from the local queues of each diskserver'''
  dslist = diskServerList.getlist()
  for diskPool in dslist:
    for ds in dslist[diskPool]:
      try:
        # first list the running d2dsrc transfers
        for transferid, transfer, arrivaltime in connections.getConnection(ds).getRunningD2dSource(socket.getfqdn()):
          queueingTransfers.putRunningD2dSource(ds, transferid, transfer, arrivaltime)
        # then list queueing transfers
        for transferid, transfer, transfertype, arrivaltime in connections.getConnection(ds).getQueueingTransfers(socket.getfqdn()):
          queueingTransfers.put(ds, transferid, transfer, arrivaltime, transfertype)
      except Exception, e:
        # we could not connect. No problem, the scheduler is probably not running, so no queue to retrieve
        # 'No queue could be retrieved' message
        dlf.writenotice(msgs.NOQUEUERETRIEVED, diskserver=ds, error=str(e))

def drain():
  '''moves the server to drain mode where dispatcher, aborter and synchronizer threads are stopped.
  This allows to handled ongoing transfers before stopping the server'''
  try:
    if dispatch:
      dispatch.stop()
      dispatch.join()
  except:
    pass
  abort = None
  try:
    if abort:
      abort.stop()
      abort.join()
  except:
    pass
  abort = None
  try:
    if synchro:
      synchro.stop()
      synchro.join()
  except:
    pass
  synchro = None

import signal
def handler(signum, frame):
  '''signal handler for the transfer manager daemon, only logging and calling sys.exit'''
  # 'Received signal' message
  dlf.write(msgs.SIGNALRECEIVED, signal=signum)
  sys.exit()
signal.signal(signal.SIGTERM, handler)
signal.signal(signal.SIGINT, handler)

# Note that from python 2.5 on, the daemonization should use the "with" statement :
# with daemon.DaemonContext():
#   rest of the code
if daemonize:
  context = daemon.DaemonContext(detach_process=True)
  context.__enter__()
try:
  try :
    # get configuration
    configuration = castor_tools.castorConf()
    # setup logging
    dlf.init('transfermanagerd')
    # global cache on the list of diskservers
    diskServerList = DiskServerListCache()
    # create a connection pool for connections to the DiskServers
    connections = connectionpool.ConnectionPool()
    # a dictionnary holding the list of transfers submitted by us and still queueing for each diskserver
    queueingTransfers = serverqueue.ServerQueue(connections)
    # initialize the queues of pending and running transfers using the knowledge of the diskservers
    initQueues()
    # defines a service listening for monitoring queries and answering them
    from rpyc.utils.server import ThreadedServer
    transfermanager = ThreadedServer(TransferManagerService,
                                     port = configuration.getValue('Scheduler', 'Port', 15011, int),
                                     auto_register=False)
    transfermanager.daemon = True
    # launch a processing thread that will regularly check if we need to schedule new transfers
    # and dispatch them on the relevant diskservers
    maxNbTransfersScheduledPerSecond = configuration.getValue('TransferManager', 'MaxNbTransfersScheduledPerSecond', None, int)
    dispatch = dispatcher.Dispatcher(connections, queueingTransfers, maxNbTransfersScheduledPerSecond, nbWorkers)
    dispatch.setName('Dispatcher')
    dispatch.setDaemon(True)
    dispatch.start()
    # launch a processing thread that will regularly check if there are transfers to abort
    abort = aborter.Aborter(connections)
    abort.setName('Aborter')
    abort.setDaemon(True)
    abort.start()
    # launch a processing thread that will regularly synchronize the stager DB with the running
    # transfers, meaning that it will check that transfers running for already long according to the DB
    # are effectively still running. If they are not, is will update the DB accordingly
    synchro = synchronizer.Synchronizer(connections, diskServerList)
    synchro.setName('Synchronize')
    synchro.setDaemon(True)
    synchro.start()
    # starts listening
    transfermanager.start()
  except SystemExit:
    # we are exiting, fine
    pass
  except Exception, e:
    # 'Caught unexpected exception, exiting' message
    print 'Caught unexpected exception, exiting : (' + str(e.__class__) + ') : ' + str(e)
    dlf.writeemerg(msgs.UNEXPECTEDEXCEPTION, type=str(e.__class__), msg=str(e))
    raise
finally:
  # clean up all connections and threads that may be running
  try:
    connections.closeall()
  except:
    pass
  # drain will stop the dispatch, abort and synchro threads
  drain()
  try:
    transfermanager.close()
  except:
    pass
  # if we are a daemon, call __exit__
  if daemonize:
    try:
      context.__exit__(None, None, None)
    except:
      pass
