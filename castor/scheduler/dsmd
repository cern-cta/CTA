#!/usr/bin/python
#/******************************************************************************
# *                   dsmd.py
# *
# * This file is part of the Castor project.
# * See http://castor.web.cern.ch/castor
# *
# * Copyright (C) 2003  CERN
# * This program is free software; you can redistribute it and/or
# * modify it under the terms of the GNU General Public License
# * as published by the Free Software Foundation; either version 2
# * of the License, or (at your option) any later version.
# * This program is distributed in the hope that it will be useful,
# * but WITHOUT ANY WARRANTY; without even the implied warranty of
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# * GNU General Public License for more details.
# * You should have received a copy of the GNU General Public License
# * along with this program; if not, write to the Free Software
# * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
# *
# * @(#)$RCSfile: castor_tools.py,v $ $Revision: 1.9 $ $Release$ $Date: 2009/03/23 15:47:41 $ $Author: sponcec3 $
# *
# * the disk server manager daemon of CASTOR
# *
# * @author Castor Dev team, castor-dev@cern.ch
# *****************************************************************************/

import os, sys
import getopt
import time
import rpyc
import subprocess
import socket
import tempfile
import threading
import connectionpool, castor_tools, localqueue, runningjobsset
import daemon
import dlf
from dsmddlf import msgs

# usage function
def usage(exitCode):
  print 'Usage : ' + sys.argv[0] + ' [-h|--help] [-f|--foreground] [--fakemode]'
  sys.exit(exitCode)

# first parse the options
daemonize = True
fake = False
try:
  options, args = getopt.getopt(sys.argv[1:], 'hvf', ['help', 'foreground', 'fakemode'])
except Exception, e:
  print e
  usage(1)
for f, v in options:
  if f == '-h' or f == '--help':
    usage(0)
  elif f == '-f' or f == '--foreground':
    daemonize = False
  elif f == '--fakemode':
    fake = True
  else:
    print "unknown option : " + f
    usage(1)

# If any arg, complain and stop
if len(args) != 0:
  print "Unknown arguments : " + ' '.join(args) + "\n"
  usage(1)


class DiskServerManagerService(rpyc.Service):
  '''implementation of the DiskServer Manager service.
  This service handles all the calls from the central scheduling, from
  job starting to monitoring'''

  def exposed_scheduleJob(self, scheduler, jobid, job, arrivaltime, jobtype='standard'):
    '''Called when a new job needs to be scheduled.
    Queues the new job.'''
    dlf.writedebug(msgs.SCHEDULEJOBCALLED, scheduler=scheduler, subreqid=jobid,
                   jobtype=jobtype, fileid=(job[8],int(job[6])))
    jobQueue.put(scheduler, jobid, job, jobtype, arrivaltime)

  def exposed_nbJobs(self):
    '''Called when bhosts or bqueues monitoring is needed.
    Returns the number of slots. running jobs and pending jobs on the diskserver'''
    dlf.writedebug(msgs.NBJOBSCALLED)
    return (configuration.getValue('DiskManager', 'NbSlots', None, int), jobQueue.nbJobs(), runningJobs.nbJobs())

  def exposed_bjobs(self):
    '''Called when bjobs monitoring is needed.
    Lists all jobs running or pending on this host'''
    dlf.writedebug(msgs.BJOBSCALLED)
    return tuple(runningJobs.bjobs() + jobQueue.bjobs())

  def exposed_jobset(self):
    '''Lists all pending and running jobs'''
    dlf.writedebug(msgs.JOBSETCALLED)
    return tuple(runningJobs.jobset() | jobQueue.jobset())

  def exposed_bkill(self, jobids):
    '''Called when some jobs should be killed'''
    dlf.writedebug(msgs.BKILLCALLED, jobids=', '.join(jobids))
    # remove jobs from the local queue
    jobsCanceled = jobQueue.remove(jobids)

  def exposed_getQueuingJobs(self, scheduler):
    '''returns the list of jobs in the queue'''
    dlf.writedebug(msgs.GETQUEUINGJOBSCALLED, scheduler=scheduler)
    return tuple(jobQueue.listQueuingJobs())

  def exposed_getRunningD2dSource(self, scheduler):
    '''returns the list of running d2dsource jobs'''
    dlf.writedebug(msgs.GETRUNNINGD2DSOURCECALLED, scheduler=scheduler)
    return tuple(runningJobs.listRunningD2dSources())

  def exposed_d2dend(self, jobid):
    '''called when a d2d copy is over and we are the source'''
    dlf.writedebug(msgs.D2DENDCALLED, subreqid=jobid)
    runningJobs.d2dend(jobid)

  def checkSpaceLeftOnFS(fs):
    ''' check whether there is space available on a given filesystem'''
    # find out the limit in terms of free space, from the config file
    minFreeSpacePerc = configuration.getValue('RmNode','MinAllowedFreeSpace',0,float)
    # get status of the filesystem
    stat = os.statvfs(fs)
    availableSpace = s.f_bavail * s.f_frsize
    totalSpace = s.f_blocks * s.f_bsize
    # do the check
    return availableSpace > minFreeSpacePerc * totalSpace

class ActivityControl(threading.Thread):
  '''activity control thread.
  This thread is responsible for starting new jobs when free slots are available'''

  def __init__(self):
    '''constructor'''
    super(ActivityControl,self).__init__(name='ActivityControl')
    self._alive = True

  def run(self):
    '''main method, containing the infinite loop'''
    while True:
      try:
        # check number of running jobs against limit
        if runningJobs.nbUsedSlots() < configuration.getValue('DiskManager', 'NbSlots', None, int):
          # get a new job from the jobQueue
          scheduler, jobid, job, jobtype, arrivalTime = jobQueue.get()
          fileid = (job[8], int(job[6]))
          try:
            # in case of jobs wanting to write data, we check that space is available
            if not checkSpaceLeftOnFS(jobtype, job):
              # not enough space, we cancel the job
              connections.jobsCanceled(scheduler, socket.getfqdn(), set([(jobid, fileid, 28, 'No space left on device')])) # ENOSPC
            else:
              # notifies the central scheduler that we want to start this job
              # this may raise a ValueError exception if it already started
              # somewhere else
              connections.jobStarting(scheduler, socket.gethostname(), jobid, jobtype)
              # "Job starting" message
              dlf.write(msgs.JOBSTARTING, subreqid=jobid, fileid=fileid, jobtype=jobtype)
              # in case of d2dsource, do not actually start for real, only take note
              if jobtype == 'd2dsource':
                runningJobs.add(jobid, scheduler, job, None, None, jobtype, arrivalTime)
              else:
                # create a local file to hold the former LSF scheduler info
                notifFileHandle, notifFileName = tempfile.mkstemp()
                notifFile = os.fdopen(notifFileHandle, 'w')
                notifFile.write(job[-1])
                notifFile.close()
                os.chmod(notifFileName, 0666)
                # build the stagerJob command line and execute it
                cmd = list(job[:-1])
                cmd.append('file://'+notifFileName)
                # start the job
                if not fake:
                  process = subprocess.Popen(cmd)
                else:
                  process = 0
                runningJobs.add(jobid, scheduler, job, notifFileName, process, jobtype, arrivalTime)
          except ValueError:
            # 'Job start canceled (already started on another host)' message
            dlf.writedebug(msgs.JOBALREADYSTARTED, subreqid=jobid, fileid=fileid)
            # the job has already started somewhere else, so give up
            pass
          except EnvironmentError:
            # we have tried to start a disk to disk copy and the source is not yet ready
            # we will put it in a priority queue
            # "Start postponned until source is ready" message
            dlf.write(msgs.POSTPONEDFORSRCNOTREADY, subreqid=jobid, fileid=fileid)
            jobQueue.d2dDestReady(scheduler, jobid, job, arrivalTime, jobtype)
          except Exception, e:
            # we could not connect to scheduler to check whether the job can be started
            log(syslog.LOG_ERR, "Informing " + scheduler + " that job " + jobid + ' started failed with error ' + str(e))
            # put back the job in the queue
            jobQueue.priorityPut(scheduler, jobid, job, jobtype, arrivalTime)
          except Exception, e:
            # startup of the job failed with unexpected error
            # 'Job starting failed' message
            dlf.writeerr(msgs.JOBSTARTINGFAILED, subreqid=jobid, fileid=fileid, error=str(e))
            time.sleep(1)
        else:
          time.sleep(.05)
      except:
        # "Caught exception in ActivityControl thread" message
        dlf.writeerr(msgs.ACTIVITYCONTROLEXCEPTION, type=str(e.__class__), msg=str(e))
        time.sleep(1)

  def stop(self):
    '''stops processing in this thread'''
    self._alive = False

class ManagementThread(threading.Thread):
  '''Management thread of the disk server manager, dealing with job completion control and associated cleanup,
  with job cancelation in case of no space, timeouts, etc... and with restart of disk to disk transfer
  that are waiting on the sources when these are ready'''

  def __init__(self):
    '''constructor'''
    super(ManagementThread,self).__init__(name='ManagementThread')
    self._alive = True

  def run(self):
    '''main method, containing the infinite loop'''
    while self._alive:
      try:
        # check for running jobs that have completed
        runningJobs.poll()
        # check for d2d destination jobs that could now start
        jobQueue.pollD2dDest()
        # check for job to be canceled for various reasons (timeout, missing resources, ...)
        jobQueue.checkForJobCancelation()
      except Exception, e:
        # 'Caught exception in Management thread' message
        dlf.writeerr(msgs.MGMTEXCEPTION, type=str(e.__class__), msg=str(e))
      # do not loop too fast
      time.sleep(1)

  def stop(self):
    '''stops processing in this thread'''
    self._alive = False

def initQueues():
  for scheduler in configuration['DiskManager']['ServerHosts'].split():
    try:
      # rebuild list of running d2dsource jobs
      for jobid, job, arrivaltime in connections.getRunningD2dSourceJobs(scheduler,socket.gethostname()):
        runningJobs.add(jobid, scheduler, job, None, None, 'd2dsource', arrivaltime)
      # rebuild job's queue
      for jobid, job, arrivaltime, jobtype in connections.getQueuingJobs(scheduler,socket.gethostname()):
        jobQueue.put(scheduler, jobid, job, jobtype, arrivaltime)
    except Exception, e:
      # we could not connect. No problem, the scheduler is probably not running, so no queue to retrieve
      # 'No queue could be retrieved' message
      dlf.writenotice(msgs.NOQUEUERETRIEVED, scheduler=scheduler, error=str(e))

import signal
def handler(signum, frame):
  '''signal handler for the disk server manager daemon, only logging and calling sys.exit'''
  # 'Received signal' message
  dlf.write(msgs.SIGNALRECEIVED, signal=signum)
  sys.exit()
signal.signal(signal.SIGTERM, handler)
signal.signal(signal.SIGINT, handler)

# Note that from python 2.5 on, the daemonization should use the "with" statement :
# with daemon.DaemonContext():
if daemonize:
  context = daemon.DaemonContext()
  context.__enter__()
try:
  try:
    # get configuration
    configuration = castor_tools.castorConf()
    # setup logging
    dlf.init('dsmd')
    # create a connection pool for connections to the stagers
    connections = connectionpool.ConnectionPool()
    # create a queue of jobs to be run
    jobQueue = localqueue.LocalQueue(configuration, connections)
    # create a list of running Jobs
    runningJobs = runningjobsset.RunningJobsSet(connections, fake)
    # initialize the queues of pending and running jobs using the knowledge of the central schedulers
    initQueues()
    # launch a processing thread that will regularly check if we need to start new jobs from the queue
    activityControl = ActivityControl()
    activityControl.setDaemon(True)
    activityControl.start()
    # launch a management thread that will regularly check job completions, d2d transfer to be restarted
    # and jobs to be dropped from the queue, because of timeouts, lack of space, etc...
    management = ManagementThread()
    management.setDaemon(True)
    management.start()
    # launch a service listening for new jobs from the central scheduler and filling the queue
    from rpyc.utils.server import ThreadedServer
    t = ThreadedServer(DiskServerManagerService, 
                       port = configuration.getValue('DiskManager', 'Port', 15011, int),
                       auto_register=False)
    t.daemon = True
    t.start()
    # we reach this point when the service has stopped
    # so let's stop the other threads
    activityControl.stop()
    management.stop()
  except SystemExit:
    # we are exiting, fine
    pass
  except Exception, e:
    # 'Caught unexpected exception, exiting' message
    print 'Caught unexpected exception, exiting : (' + str(e.__class__) + ') : ' + str(e)
    dlf.writeemerg(msgs.UNEXPECTEDEXCEPTION, type=str(e.__class__), msg=str(e))
    raise
finally:
  if daemonize:
    try:
      context.__exit__(None, None, None)
    except:
      pass
