#!/usr/bin/python
#/******************************************************************************
# *                   dsmd.py
# *
# * This file is part of the Castor project.
# * See http://castor.web.cern.ch/castor
# *
# * Copyright (C) 2003  CERN
# * This program is free software; you can redistribute it and/or
# * modify it under the terms of the GNU General Public License
# * as published by the Free Software Foundation; either version 2
# * of the License, or (at your option) any later version.
# * This program is distributed in the hope that it will be useful,
# * but WITHOUT ANY WARRANTY; without even the implied warranty of
# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# * GNU General Public License for more details.
# * You should have received a copy of the GNU General Public License
# * along with this program; if not, write to the Free Software
# * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
# *
# * @(#)$RCSfile: castor_tools.py,v $ $Revision: 1.9 $ $Release$ $Date: 2009/03/23 15:47:41 $ $Author: sponcec3 $
# *
# * the disk server manager daemon of CASTOR
# *
# * @author Castor Dev team, castor-dev@cern.ch
# *****************************************************************************/

import os, sys
import getopt
import time
import rpyc
import subprocess
import socket
import tempfile
import threading
import connectionpool, castor_tools, localqueue, runningjobsset
import syslog
import daemon

# usage function
def usage(exitCode):
  print 'Usage : ' + sys.argv[0] + ' [-h|--help] [-f|--foreground] [--fakemode]'
  sys.exit(exitCode)

# first parse the options
daemonize = True
fake = False
try:
  options, args = getopt.getopt(sys.argv[1:], 'hvf', ['help', 'foreground', 'fakemode'])
except Exception, e:
  print e
  usage(1)
for f, v in options:
  if f == '-h' or f == '--help':
    usage(0)
  elif f == '-f' or f == '--foreground':
    daemonize = False
  elif f == '--fakemode':
    fake = True
  else:
    print "unknown option : " + f
    usage(1)

# If any arg, complain and stop
if len(args) != 0:
  print "Unknown arguments : " + ' '.join(args) + "\n"
  usage(1)


class DiskServerManagerService(rpyc.Service):
  '''implementation of the DiskServer Manager service.
  This service handles all the calls from the central scheduling, from
  job starting to monitoring'''

  def exposed_scheduleJob(self, scheduler, jobid, job, arrivaltime, jobtype='standard'):
    '''Called when a new job needs to be scheduled.
    Queues the new job.'''
    log(syslog.LOG_DEBUG, 'scheduleJob called from ' + scheduler + ' for jobid ' + jobid + ' (' + jobtype + ')')
    jobQueue.put(scheduler, jobid, job, jobtype, arrivaltime)

  def exposed_nbJobs(self):
    '''Called when bhosts or bqueues monitoring is needed.
    Returns the number of slots. running jobs and pending jobs on the diskserver'''
    log(syslog.LOG_DEBUG, 'nbJobs called')
    return (int(configuration['DiskManager']['MaxNbJobs']), jobQueue.nbJobs(), runningJobs.nbJobs())

  def exposed_badmin(self):
    '''Called when the diskserver should be reconfigured, e.g. on a badmin call'''
    log(syslog.LOG_DEBUG, 'badmin called')
    configuration.refresh()

  def exposed_bjobs(self):
    '''Called when bjobs monitoring is needed.
    Lists all jobs running or pending on this host'''
    log(syslog.LOG_DEBUG, 'bjobs called')
    return tuple(runningJobs.bjobs() + jobQueue.bjobs())

  def exposed_getQueuingJobs(self, scheduler):
    '''returns the list of jobs in the queue'''
    log(syslog.LOG_DEBUG, 'getQueuingJobs called from ' + scheduler)
    return tuple(jobQueue.listQueuingJobs())

  def exposed_getRunningD2dSource(self, scheduler):
    '''returns the list of running d2dsource jobs'''
    log(syslog.LOG_DEBUG, 'getRunningD2dSource called from ' + scheduler)
    return tuple(runningJobs.listRunningD2dSources())

  def exposed_d2dend(self, jobid):
    '''called when a d2d copy is over and we are the source'''
    log(syslog.LOG_DEBUG, 'd2dend called for jobid ' + jobid)
    runningJobs.d2dend(jobid)

class ActivityControl(threading.Thread):
  '''activity control thread.
  This thread is responsible for starting new jobs when free slots are available'''

  def __init__(self):
    '''constructor'''
    super(ActivityControl,self).__init__(name='ActivityControl')
    self._alive = True

  def run(self):
    '''main method, containing the infinite loop'''
    while True:
      try:
        # check number of running jobs against limit
        if runningJobs.nbJobs() < int(configuration['DiskManager']['MaxNbJobs']):
          # get a new job from the jobQueue
          scheduler, jobid, job, jobtype, arrivalTime = jobQueue.get()
          try:
            # notifies the central scheduler that we want to start this job
            # this may raise a ValueError exception if it already started
            # somewhere else
            connections.jobStarting(scheduler, socket.gethostname(), jobid, jobtype)
            # log job start
            log('Starting ' + jobid + ' (' + jobtype + ')')
            # in case of d2dsource, do not actually start for real, only take note
            if jobtype == 'd2dsource':
              runningJobs.add(jobid, scheduler, job, None, None, jobtype, arrivalTime)
            else:
              # create a local file to hold the former LSF scheduler info
              notifFileHandle, notifFileName = tempfile.mkstemp()
              notifFile = os.fdopen(notifFileHandle, 'w')
              notifFile.write(job[-1])
              notifFile.close()
              os.chmod(notifFileName, 0666)
              # build the stagerJob command line and execute it
              cmd = list(job[:-1])
              cmd.append('file://'+notifFileName)
              # start the job
              if not fake:
                process = subprocess.Popen(cmd)
              else:
                process = 0
              runningJobs.add(jobid, scheduler, job, notifFileName, process, jobtype, arrivalTime)
          except ValueError:
            log('Canceled start of ' + jobid + '(already started on another host)')
            # the job has already started somewhere else, so give up
            pass
          except EnvironmentError:
            # we have tried to start a disk to disk copy and the source is not yet ready
            # we will put it in a priority queue
            log('Start postponned until source is ready for ' + jobid)
            jobQueue.d2dDestReady(scheduler, jobid, job, arrivalTime, jobtype)
          except Exception, e:
            # we could not connect to scheduler to check whether the job can be started
            log(syslog.LOG_ERR, "Informing " + scheduler + " that job " + jobid + ' started failed with error ' + str(e))
            # put back the job in the queue
            jobQueue.priorityPut(scheduler, jobid, job, jobtype, arrivalTime)
            time.sleep(1)          
        else:
          time.sleep(.05)
      except Exception, e:
        log(syslog.LOG_ERR, "Caught exception in ActivityControl thread : " + str(e))
        time.sleep(1)          

  def stop(self):
    '''stops processing in this thread'''
    self._alive = False

class JobCompletionControl(threading.Thread):
  '''Core of the job completion control thread. This thread is responsible for detecting
  terminated jobs and cleaning up the list of running jobs accordingly'''

  def __init__(self):
    '''constructor'''
    super(JobCompletionControl,self).__init__(name='JobCompletionControl')
    self._alive = True

  def run(self):
    '''main method, containing the infinite loop'''
    while self._alive:
      try:
        # check for running jobs that have completed
        runningJobs.poll()
        # check for d2d destination jobs that could now start
        jobQueue.pollD2dDest()
        # do not loop too fast
      except Exception, e:
        log(syslog.LOG_ERR, "Caught exception in JobCompletionControl thread : " + str(e))
      time.sleep(1)

  def stop(self):
    '''stops processing in this thread'''
    self._alive = False

def initQueues():
  for scheduler in configuration['DiskManager']['ServerHosts'].split():
    try:
      # rebuild list of running d2dsource jobs
      for jobid, job, arrivaltime in connections.getRunningD2dSourceJobs(scheduler,socket.gethostname()):
        runningJobs.add(jobid, scheduler, job, None, None, 'd2dsource', arrivaltime)
      # rebuild job's queue
      for jobid, job, arrivaltime, jobtype in connections.getQueuingJobs(scheduler,socket.gethostname()):
        jobQueue.put(scheduler, jobid, job, jobtype, arrivaltime)
    except Exception, e:
      # we could not connect. No problem, the scheduler is probably not running, so no queue to retrieve
      log('No queue could be retrieved from ' + scheduler + ' (' + str(e) + ')')
      pass

import signal
def handler(signum, frame):
  if signum == signal.SIGTERM:
    log('SIGTERM received. Exiting')
  elif signum == signal.SIGINT:
    log('SIGINT received. Exiting')
  else:
    log('Unexpected signal %d received. Exiting') % signum
  sys.exit(1)
signal.signal(signal.SIGTERM, handler)
signal.signal(signal.SIGINT, handler)

# Note that from python 2.5 on, the daemonization should use the "with" statement :
# with daemon.DaemonContext():
if daemonize:
  context = daemon.DaemonContext()
  context.__enter__()
try:
  # get configuration
  configuration = castor_tools.castorConf()
  # setup logging
  verbosity = configuration['dsmd']['LogMask']
  syslog.openlog('dsmd', 0, syslog.LOG_LOCAL3)
  syslog.setlogmask(syslog.LOG_UPTO(getattr(syslog, verbosity)))
  log = syslog.syslog
  # create a queue of jobs to be run
  jobQueue = localqueue.LocalQueue(configuration)
  # create a connection pool for connections to the stagers
  connections = connectionpool.ConnectionPool(int(configuration['Sched']['Port']))
  # create a list of running Jobs
  runningJobs = runningjobsset.RunningJobsSet(connections, fake)
  # initialize the queues of pending and running jobs using the knowledge of the central schedulers
  initQueues()
  # launch a processing thread that will regularly check if we need to start new jobs from the queue
  activityControl = ActivityControl()
  activityControl.setDaemon(True)
  activityControl.start()
  # launch a processing thread that will regularly check job completions
  jobCompletionControl = JobCompletionControl()
  jobCompletionControl.setDaemon(True)
  jobCompletionControl.start()
  # launch a service listening for new jobs from the central scheduler and filling the queue
  from rpyc.utils.server import ThreadedServer
  t = ThreadedServer(DiskServerManagerService, 
                     port = int(configuration['DiskManager']['Port']),
                     auto_register=False)
  t.daemon = True
  t.start()
  # we reach this point when the service has stopped
  # so let's stop the other threads
  activityControl.stop()
  jobCompletionControl.stop()
finally:
  if daemonize:
    context.__exit__(None, None, None)
