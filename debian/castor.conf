######################################################################################
#
# CASTOR2 Sample Configuration File
# $Id: castor.conf,v 1.76 2007/10/19 09:36:48 kotlyar Exp $
#
######################################################################################

#
# Service to Host Mapping
#

#CNS        HOST castorns
#EXPERT     HOST castorexpert
#JOBMANAGER HOST castorjobmanager
#JOBMANAGER NOTIFYPORT 15011
#MSG        HOST castormsg
#REPACK     HOST castorrepack
#RM         HOST castorscheduler
#STAGER     HOST castorrh
#STAGER     NOTIFYHOST castorstager
#STAGER     JOBREQNOTIFYPORT    10001
#STAGER     PREPREQNOTIFYPORT   10002
#STAGER     STAGEREQNOTIFYPORT  10003
#STAGER     QUERYREQNOTIFYPORT  10004
#STAGER     ERRORNOTIFYPORT     10010
#STAGER     JOBNOTIFYPORT       10011
#STAGER     GCNOTIFYPORT        10012
#VDQM	    HOST castorvdqm
#VMGR       HOST castorvmgr
#UPV	    HOST castorcupv


## Client configuration #############################################################

# The port range to be used by clients for the stager callbacks. If not present a port
# range of 30000-30100 is used. You can disable the port range by setting both values
# to 0

CLIENT   LOWPORT        30000
CLIENT   HIGHPORT       40000

# Maximum number of responses allowed when a file query using reqexp is done. Default
# is 1000, max allowed is 30000. Can also be set by defining the environment variable
# FILEQUERY_MAXNBRESPONSES.

FILEQUERY	MAXNBRESPONSES	1000


## Service Configuration ############################################################

# The default port on which the request handler should listen. Note that this can
# be overwritten by environment variable and command line argument

#RH PORT 9002

# Whether the request handler should use access lists. Default is no.
# Set it to YES (case insensitive) to enable it

#RH USEACCESSLISTS NO

# The default service class and protocol used by repack2 for writing files to disk and
# back to tape.

#REPACK SVCCLASS default
#REPACK PROTOCOL rfio

# rmMasterDaemon (Resource Monitoring Daemon)
#   - responsible for collecting monitoring information from rmNodeDaemon's and 
#     managing state information
#
# The frequency at which the rmMasterDaemon updates/synchronises the monitoring
# information stored in the shared memory to the stager database

RmMaster UpdateInterval  	10

# Defines the maximum amount of time in seconds that an rmNodeDaemon can be out of
# contact with the rmMasterDaemon before automatically being set to a disabled state.
# A value of -1 disables this check

RmMaster HeartbeatTimeout  	20

# rmNodeDaemon 
#   - responsible for collecting monitoring information from diskservers and reporting
#     the collected data back to the rmMasterDaemon. 
#   - Also responsible for sub directory creation in mount points!!
#
# Note: Don't forget to specify MountPoints option which replaces the old RMNODECONFIG
#       file in pre 2.1.3 releases.
#
# Defines the frequency in seconds that state updates are sent to the rmMastserDaemon.
# This includes the status of the node and slow moving metrics like total memory and
# total swap which does not vary outside of reboots

RmNode	 StateUpdateInterval  	60

# The frequency in seconds at which metric information is sent to the rmMasterDaemon
# about the filesystems defined in RmNode/MountPoints.

RmNode	 MetricsUpdateInterval 	10
RmNode	 MountPoints  		/srv/castor/01/ /srv/castor/02/ /srv/castor/03/

# The location of the nodes status file detailing the status of the disk server and
# its associated filesystems as seen by the central server.
#
# Note: The rmNodeDaemon always sends a status of PRODUCTION, this status can only
#    be changed by forcing it into another state via the use of rmAdminNode. As the
#    the state reported by the client is always PRODUCTION the status file can be
#    used to provide input to third party monitoring systems as to the true status
#    of the diskserver i.e. how the LSF plugin and stager see the diskserver.

RmNode	 StatusFile		/etc/castor/status

# The following definition allow the min, max and minallowed free space to be defined
# at the disk server level respectively. In releases prior to 2.1.3 this had to be
# done via manual SQL statements in the database

RmNode 	 MinFreeSpace 		.10
RmNode 	 MaxFreeSpace 		.15
RmNode 	 MinAllowedFreeSpace 	.05

# scheduler (LSF 7.0 plugin)
#
# The following values define the predicated cost of starting a new job on a
# filesystem once that filesystem has been selected to run a job. A predicated value 
# is  required to compensate for the latency of monitoring information between the 
# client and rmMasterDaemon. So for example, if a job is selected to run on filesystem
# A for a read the monitoring information for the number of read streams will be
# incremented by the value of jobNbReadStreamCost. 

SchedCoeffs     jobReadRateCost          0     # bytes/second
SchedCoeffs     jobWriteRateCost         0     # bytes/second
SchedCoeffs     jobNbReadStreamCost      0
SchedCoeffs     jobNbReadWriteStreamCost 1
SchedCoeffs     jobNbWriteStreamCost     0

# The expected average file size in bytes (used to estimate the amount of data to be
# written by ongoing streams). Units like k M G T P can be used

SchedCoeffs     ExpectedAverageFileSize  2G

# In version 2.1.3 message box 4 from the LSF plugin (used to tell the client which
# machine and filesystem to use) was replaced by notification files stored on a
# shared filsystem. This alternative approach removes the need to suspend jobs on
# entrance to LSF, lowers the required number of communications with LSF and increases
# performance.
#
# For environment where a shared/common filesystem is unavailable, the files can be
# stored locally and made accessible to clients via a web server
#
# The following definition defines the location where the plugin should write the 
# notification files. The directory should be writeable by lsfadmin

Sched   SharedLSFResource       /var/www/html/lsf

# The schedule selects the best filesystem to use based on policies defined in an
# external Python module. The location of the module is defined in this definition

Sched	PolicyModule		/etc/castor/policies.py

# The location of the Python shared object library. This is only necessary for SLC4 
# headnodes.
#Sched	DynamicPythonLib	/usr/lib/libpython2.3.so

# The following option defines how long a job is allowed to remain in a queue 
# waiting for resources before being killed by the job manager. For convenience, a
# svcclass name of "all" can be used to define a default value for all service
# classes.
#
# The format of the value is:
#   <svcclass1>:<timeout1> [svcclass2:timeout2[...]]

#JobManager	PendingTimeouts		default:120

# Should the job manager kill jobs whose resource requirements e.g requested 
# filesystems can no longer be satisfied ? [yes|no]. The default is no.

#JobManager     ResReqKill      yes

# The delay in seconds between failed LSF calls to kill a job inside the job manager.
# Note: the value must be greater then 10 seconds

JobManager	KillRetryInterval	30

# Should the job manager perform reverse uid lookups on all new job requests? [yes|no]
# The default value is yes.

#JobManager     ReverseUidLookups       yes

# In order to improve submission speeds into LSF the job manager pre creates a number
# of child processes which are instructed to submit jobs into LSF. This eliminates
# the need to fork a independent process for every submission.
#
# Note: the value must be less then 200. Setting the value too high could kill the
#       LSF master, the default is 2

JobManager	PreforkedWorkers	2

# As notifications files can be served from a web server it is necessary to also
# define the location where the Job (stagerJob) should go to retrieve the file. The
# location must be prefixed with file:// or http:// to denote whether the file is on
# a shared filesystem or served via web server. E.g. file:///usr/share/lsf/castor_job

JobManager      SharedLSFResource       http://castorscheduler.cern.ch/lsf

# Allow the stager job to print debug information. An additional log file must be
# defined if you want to see the output of STAGER_TRACE. Note: don't forget to define 
# the LOGDEBUG severity level for the job facility in the logging section.

#Job	Debug   yes
#Job	LogFile /var/spool/job/log

# The frequency at which the GC daemon runs on the client/diskserver in seconds. The
# value should be larger then or equal to the frequency at which the Garbage
# Collection job runs internal inside the stager database. 

GC 	INTERVAL 	300

# Cleaning Daemon
# 
# The maximum amount of time in days that a subrequest can remain in the database
Cleaning 	DAYS4OUTDATE 	3

# The maximum amount of time in hours that an achieved request can stay in the
# database.
Cleaning 	HOURS4ARCHIVED 	24

# The frequency in minutes between two operations for cleaning out of date requests
Cleaning 	PERIODOUTDATE 	240

# The frequency in minutes between two operations for cleaning archived requests
Cleaning 	PERIODARCHIVED 	60


## Transfer Protocols ###############################################################

# Enable this variable only in diskservers running xrootd to point to the directory
# where bin/XrdCS2e is located .

#XROOT XROOTSYS /xroot/ 

#
# RFIO/RFIOD
#
RFIO	USE_CASTOR_V2	YES		# USE CASTOR2 with castor2 client or NOT
RFIO	CONNTIMEOUT	10		# Timeout on the connect() system call
RFIO	CONRETRY	10		# Number of connection retry
RFIO	CONRETRYINT	1		# Number of second between every retry
RFIO	DAEMONV3_RDSIZE	2097152		# Daemon buffer size for read
RFIO	DAEMONV3_WRSIZE	2097152		# Daemon buffer size for write
RFIOD 	TCP_NODELAY 	YES		# Use TCP_NODELAY on the server side
RFIOD	KEEPALIVE	YES		# Use KEEPALIVE socket option
#RFIOD	DIRECTIO	YES		# Enabled O_DIRECT support to bypass kernel page cache
                                        # experimental, please contact castor dev for details
#RFIOD	XFSPREALLOC	1024		# Enables preallocation for XFS
                                        # experimental, please contact castor dev for details

RFIOD	DEBUG		NO
RFIOD	LOGFILE		/var/spool/rfio/rfiod.log

RFIOD 	WTRUST 		castoradm4 
RFIOD 	RTRUST 		castoradm4
RFIOD 	XTRUST 		castoradm4
RFIOD 	FTRUST 		castoradm4

#
# GSIFTP
#
# should be redefined if they are not like default 
#GSIFTP GLOBUS_LOCATION  /opt/globus                            # the root dir for globus installation
#GSIFTP GRIDMAP          /etc/grid-security/grid-mapfile-stage	# grid mapfile to be used
#GSIFTP LOGFILE          /var/spool/gridftp/log			# log file for the gridftp server
#GSIFTP NETLOGFILE       /var/spool/gridftp/netlog		# netlog file for traffic calculation
#GSIFTP LOGLEVEL         INFO					# loglevel for the log file
                                                                # one of following ERROR, WARN, INFO, DUMP, ALL
#GSIFTP CONTROL_TCP_PORT_RANGE  20000,21000                     # port range for control ports min,max
#GSIFTP DATA_TCP_PORT_RANGE     20000,21000                     # port range for data ports min,max
                                                                # min>1023, max<65536
# key and cert files must be chown stage:st
#GSIFTP X509_USER_CERT   /etc/grid-security/hostcert-stage.pem
#GSIFTP X509_USER_KEY    	/etc/grid-security/hostkey-stage.pem

## Tape #############################################################################

ACCT    RTCOPY          YES             # Rtcopy accounting
ACCT    TAPE            YES             # Tape accounting

RTCOPY  RETRY           12              # Number of TMS retries (obsolete)
RTCOPY  SLEEP           300             # Sleep between every TMS retry (obsolete)
#RTCOPYD        NB_BUFS 300             # Number of rtcopy buffers for a 2G machine (1G machine? put 160)
#RTCOPYD        BUFSZ   4194304         # Size of a buffer (4M is the default)

# Only useful if the catalog contains references to disk files that have been
# physically deleted, which should normally never be the case.

migrator        CHECKFILE       NO

# If CHECKFSEQ is set to YES the migrator will check the start file sequence number
# (FSEQ) of the tape given by VMGR with the last FSEQ known by the CASTOR name server.
# This is an extra check to make sure that the VMGR and CASTOR name server are
# consistent.

migrator        CHECKFSEQ       NO

# Not needed unless one suspects a problem with the recall of multi-segment files.

recaller        CHECKFILE       YES

# Config the drive down in case of a tape alert (YES/NO). 
#
# If the option is omitted, the default value (YES) will be used.

TAPE    DOWN_ON_TPALERT         YES

# What to do in case of a bad MIR on load. The valid options are:
#   REPAIR : SPACE to EOD, REWIND
#   IGNORE : simply go on
#   CANCEL : cancel the request
#
# If the option is omitted, the default value (CANCEL) will be used.

TAPE    BADMIR_HANDLING         REPAIR

# Let tpdaemon confirm its idle state to VDQM at certain intervals
# (this is to cleanup inconsistencies where VDQM sees a drive busy,
# while tpdaemon reports the drive to be idle)
#
#   CONFIRM_DRIVE_FREE       : switch on/off this update mechanism
#   CONFIRM_DRIVE_FREE_INTVL : check/set interval in seconds (default: 900)
#
# If the option is omitted, the default value (NO) will be used.

TAPE    CONFIRM_DRIVE_FREE              NO
#TAPE   CONFIRM_DRIVE_FREE_INTVL        900


## Logging ##########################################################################
#
# EMERGENCY  : Not used
# ALERT      : Very important error (e.g. service cant start)
# ERROR      : All errors during normal operation
# WARNING    : Self-monitoring warning
# AUTH       : Authorization error (e.g. Cupv failed)
# SECURITY   : Csec error
# USAGE      : Trace of routine calls
# SYSTEM     : Normal service messages (cf. old LOG_INFO)
# IMPORTANT  : Not used
# MONITORING : Monitoring and statistics information
# DEBUG      : Debug level
#
# For convenience a LOGSTANDARD severity is provided which logs everything locally
# excluding DEBUG messages and does not record USAGE message remotely

Cleaning 	 LOGSTANDARD file:///var/spool/cleaning/log x-dlf://castordlf.cern.ch/	
GC               LOGSTANDARD file:///var/spool/gc/log x-dlf://castordlf.cern.ch/
job 		 LOGSTANDARD file:///var/spool/job/log x-dlf://castordlf.cern.ch/
JobManager       LOGSTANDARD file:///var/spool/jobmanager/log x-dlf://castordlf.cern.ch/		
MigHunter 	 LOGSTANDARD file:///var/spool/rtcpclientd/MigHunter x-dlf://castordlf.cern.ch/	
migrator 	 LOGSTANDARD file:///var/spool/rtcpclientd/migrator x-dlf://castordlf.cern.ch/	
recaller 	 LOGSTANDARD file:///var/spool/rtcpclientd/recaller x-dlf://castordlf.cern.ch/	
repack 		 LOGSTANDARD file:///var/spool/repack/log x-dlf://castordlf.cern.ch/
rfio 		 LOGSTANDARD file:///var/spool/rfio/log x-dlf://castordlf.cern.ch/
RequestHandler   LOGSTANDARD file:///var/spool/rhserver/log x-dlf://castordlf.cern.ch/
RmMaster 	 LOGSTANDARD file:///var/spool/rmmaster/monitoring x-dlf://castordlf.cern.ch/	
RmNode           LOGSTANDARD file:///var/spool/rmnode/log x-dlf://castordlf.cern.ch/
rtcpcld          LOGSTANDARD file:///var/spool/rtcpclientd/rtcpcld x-dlf://castordlf.cern.ch/
scheduler 	 LOGSTANDARD file:///var/spool/scheduler/log x-dlf://castordlf.cern.ch/	
stager		 LOGSTANDARD file:///var/spool/stager/log x-dlf://castordlf.cern.ch/
TapeErrorHandler LOGSTANDARD file:///var/spool/rtcpclientd/TapeErrorHandler x-dlf://castordlf.cern.ch/		
Vdqm             LOGSTANDARD file:///var/spool/vdqm/log x-dlf://castordlf.cern.ch/


#####################################################################################

#
# Service mapping - which type of database should be used for a given service
#

SvcMapping             DBCNV           16 #SVC_ORACNV
SvcMapping             DBRHSVC         42 #SVC_ORARHSVC
SvcMapping             DBSTAGERSVC     18 #SVC_ORASTAGERSVC
SvcMapping             DBTAPESVC       19 #SVC_ORATAPESVC
SvcMapping             DBFSSVC         20 #SVC_ORAFSSVC
SvcMapping             DBJOBSVC        21 #SVC_ORAJOBSVC
SvcMapping             DBGCSVC         22 #SVC_ORAGCSVC
SvcMapping             DBQUERYSVC      23 #SVC_ORAQUERYSVC
SvcMapping             DBVDQMSVC       24 #SVC_ORAVDQMSVC
SvcMapping             DBCLEANSVC      37 #SVC_ORACLEANSVC
SvcMapping             DBRMMASTERSVC   38 #SVC_ORARMMASTERSVC
SvcMapping             DBJOBMANAGER    40 #SVC_ORAJOBMANAGERSVC

#
# Library definition (dlopen) - which library holds which service
#

DynamicLib             ORACNV         libcastorCommonOra.so.2.1
DynamicLib             ORARHSVC       libcastorCommonOra.so.2.1
DynamicLib             ORASTAGERSVC   libcastorCommonOra.so.2.1
DynamicLib             ORATAPESVC     libcastorCommonOra.so.2.1
DynamicLib             ORAFSSVC       libcastorCommonOra.so.2.1
DynamicLib             ORAJOBSVC      libcastorCommonOra.so.2.1
DynamicLib             ORAGCSVC       libcastorCommonOra.so.2.1
DynamicLib             ORAQUERYSVC    libcastorCommonOra.so.2.1
DynamicLib             ORACLEANSVC    libcastorCommonOra.so.2.1
DynamicLib             ORAVDQMSVC     libcastorVdqmOra.so.2.1


# End-of-File
