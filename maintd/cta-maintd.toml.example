# SPDX-FileCopyrightText: 2026 CERN
# SPDX-License-Identifier: GPL-3.0-or-later

####################################################################################################
#
# CTA Maintd Service Configuration
#
####################################################################################################

[experimental]

telemetry = false

[catalogue]
# Unique string to identify CTA's instance the maintd process is serving (i.e: production, preproduction).
# namespace = "production"

# Path to the CTA Catalogue configuration file
config_file = "/etc/cta/cta-catalogue.conf"


[scheduler]

# URL of the objectstore (CTA Scheduler Database). Usually this will be the URL of a Ceph RADOS
# objectstore. For testing or small installations, a file-based objectstore can be used instead.
objectstore_backendpath = "rados://cta@tapecta:cta"

# Defaults to 600 seconds if not set.
tape_cache_max_age_secs = 600

# Defaults to 10 seconds if not set.
retrieve_queue_cache_max_age_secs = 10

[logging]
# Logs with a level lower than the LogMask value will be masked. Possible values are EMERG, ALERT,
# CRIT, ERR, WARNING, NOTICE (USERERR), INFO, DEBUG.
level = "INFO"

# The following key-value pairs are added to all log lines
# All keys are optional as this is purely for monitoring purposes
attributes = {
  # Unique string to identify CTA's instance the maintd process is serving (i.e: production, preproduction).
  # Each of these instances should be associated with a specific CTA catalogue instance.
  instance_name = "production"

  # The unique string to identify the backend scheduler resources. As an example, it can be structured as:
  # "[ceph|postgres|vfs][User|Repack]".
  scheduler_backend_name = "cephUser"
}

# This is what is used to connect to the disk instance
[xrootd]

# Overrides XrdSecPROTOCOL
security_protocol = "sss"

# Overrides XrdSecSSSKT
sss_keytab_path = "/etc/cta/cta.sss.keytab"

[routines]

# All routines run sequentially: A -> B -> C -> sleep -> A -> B -> C -> sleep ...
# This value defines the sleep interval between all routines
global_sleep_interval_secs = 1

# Disk reporting
disk_report_archive  = { enabled = true, batch_size = 500,  soft_timeout_secs = 30 }
disk_report_retrieve = { enabled = true, batch_size = 500,  soft_timeout_secs = 30 }

# Garbage collection
garbage_collect = { enabled = true }

# Queue cleanup
queue_cleanup = { enabled = true, batch_size = 500 }

# Repack
repack_expand = { enabled = true, max_to_expand = 2 }
repack_report = { enabled = true, soft_timeout_secs = 30 }

# Postgres-only queue cleanup
# user_active_queue_cleanup   = { enabled = true, batch_size = 1000, age_for_collection_secs = 900 }
# repack_active_queue_cleanup = { enabled = true, batch_size = 1000, age_for_collection_secs = 900 }

# user_pending_queue_cleanup   = { enabled = true, batch_size = 1000, age_for_collection_secs = 900 }
# repack_pending_queue_cleanup = { enabled = true, batch_size = 1000, age_for_collection_secs = 900 }

# scheduler_maintenance_cleanup = { enabled = true, batch_size = 1000, age_for_deletion_secs = 1209600 }

[telemetry]
# Used to control cardinality.
# If set to false, each restart of a process will generate a new unique ID for the `service.instance.id`.
# If set to true, `service.instance.id` remains constant across restarts.
retain_instance_id_on_restart = false


[telemetry.metrics]

# Metrics backend to use. Possible options are NOOP, OTLP_HTTP, OTLP_GRPC, STDOUT, FILE
# Default is NOOP, meaning no metrics are collected/exported
backend = "NOOP"

# Amount of time between exports
interval_ms = 15000
# timeout for a single export
timeout_ms = 3000

[telemetry.metrics.otlp]
# Service location of the OTLP collector in case the OTLP backend is used
endpoint = "endpoint:port"
# File location containing "username:password" (not base64 encoded) to set up basic auth for push metrics over HTTP
# Adds the header "authorization: Basic <base64(username:password)>"
basic_auth_file = "/path/to/username:password"
