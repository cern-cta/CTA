------------
- 2.1.3-23 -
------------

  Bug Fixes
  ---------
  - #27196 request waiting on D2D copy
  - #26789 streamsToDo returns streams with nothing to do
  - #27206 bad decision to STAGEIN if diskcopy in CANBEMIGR in disabled filesystem
  - #27302 spacetobefreed has negative values
  - #27564 MigHunter startup does not check for duplicates
  - #27130 rmNodeDaemon problems on start-up
  - #27507 recaller crash for old castor files without checksum
  - 'volume in use' after EBUSY leading to disabled tapes
  - Streamlined logging output in case of the new 'Bad MIR'
  - getHostByName not thread safe, as well as the CASTOR clients using it
  - #27794 cleaning daemon too slow to cope with heavy loads
  - internalPutDoneFunc suffered from bad execution plans from ORACLE
  - corrupted ns hostname logging in repack
  - broken http downloads in stagerJob due to unintialised variable

  Upgrade from 2.1.3-17
  ---------------------
  - GRANT new privileges on the DLF DB. Here is an example of what to run,
    has to be modified for each site :
      alter system set db_create_file_dest='/ORA/dbs03/oradata/CASTORDLF/' scope=both ;
      grant create tablespace to castor_dlf;
      grant unlimited tablespace to castor_dlf;
      grant alter tablespace to castor_dlf;
  - Apply the SQL upgrade script to the dlf DB
  - Stop all daemons
  - Apply the SQL upgrade script to the stager DB
  - Upgrade all nodes
  - Restart all daemons

------------
- 2.1.3-17 -
------------

  New Features and Changes
  ------------------------
  - Better naming of version and releases within the stager DB
  - introduced partitionning the subrequest table. This allows to avoid
    function based indexes and thus to shrink tables dynamically
  - Fixed updating of the stager database for monitoring information when
    a diskserver was deleted via rmAdminNode
  - Fix garbage collector in case of FileSystems with no GC policy
  - Added check of the version of the dlf schema at startup of the dlf server
  - fixed handling of command line options in modifyFileClass
  - flag tape as unused in resetStream when stream is in WAITDRIVER
  - fixed signal handling in rtcpclientd and reset streams with no active
    process on shutdown.
  - Added an automatic repair of corrupted tape directories (MIR).
    The repair is done by SPACE to EOD and REWIND SCSI commands.
    The handling of bad MIRs is controlled via the new option BADMIR_HANDLING.
  - Adapted repack to castor 2.1.3

  Upgrade from 2.1.3-15
  ---------------------
  - Stop all daemons
  - Apply the SQL upgrade script to the stager DB
  - Upgrade all nodes
  - Restart all daemons

------------
- 2.1.3-15 -
------------

  New Features and Changes
  ------------------------
  * Fixed in stager_qry for the case of a file with no diskcopy and no subrequest but which still has a CASTOR file.
    We were answering STAGED instead of File Not Found in previous releases.
  * Added more retries on system and tape errors
  * Fixed iptable rules for the range of open ports on a Castor client (port 30101 was open by mistake)
  * Use new error code ERTWRONGSIZE to flag tape recalls with wrong filesize
  * Added support for separate stager trace and rfiod log files
  * Avoided that segments stay in SEGMENT_SELECTED if filesize is wrong on a tape recall
  * Avoided deadlock with castor2 clients after errors in Ctape_reserve() or Ctape_mount()
  * Expanded the list of customised pending errors to help in operational debugging
  * Fixed missing Python 2.3 symbols at runtime
  * Changed signal handling to prevent mount/dismount race in case of a killed mount request.
    This should fix at least some of the 'volume in use' cases and the 'Destination Element Full' problems.

  Upgrade from 2.1.3-14
  ---------------------
  - There is no need to stop the service for this upgrade
    and the order in which the update steps are taken does
    not matter
  - Upgrade all head nodes and tapeServers
  - Apply the SQL upgrade script

------------
- 2.1.3-14 -
------------

  New Features and Changes
  ------------------------
  They are so numerous here that they were divided into categories

    Broad view and transverse items
    -------------------------------
    - the monitoring has been fully rewritten, see details below
    - the LSF plugin has been fully rewritten, see details below
    - clips was dropped, python replaces it
    - fully qualified domain names are used everywhere
    - all executables and libraries have a new symbol storing the CASTOR version number.
      just do an "nm <exe or lib file> | grep CastorVersion" to get it
    - putDones are not scheduled anymore
    - the maximum file name length in the nameserver was changed from 231 to 255 characters
    - garbage collection on diskservers was "desynchronized". The point is to avoid that all GCs
      start concurrently both for load and smoothing reasons

    Monitoring
    ----------
    - rmmaster daemon is now only doing job scheduling and is not used anymore for monitoring
      although it is still active. As a consequence, rmgetnodes won't give anything
    - as a corrolate, rmnode is no more distributed and does not run anymore on the diskservers.
      rmadminnode should not be used anymore and the fileSystem service of the stager was dropped.
    - the new monitoring daemon is called rmMasterDaemon and should run on the scheduler node.
      It is distributed in the same package castor-rmmaster as rmmaster so this package should be
      installed on both the scheduler node and the rmmaster node even if both daemons are not
      needed in both places.
    - rmMasterDaemon uses shared memory to store the data. This set of memory will survive it
      in case of crash and be reused when restarting. In case of reboot, it can be reinitialized from
      the data contained in the stager DB, which are constantly updated.
    - the new command lines are rmAdminNode and rmGetNodes (the second one being only available on
      the scheduler node).
    - the new monitoring daemon is called rmNodeDaemon and is distributed in castor-rmnode. On top
      of monitoring the node and sending data to the rmMasterDaemon, it also keeps the current status
      of the node in the daemon in /etc/castor/status (default). It initializes the shared memory when
      starting by reseting the metrics, setting statuses to Down and Admin statuses to what they are
      in the stager DB.
    - the new monitoring monitors :
      - for each diskserver : ram(total+free), memory(total+free), swap(total+free),
        nbRead/ReadWrite/Write/Recaller/MigratorStreams, read/writeRate, status and adminStatus
      - for each filesystem : space(total+free), nbRead/ReadWrite/Write/Recaller/MigratorStreams,
        read/writeRate, status and adminstatus
    - the status can be Production, Down, Draining.
    - the admin status can be None, Force or Deleted. If None, the status in the master is updated according
      to monitoring. It Force, it is not updated. If Deleted, the diskServer/FileSystem is dropped from the
      stager database and will disappear at the next reset of the shared memory (e.g. reboot).
    - the new monitoring system is able to disable diskservers that did not send data for too long

    Around LSF
    ----------
    - there is a new LSF plugin with same name and same interface as the old one.
    - it uses shared memory to get data from the rmMasterDaemon and thus has to run on the same machine
    - this shared memory is also accessed by rmGetNodes that basically dumps it
    - In case of corruption, the way to reset the shared memory is to stop rmMasterDaemon and LSF and
      use ipcrm. the key used is 0x946
    - scheduling policies are now defined in python and can be defined per svcclass.
      The python code is precompiled at plugin start in order to speed up the execution.
      Policies can be updated with no service interuption by reloading the plugin (badmin reconfig).
      Default policy takes only the number of streams per filesystem into account.
    - CASTOR now uses multiple queues in LSF, namely one queue per svcclass
    - we don't use message boxes anymore inside LSF. The first one was dropped while the second one is
      replaced by file sharing, either via distributed filesystem or via a web server running on the
      scheduler machine. This also implies that jobs are no longer suspended and resumed.

    Around the stage DB
    -------------------
    - all indexes and constraints in the stager DB schema are now properly named
    - reservedSpace has been dropped from the filesystem table. It is replaced by loose heuristics
      leading to imprecisions on the actual and expected free space but avoiding any accumulation
      problems.
    - the nullGCPolicy does not exist anymore since "INVALID" diskcopies are cleaned up independently.
      In order to not use any garbage collection policy, an empty name should be used.
    - the way to choose the best filesystem to be used for migration (migration policy) has changed
      to favor the reuse of filesystems. Namely, filesystems will be reused during 15 minutes by
      migrators (as long as there are candidates and the filesystems are in Production). This allows
      the scheduler plugin to adapt its behavior by reducing the number of concurrent reads and has
      very good effects on the migration speed.
    - many schema changes took place. Here is an overview of the biggest ones :
       - min/max/minAllowedFreeSpace were dropped. They now reside in the monitoring shared memory
         of the new rmMasterDaemon and are defined in the castor.conf of the diskServers
       - the load of diskServers and the weight of fileSystems have been dropped. They are replaced by
         nbRead/ReadWrite/Write/Recaller/MigratorStreams and read/writeRate. The values stored in the
         database are actually only copies from the mnitoring shared memory that is the primary source.
       - procedure bestFilesystemForJob is gone, replaced by the python policies of the scheduler plugin.
       - reservedSpace disappeared from the filesystem table as already mentionned.
       - the stream table has new columns : lastFileSystemChange, lastFileSystemUsed and lastButOneFileSystemUsed.
         This allows the migration policies to be more clever (see above).

    Tape related
    ------------
    - Extended tpdaemon to use proper DFL logging
    - Concerning bad MIRs, take the operator out of the loop and rely on the healing capabilities of the
      drive during the next access. This can be controlled by new option TAPE CANCEL_ON_BADMIR.
    - a restriction was added on the repack server to dismiss users with 0 id
    - the repack -a has been modified. "repack -a Vid" is used to archive a specific finished tape
      while "repack -A" archive all the finished one.
    - repack -x has been extended to give information about the different segments that were on a
      repacked tape querying the name server.
    - A new couter has been added to repack with the STAGED files (properly migrated) and it will be
      updated by the monitoring. At the end the last snapshot is taken before deleting the subrequest.

    Bug Fixes
    ---------
    - #26534  GC policies are not taking gcWeight attribute into account
    - #26489  Allow results to be displayed in reverse chronological order
    - #26177  putDone does not to update the filesize in the nameserver?
    - #26176  Accumulation of orphaned subrequests in SUBREQUEST_WAITSUBREQ (5)
    - #26135  Customized pending reason number 20003
    - #26060  enterSvcClass should fail if it detects illegal options
    - #25344  misleading error message from rfcp
    - #25029  reservedSpace leak for SRM 'put' request cycles
    - #24864  Tape recalls do not work properly on 2.1.2.6 (itdc)
    - #24407  errcode == 32102 doesn't resurrect
    - #23961  Tape removed from vmgr makes rtcpclientd loop
    - #23795  UID zero checking 
    - #23794  FAILING file count
    - #23793  repack -a
    - #23709  Kill and terminate a repack session 
    - #23705  repack -s status information
    - #21953  problem with subdirectory creation on diskservers 
    - #21823  putDone failure not well handled if file does not exist on diskserver
    - #21745  RFE: CASTOR to support FQDN internally for disk servers
    - #20353  The 'no valid copy found on tape' instead of 'Internal error' is still not fixed!
    - #20079  filesystems not being updated are not taken out of production!
    - #17153  LSF job can remain in PSUSP after a timeout between stager - rmmaster
    - #15832  RMMASTER does not keep node states
    - #11550  Leak in stager
    - #1288   rfmkdir -p
    - the default values for the cleaning daemon time intervals were fixed
    - a PutDone is now accepted on a disabled diskserver
    - read only jobs go through whatever the space situation is. However "false" read jobs
      (read jobs that would trigger disk2diskCopy or recalls) don't
    - fixed automatic renewal of the database connection for all hand-written services
    - regular retries on disabled or archived tapes
    - jump over files with bad size in tape recalls. This avoids looping blocking other correct files.
      There will still be an alert log for the wrong file.
    - fixed tape stuck in START problem. The code ignores consistency problems with the status of a tape in VMGR.
    - fix for the device or resource busy problem leading to put all drives down. A simple retry worked here.
    - fix tape stuck in UNKNOWN status in case of tape alert. The new parameter DOWN_ON_TPALERT allows
      to configure whether to put the drive down or to ignore the alert.
    - in case of time out in contacting the stager the  repack subrequest is not ending up into "FAILED" status.

  Upgrade from 2.1.1-*
  --------------------
    - you basically have to update to 2.1.2-x (with x >= 4) and then follow the instructions for updating from 2.1.2-*.
    - you can avoid to update the RPMs
    - concerning the stager database updates :
       - if you start from a 2.1.1-4, first run 2.1.1-4_to_2.1.1-9
         http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.1-9/upgrades/
       - from a 2.1.1-9, update to 2.1.2-4 using 2.1.1-9_to_2.1.2-4
         http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.2-*/2.1.2-4/upgrades/
    - concerning the DLF database :
       - if starting from a 2.1.1, drop and recreate the DLF DB.
         The easiest is to use the 2.1.3 drop script (a generic one that can be applied to any version)
         and recreate the DLF schema using the 2.1.2-4 script
         (both can be found on http://castor.web.cern.ch/castor/software.htm)

  Upgrade from 2.1.2-*
  --------------------
    - first make sure that no LSF jobs stay in the queue. bkill all the remaining ones
    - stop all daemons and services (rhserver, stager, rtcpclientd, cleaningDaemon, rmmaster, dlfserver, expertd, rmnode, gcdaemon, LSF)
    - optionnaly backup your stager DB (one never knows...)
    - upgrade the stager database
       - first save somewhere the values of min/max/minAllowedFreeSpace for all filesystems. They will needed
         when upgrading the diskservers.
       - use script 2.1.2-*_to_2.1.3-14 (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.3-14/upgrades)
    - upgrade the diskServer names in the stager database to include the fully qualified domain :
         UPDATE DiskServer SET name = name || '.cern.ch';
         COMMIT;
    - update DLF DB using script dlf_2.1.2-x_to_2.1.3-x
       (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.3-14/upgrades)
    - update the rpms on the different nodes. Some changes were made in the list of packages to install :
       - on the scheduler machine, castor-rmmaster and castor-lib-monitor should be added
       - on the diskservers, castor-lib-monitor should be added
       - clips can be dropped from the expert machine
    - update your monitoring tools according to the daemon changes :
       - on the diskServer, rmnode was replaced by rmNodeDaemon
       - on the scheduler machine, rmMasterDaemon was added
    - upgrade to LSF7 and modify the LSF configuration files :
       - use fully qualified machine names :
          - update machine names accordingly in all config files
          - remove LSF_STRIP_DOMAIN from lsf.conf on all machines
       - define one queue per svcclass in lsb.queues
       - remove or increase the 1000 jobs limitation in lsb.params
          - e.g. CERN has a 30000 limit on the number of jobs
       - allow for extsched option in job submission :
          - add LSF_ENABLE_EXTSCHEDULER in lsf.conf of the scheduler and rmmaster machines
    - define python scheduling policies :
       - create the policy python file from the .example one (/etc/castor/policies.py.example)
       - optionnaly modify it to adapt policies
       - optionnaly define different policies for different service classes
    - update the CASTOR configuration files on all machines
       - update castor.conf using the template giving in castor.conf.example
       - define the way to share files between the scheduler machine and the diskservers
         - on the scheduler machine, define the Sched/SharedLSFResource option giving the place where to put files
           Note that this ca be either a shared filesystem or a local directory accessible through a web server
           In both cases, lsfadmin needs write access to the directory
         - on the rmmaster machine, define the Job/SharedLSFResource option giving the place where diskservers
           can find the files. Again, this can be either a shared filesystem or an http location (e.g.
           dir:/var/lsf/sharedfiles or http://castorscheduler.cern.ch/lsf)
       - on diskservers :
         - edit castor.conf to define the proper name for the RM/HOST option (scheduler machine)
         - edit castor.conf to define proper mountPoints in the RmNode/MountPoints option. e.g.
           RmNode MountPoints /srv/castor/01/ /srv/castor/02/ /srv/castor/03/
         - optionnaly define option RmNode/StatusFile in castor.conf. The given file will contain the filesystem
           and node status as seen by the central servers, constantly updated
         - if needed, edit castor.conf to define the min/max/minAllowedFreeSpace according to the values
           saved from the stager DB. Default values are .10, .15 and .05
    - restart all daemons
       - on diskservers : rmNodeDaemon, gcDaemon
       - on scheduler : rmMasterDaemon, LSF
       - on other nodes, nothing changed (including on rmmaster, the old rmmaster must still run)
    - enable diskservers one by one using rmAdminNode :
         rmAdminNode -n <fully qualified node name> -r -R

-----------
- 2.1.2-10-
-----------

  New Features and Changes
  ------------------------
  This release brings selected fixes and no new functionality with respect to the previous 2.1.2:
  - Fix the back signal handler modifications from 2.1.3

  Upgrade from 2.1.2-9
  --------------------
  - Upgrade all head nodes (the order is not important)

-----------
- 2.1.2-9 -
-----------

  New Features and Changes
  ------------------------
  This release brings selected fixes and no new functionality with respect to the previous 2.1.2:
  - Fixes for reservedSpace accumulation in case of failed Puts and Puts from SRM
  - Fixes in the stager to prevent segmentation faults in case of exception handling for tape recalls
  - Fix in the scheduling of disk-to-disk copies
  - Fix for the stagemap handling when only the user gid is mapped in stagemap.conf

  Upgrade from 2.1.2-6/7
  ----------------------
  - Upgrade the database 
  - Upgrade all head nodes (the order is not important)


-----------
- 2.1.2-7 -
-----------

  This release is broken in disk-to-disk copy handling.


-----------
- 2.1.2-6 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes:
   - Fixed potential double/invalid frees in the database access layer
   - Fixed error checking in the PrepareToGet in case of tape recalls
   - Bug Fixed in repack worker and monitoring now is able to deal with the stager timeout.
   - Fixed the socket leak in DLF client when the DLF server is not reachable
   - Fixed the stagerJob for xroot. We were calling prepareForMigration while xroot does it for us.
   - Optimized version of stage_qry
   - Fixes for the processing of update requests
   - Fixed the reservedSpace accumulation problem
   - Fixed garbage collector job
  
  Upgrade from 2.1.2-4
  --------------------
  - Upgrade the database 
  - Upgrade all head nodes (the order is not important)


-----------
- 2.1.2-4 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes and improvements:
  - A PrepareToGet request is not scheduled anymore if the requested file is to be recalled from tape
  - Bugfixes and finalization of the Repack system
  - Implemented Repack check and Repack undo
  - Fixes around the DLF and the web interface
  - Fixed all known database deadlocks to date
  - Reconnection to the db more robust in case of transient failures, many more are recognized now
  - Cleaned NS clients and CUPV code for 64bit
  - Many optimizations in the PL/SQL code
  - Minor fixes to be able to compile the C++ part with gcc-3.4 and gcc-4.1 without warnings
  - Fixed a problem in TapeErrorHandler to allow tape retries upon error
  - Fixed query used by the cleaningDaemon to delete old failed subRequests
  - Bugfix for #22620: Uncaught C++ exception causes TapeErrorHandler to Abort
  - Bugfix for #22382: rtcpclientd does not stop gracefully upon signal
  - Bugfix for #21824: internalPutDoneFunc tries to delete the prepareTo request too hard
  - Bugfix for #21541: change location of RTCOPY
  - Bugfix for #21482: bad values on defaultFileSize when using modifySvcClass
  
  Upgrade from 2.1.1-9
  --------------------
  - Stop all the CASTOR daemons
  - Upgrade the database; new database code does not work with old stagers
  - Upgrade all nodes (the order is not important) and restart the service


-----------
- 2.1.1-9 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes and improvements:
  - Improvements around the Repack system, added Repack multi-stager ability
  - Bugfix for #21093: strange files created on local disk, when DLF is not able to log
  - Bugfix for #20069: if user has the wrong (non existent) svcclass there is no intelligible error reported
  - Bugfix for very large files
  - Improved selection of migration candidates via PL/SQL procedure
  - Improved port handling in clients in case of many concurrent callbacks on a small port range
  - Improved stager_qry in case of castorFiles with many diskCopies
  - Bugfix for stager_qry (#19044, #19836)
  - Improved logging in the Request Handler
  - Support for 700Gb tapes

  Upgrade from 2.1.1-4
  --------------------
  - Upgrade the database
  - Upgrade all nodes (the order is not important).


-----------
- 2.1.1-4 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes:
  - Fixes around the Repack system
  - Improved cleaning procedures and config for the cleaningDaemon, now it shares castor.conf
  - Fixed Oracle deadlock which was leading to many diskcopies left in status 9
  - Fixed Oracle deadlock in streams leading to inconsistencies in NbTapeCopiesInFS
  - Fixes in the migrator
  - Improved GC trigger to resurrect the GC job if it previously died
  - Improved stagerJob to prevent it to loop forever for a missing msgbox 4 from LSF

  Upgrade from 2.1.1-0
  --------------------
  - Upgrade the database
  - Upgrade the head nodes and the diskservers (the order is not important).


-----------
- 2.1.1-3 -
-----------

  New Features and Changes
  ------------------------
  This release brings the new Repack for CASTOR2 system, and a number of bug fixes:
  - Fixed leak of stager_rm subrequests in status ready
  - Fixed deadlock in the updateFsFileOpen/Close SQL methods
  - Fixed stager_qry for prepareToPut requests
  - Fix concerning retrieval of castorversion from stagemap if the env variable is not set
  - Take '*' as a wildcard for the svcClass in a stager_qry
  - Changed compression factor for IBM3592 in order to activate NVC bit
  - Included modifications for xrootd

  Upgrade from 2.1.1-0
  --------------------
  - Upgrade the head nodes
  - Then for the other nodes no special order is required.


-----------
- 2.1.1-2 -
-----------

  - This release is broken in the rtcpclientd.


-----------
- 2.1.1-1 -
-----------

  New Features and Changes
  ------------------------
  This release is exactly the same as the 2.1.1-0 except for a minor fix on the DLF server.
  It has been rebuilt to widely deploy it at CERN, as the first attempt for the 2.1.1-0 had
  a major problem leading to backward incompatibilities on the client.
  Note that the RPMs in the castor download area are correct for both 2.1.1-0 and 2.1.1-1.

  Upgrade from 2.1.1-0
  --------------------
  Not needed. If you're running an older version, please follow upgrade instructions below.


-----------
- 2.1.1-0 -
-----------

  New Features and Changes
  ------------------------
  This is a major release with many fixes and features. See the changelog for all the details.
  The most important new features are:
  - New rewritten version of DLF
  - Resurrection of the db connection when it's lost
  - Many fixes to the SQL code
  - Fixed some deadlock between the GC and the updateFsFileClosed PL/SQL function
  - Fixed the random port issue on the client
  - Changed default nb of threads to 20 for DB and JOB (optimal value according to Olof's tests)
  - Modifications to be ready with RFIO merge
  - Increased the number of test suites
  
  Upgrade from 2.1.0-x
  --------------------
  The upgrade to be done concerns the whole system (database, stager, clients, tape servers),
  and backward compatibility is ensured only if clients are uprgraded BEFORE servers.
  - Upgrade all clients (they can still connect to an OLD stager).
  - Upgrade the stager database (you don't need to stop the services nor daemons for that).
  - Drop and recreate the DLF database.
  - Then upgrade the head nodes.

-----------
- 2.1.0-8 -
-----------

  New Features and Changes (from 2.1.0-7)
  ---------------------------------------
  - Fixed temporaly the problems with rfcp command for CASTOR1

  Upgrade from 2.1.0-7
  --------------------
  Only CLIENTS using CASTOR1 should upgrade. For CASTOR2 there are no changes
    
-----------
- 2.1.0-7 -
-----------
  
  New Features and Changes (from 2.1.0-6)
  ---------------------------------------
  - Fixed a problem with the memory leak in RFIO
  - Temporary fix, the port used in the bind is now a pseudo-random value that change for each request.To reduce the probability that the answer of an aborted request is returned to a new one.

  Upgrade from 2.1.0-6
  --------------------
  Non special order is required.
  
-----------
- 2.1.0-6 -
-----------
  New Features and Changes (from 2.1.0-5)
  ---------------------------------------
  - Made the output of stager_qry -s less wide
  - Added CF.lastKnownFileName as a result of diskServer_qry -i 
  - Mir valid/invalid checking on IBM3592E05 tape drives, new IBM3592E05 firmware release (172E) fixed problem seen before.
  - Connection reset messages from rmc are just sent to msgdaemon after 2 retries.
  - Rmcdaemon now checks if tape is in the storage slot before returning dismount OK
  - Fixed the cleaning of the DiskCopy in case of put failure. This was leading to orphaned DiskCopies
  - Fixed a bug in updateFiles2Delete
  - Handle properly exceptions in sendRequest and especially release memory and close the socket
  - Fixed missing filling of the svcclass of a castorfile in memory.The consequence was that the svcclass could be overwritten in case  of reuse of a castorfile in 2 different service classes


  Upgrade from 2.1.0-5
  --------------------

  - Upgrade the Database
  - Upgrade the head nodes
  - Then for the other components there are no restrictions in that release.

-----------
- 2.1.0-5 -
-----------
  New Features and Changes (from 2.1.0-5)
  ---------------------------------------
  - Fixed some deadlock between the GC and the updateFsFileClosed PL/SQL function
  - Fixed a missing chkconfig line in debian/cleaning.init
  - RFIO_USE_CASTOR_V2 NOT case sensitive.
  - Fixed the bug of permissions when creating a file (stager_put)
  - Remove check for already migrated files preventing recreated files from being migrated
  - Fixed stage_open, broken since the integration of O_LARGEFILE
  - Fixed tpdump for toot usage

  Upgrade from 2.1.0-4
  --------------------

  - Upgrade the Database
  - Upgrade the head nodes
  - Then for the other components there are no restrictions in that release.

-----------
- 2.1.0-4 -
-----------
  New Features and Changes (from 2.1.0-3)
  ---------------------------------------
  This is a bug fix release, strictly identical to 2.1.0-3 with some exceptions:
  - The name server client commands(nsln, nsgetacl and nssetacl) for manipulating the ACLs are included in castor-ns-client package
  - Fixed TURL parsing in rfcp
  - Updated the man pages with the new turl
  - Added some changes in the stager and disk server to support xrootd protocol
  - Fixed bug #17720, backport fix in rtcpcld_putFailed from 1.143.0.2
  - Added message when skipping not-staged files
  - Stager client command line is CASTOR2 by default (RFIO_USE_CASTOR_V2 doesn't need to be set with this client version)
  - Check of tapealerts hard error, media error, write failure, read failure at release time. 
  - Removal of MIR validity check, on IBM3592EO5 in some situation gives false positive results.
  

  Upgrade from 2.1.0-3
  --------------------

   - The upgrade to be done concerns to the whole system but the database( stager, client, disk servers and tape servers)
   - There are no specific modifications of the database
   - If your are upgrading 2.1.0-3 no order is required, but if you are upgrading from 2.1.0-2 or previous versions please follow the order specified in "2.1.0-3 upgrade from 2.1.0-2"  
   



-----------
- 2.1.0-3 -
-----------
  New Features and Changes (from 2.1.0-2)
  ---------------------------------------
  This is a bug fix release, strictly identical to 2.1.0-2 with some exceptions:
  - Fixes concerning the TURL parsing in RFIO
  - The garbage collection was optimized 
  - Added the check of MIR validity at mount time for SUN/STK T10000, LTO and IBM3592
  - Fixes concerning the deadlocks in PL/SQL and avoiding replication of STAGEOUT files
  - Added dependency on castor_lib_compact 2.0.2_1 to avoid relinking.
  - Fixed the inicialitation of the file systemd for minAllowedFreeSpace
  

  Upgrade from 2.1.0-2
  --------------------

    - The upgrade to be done concerns the whole system (database, stager, client, tape servers)
    - Upgrade the database (you don't need to stop the services nor daemons for that).
    - Then upgrade the rest of the system. 


  

-----------
- 2.1.0-2 -
-----------

  New Features and Changes (from 2.1.0-0)
  ---------------------------------------
  
  This is a bug fix release, strictly identical to 2.1.0-0 except
  for the tape part.
  On previous releases of Castor, the report of compression statistics
  from SUN T10000 tape drives was wrong. This behaviour was due to the
  wrong handling of the T10000 device when decoding the sequential log page.
  Castor will report wrong compression statistics on IBM3592 tape drives
  if the tape drive is set with NVC mode on. The tape drive will only post
  correct statistics after the flush of the NVC buffer, this behaviour is
  particularly seen when files are smaller than the NVC buffer size
  (~300MBytes).

  Upgrade from 2.1.0-0
  --------------------

    - The only upgrade to be done concerns the tape servers.


-----------
- 2.1.0-1 -
-----------

  New Features and Changes (from 2.1.0-0)
  ---------------------------------------
  This is not really a release but modifications at the rpm level
  
-----------
- 2.1.0.0 -
-----------

  New Features and Changes (from 2.0.4-1)
  ---------------------------------------

  This version is strictly identical to 2.0.4-1.
  It is however prefered to 2.0.4-* since the soname of the libraries changed
  which was needed after we introduced some non backward compatible change in
  the interface.

  Upgrade from 2.0.4-1
  --------------------

    - The only upgrade to be done concerns the castor.conf: it has to be
      upgraded everywhere (head nodes and diskservers) with the new library
      name (ends with 2.1 instead of 2.0).
    - Note that for the client part, we advice to let castor-lib 2.0.* live
      aside the new one, so that applications linked with libshift.2.0 don't
      break suddenly. Once all users have relinked there applications with
      the new version, it can be removed.

-----------
- 2.0.4-1 -
-----------

  New Features and Changes (from 2.0.4-0)
  ---------------------------------------
    
    - Fixes concerning the TURL parsing in RFIO
    - Implementation of a port range for the ports used by the stager clients.
      Default range in [30000,30100], it can be overwritten in castor.conf
    - The file permission checks were fixed in the stager
    - The getNext functionnalities were made available to update requests
    - The cleaning daemon now has an init script
    - The flags given to RFIO when opening a file are now correctly handled
    - Queries on directories are now properly handled
    - SQL scripts are now also provided for sqlplus
    - The garbage collection was fixed (it was broken in 2.0.4-0 due to the new archiving of subrequests)
    - stager_qry -s was fixed (ordering was wrong)

  Upgrade from 2.0.4-0
  --------------------

    - The database schema has not evolved. However, some fixes in the PL/SQL code
      were added. Use upgrades/2.0.4-0_to_2.0.4-1.sql to upgrade.
    - Clients can be updated completely independently (before or after the server
      update).

-----------
- 2.0.4-0 -
-----------

  New Features and Changes (from 2.0.3-0)
  ---------------------------------------

    - Fixes concerning the IBM 3592 tapes
    - Support for preallocation and direct i/o were added to RFIO
    - A Cleaning service was added that cleans up old requests
      and errors after some (configurable) time
    - stager_qry support for regexp was improved. Regexp is
      no more the default but can be activated using options.
      Directory listing is also supported and several fixes were applied.
    - stager_qry -s was implemented for getting statistics on diskpools
    - (prepareTo)Get/Put requests now detect directory names
    - RFIO was modified for supporting extended TURLs needed for
      SRM : rfio://stager:port//castor/.../file?svcclass=...&castorversion=...
    - /etc/castor/ORASTAGERCONFIG can now contain several lines in order
      to be able to switch between several CASTOR2 instances. The environment
      variable CASTOR_INSTANCE defines which line to use
    - VMGR now updates correctly the estimated free space, even for full tapes
    - rmnode has no more default for the RMMASTER_HOST. It raises an error
      if nothing is set.
    - Better logging of rmmaster to DLF

  Upgrade from 2.0.3-*
  --------------------

    - The database schema has evolved. Use upgrades/2.0.3_to_2.0.4.sql to upgrade.
      This can be done anytime before the software upgrade and does not imply any
      service interruption.
    - The server code can be updated after the DB, still without service interruption
      if you play with several processes.
    - Clients can be updated completely independently (before or after the server
      update). However, new functionnalities will lead to strange errors in case
      servers were not updated, typically :
          > stager_qry -s
          Error: Invalid argument
          stage_diskpoolsquery: Server Error 22 :
          Unable to read Request object from socket.
          No factory found for object type 103 and representation type 1

-----------
- 2.0.3-5 -
-----------

  Prerelease of future 2.0.4-0. To be tested.

-----------
- 2.0.3-4 -
-----------

  Bug fix release. Should again only be used for IBM 3592 taperservers

-----------
- 2.0.3-3 -
-----------

  Bug fix release. This release should be used in place of v2_0_3_0. It fixes
  a packaging bug that was introducing a symbol main in the library libshift.
  It is otherwise fully identical to v2_0_3_0.

-----------
- 2.0.3-2 -
-----------

  Bug fix release. Should again only be used for IBM 3592 taperservers

-----------
- 2.0.3-1 -
-----------

  Bug fix release. Should only be used for IBM 3592 taperservers

-----------
- 2.0.3-0 -
-----------

  New Features and Changes
  ------------------------

    - Support for ACLs in the nameserver
    - Regular expression support in stager_qry. The feature is
      limited on the server side via the FILEQUERY/MAXNBRESPONSES
      entry in the castor.conf.
    - LSF plugin is using new fixes coming with LSF 6.1-24. This
      allows to remove an internal fork. This change is incompatible
      with any old version of LSF
    - Interface for passing an authorization id to the stager on top
      of a user id.
    - Framework for multithreaded applications was ported to C++
    - Fixed compilation for x86_64 architecture
    - New package castor-dlf-gui
    - tpdaemon uses sg0 device instead of sga
    - Write filemarks with sg driver and immediate bit set

  Upgrade from 2.0.2-*
  --------------------

    - LSF 6.1-24 or higher must be used
    - The database schema has evolved. Use upgrades/2.0.2_to_2.0.3.sql to upgrade
    - Once the database is upgraded, the content of the lastKnownFileName column
      of the CastorFile table is still empty. It will be updated when the files
      are accessed but you can update it by hand using upgrades/2.0.2_to_2.0.3.pl
      Note that the only disadvantage of not updating is that stager_qry -M will
      not return files that were not updated.
    - For the nameserver, the database must be updated first, using the script
      ns/Cns_oracle_add_ACL.sql. Then the daemons can be updated and finally the
      clients. Note that each step is backward compatible, so that an old daemon
      can run on a new database and an old client can contact a new daemon.

