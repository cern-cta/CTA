------------
- 2.1.12-4 -
------------

  Summary
  -------

  - this release is a bug fix release on top of 2.1.12-1
    No new major feature has been included. The list of bug fixes and minor improvements is given below.

  Note
  ----

  - similarly to release 2.1.12-1, the test suite some test cases will fail, namely :
       touch_updateFileAccess,
       touch_updateFileModification
       ns_mkdir_invalidModeTooLong
    see details in the release notes of release 2.1.12-1
  - the test noWritePermsParentDir will only pass if the user launching the test suite is
    not an ns admin in the cupv database

  CASTOR Stager
  -------------

  [Bug]
    - #92096 Incorrect handling of pending requests when a diskserver comes back online
    - #91890 deletesvcclass is broken
    - #91823 stager_abort blocked with Oracle 11g

  CASTOR Scheduler
  -------------

  [Bug]
    - #91880 listtransfers -p forgets the last pool when listing protocols
    - #91866 transfers may be lost after a restart of the diskmanagerd

  CASTOR Tape Gateway
  -------------------

  [Bug]
    - #91586 "Tapecopy not found for castorfile" should be an info level.

  [Features]
    - #91761 RFE: default migration retry policy should not retry when the file is gone from the name server

  CASTOR Central Daemons
  ----------------------

  [Bug]
    - #91956 VDQM crashes on shutdown
    - #91084 nsrm incorrectly handles mode=000 directories


  Upgrade Instructions from 2.1.12-1
  ----------------------------------

    Before upgrading to a 2.1.12-4 instance of CASTOR2 please make sure to read
    the upgrade instructions fully before proceeding.

    Stager, DLF and Repack
    ----------------------

    The upgrade of the stager database to 2.1.12-4 can be performed online while the system is running.
    The RPM upgrade can also be performed on a running system, with 2 restrictions:
      - the restart of the RH servers may be noticable by few clients (< 1s downtime);
      - the restart of the transfermanagers and diskmanagers should not be done concurrently.

      Instructions
      ------------

       1. Upgrade the STAGER database using the stager_2.1.12-1_to_2.1.12-4.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-4/dbupgrades

       2. Upgrade the DLF database using the dlf_2.1.12-1_to_2.1.12-4.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-4/dbupgrades
       
       3. Upgrade the software on the headnodes.
          Note: All daemons involved in the upgrade will be restarted automatically.

       5. Upgrade the software on the diskservers.
          Note: All daemons involved in the upgrade will be restarted automatically.

       6. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-4/testsuite

       7. Congratulations you have successfully upgraded to the 2.1.12-4 release
          of CASTOR.


    VDQM
    ----

      The upgrade of the VDQM databases to 2.1.12-4 cannot be performed online.

      Instructions 
      ------------

       1. Stop the vdqm daemon.

       2. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-4/dbupgrades

       3. Update the software to use the 2.1.12-4 RPMS.

       4. Start the vdqm daemon.

       5. Upgrade complete.


    Other Central services (CUPV, VMGR, Nameserver)
    -----------------------------------------------

      The upgrade of the CUPV, VMGR, and Nameserver databases to 2.1.12-4 can be performed online while
      the system is running.

      Instructions 
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-4/dbupgrades

       2. Update the software to use the 2.1.12-4 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.


    Monitoring
    ----------

      The upgrade of the MONITORING database can be performed online while the
      system is running.

      Instructions
      ------------

       1. Upgrade the MONITORING database using the mon_2.1.12-1_to_2.1.12-4.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-4/dbupgrades

       2. Upgrade complete.



------------
- 2.1.12-1 -
------------

  Summary
  -------

  - this release is a bug fix release on top of 2.1.12-0
    No new major feature has been included. The list of bug fixes and minor improvements is given below.

  Note
  ----

  - the test suite has been improved in this release, in particular around the nameserver tests
    This revealed ancient bugs that have not yet been fixed :
      + bug #91280: nstouch cannot selectively update access or modification time
      + bug #91080: nsmkdir truncates mode if it's somewhat valid but too long
    As a consequence, it is considered normal that the test cases touch_updateFileAccess,
    touch_updateFileModification and ns_mkdir_invalidModeTooLong are failing.


  CASTOR Central daemons
  ----------------------

  [Features]
    - #89708 limitations on privileges length in Cupv commands


  CASTOR Stager
  -------------

  [Bug]
    - #91317: Allow huge files to be recalled
    - #91316: Fixed migrateNewCopy for 2.1.12 schema
    - #91261: DLF chokes on changed log messages
    - #90938: GC drops disk-only files if they are overwritten
    - #90548: Broken logic when restarting subrequests waiting on recalls
    - #89630: stager leaves failed subrequests behind in status 10
    - #87426: stager_qry may not show STAGEIN when it should


  CASTOR Scheduler
  -------------

  [Bug]
    - #90083: transfermanager wrongly rebuilds its list of running d2dsrc


  CASTOR Tape Gateway
  -------------------

  [Bug]
    - #90425: Lack of logs for files in retry in tapegateway

  [Features]
    - #90343: RFE: Add name-server file-id to mismatch error message
    - #73546: RFE: Tape-gateway worker-thread should handle ENSTOOMANYSEGS and ENSCLASSNOSEGS

  Package changes
  ---------------
  
  - castor-mighunter-server RPM has been dropped as part of the end-of-support for rtcpclientd


  Upgrade Instructions from 2.1.12-0
  ----------------------------------

    Before upgrading to a 2.1.12-1 instance of CASTOR2 please make sure to read
    the upgrade instructions fully before proceeding.

    Stager, DLF and Repack
    ----------------------

    The upgrade of the stager database to 2.1.12-1 can be performed online while the system is running.
    The RPM upgrade can also be performed on a running system, with 2 restrictions:
      - the restart of the RH servers may be noticable by few clients (< 1s downtime);
      - the restart of the transfermanagers and diskmanagers should not be done concurrently.

      Instructions
      ------------

       1. Upgrade the STAGER database using the stager_2.1.12-0_to_2.1.12-1.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-1/dbupgrades

       2. Upgrade the DLF database using the dlf_2.1.12-0_to_2.1.12-1.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-1/dbupgrades
       
       3. Upgrade the software on the headnodes.
          Note: All daemons involved in the upgrade will be restarted automatically.

       5. Upgrade the software on the diskservers.
          Note: All daemons involved in the upgrade will be restarted automatically.

       6. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-1/testsuite

       7. Congratulations you have successfully upgraded to the 2.1.12-1 release
          of CASTOR.


    Central services (CUPV, VMGR, VDQM, Nameserver)
    -----------------------------

      The upgrade of the central databases to 2.1.12-1 can be performed online while
      the system is running.

      Instructions 
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-1/dbupgrades

       2. Update the software to use the 2.1.12-1 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.


    Monitoring
    ----------

      The upgrade of the MONITORING database can be performed online while the
      system is running.

      Instructions
      ------------

       1. Upgrade the MONITORING database using the mon_2.1.12-0_to_2.1.12-1.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-1/dbupgrades

       2. Upgrade complete.



------------
- 2.1.12-0 -
------------

  Summary of major features
  -------------------------

  - the tape schema of the stager DB has been cleaned up for migrations.
    + tapepools have now a nbDrives, minAmountData, minNbFiles and maxFileAge to
      replace previous stream policy
    + migration policy has been replaced by a migrationRouting table.
      See command print/enter/modifytapepool/migrationrout and their man pages for more details.
    + migration decisions are now taken at the opening of the file (for write mode)
      and file opening is denied if the migration is not setup properly.
    + problems of multiple concurrent migrations going to the same tape have been solved.
  - repack had been fully rewritten
     + now a simple python tool acting on the stager DB (in castor-dbtools package)
     + efficiency has dramatically improved (e.g. ~2h to start repack of 20 tapes of 200K files each)
     + output of repack -s has also greatly improved
  - the request handler has been rewritten with more efficient DB interface
  - Id2Type has been dropped from the stager DB, reducing by almost two the number of
    actions in the DB. Thus 2.1.12 is able to handle 2x more files per second than 2.1.11
    (order of 250 to be compared to 120)
  - new admin tools are provided with more intuitive interface and much better output
    + see print/insert/delete svcclass/tapepool/diskpool/migrationroute/... and their man pages
      for more details
  - a major code cleanup has been realized to remove unsupported componenents (LSF, jobmanager,
    rtcpclients, experts, someC interfaces). This has dropped 75K lines of code, 8% of total code


  CASTOR Nameserver
  -----------------

  [Features]
    - #87915: RFE: allow GRP_ADMIN privilege to execute nssetacl


  CASTOR Core Framework
  ---------------------

  [Bug]
    - #87975: DynamicThreadPool does not correctly handle the case with 0 consumers

  [Code maintenance]
    - #83111: CM: Remove Id2Type and the logic around it from the CASTOR schemata


  CASTOR Stager
  -------------

  [Bug]
    - #28752: Add proper foreign keys into the DB schema
    - #87928: GC should fail potentially remaining requests for deleted files
    - #89390: StagerJob does not react properly to RequestCanceled exceptions

  [Features]
    - #80457: RFE: Bulk Repack request handling
    - #87966: RFE: allow draindiskserver to start draining the "remaining" filesystems


  CASTOR Scheduler
  -------------

  [Bug]
    - #88715: diskmanagerd is not guessing properly the starting time of adopted transfers
    - #89401: listtransfers -q fails with exception when a diskserver has disappeared
    - #89408: Deadlock in the transfermanager's synchronizer thread

  CASTOR Tape
  -----------

  [Bug]
    - #87268: The main loop of taped does not check the return value of the
              netread_timeout of request message bodies 
    - #87946: Lack of complete routing information for migration jobs leads to
              creating holes in the tapes.
    - #88496: Wrong log level for end session errors and Worker: wrong file size
              for recalled file
    - #88560: Correct capitalisation mistake in tapebridged logs - mountTransActionId
    - #89253: tapebridged sends end-of-session before the tapegatewayd has finished
              processing the session
    - #89492: The tapebridged daemon loses migration report messages when it cannot
              connect to the tapegatewayd daemon

  [Features]
    - #82593: RFE: implement unique migration routing and optimize tape-related
              db schema
    - #85949: RFE: Add bulk messages to tapegatewayd/tapebridged protocol
    - #88404: RFE: The connectDuration log parameter of castor::tape::tapebridge::ClientTxRx
              should at least give millisecond accuracy
    - #88518: RFE: Caller context should be given when logging failed calls to
              ClientTxRx::receiveReplyAndClose


  CASTOR Tape Gateway
  -------------------

  [Bug]
    - #89008: Tape gateway fails to update the fseq in vmgr under some error conditions
    - #89020: VID is not visible in the tape gateway logs on the first step of
              migrations and other logging grieves

  [Features]
    - #58462: RFE: tapegateway, single copy file class optimization 


  Package Changes
  ---------------

  - several packages have been dropped due to end of support for rtcpclientd and LSF :
    castor-expert-server
    castor-jobmanager-server
    castor-lsf-plugin
    castor-policies
    castor-rtcopy-clientserver

  - the repack rewrite has dropped the repack packages (repack is now a tool provided in castor-dbtools) :
    castor-repack-server
    castor-repack-client


  Deployment Changes
  ------------------

  - the repack database and deamon have been obsoleted. Thus no mention of these is made
    in the following instructions. You can safely stop the repackd and dismantle the repack DB


  ORACLE Version 11
  -----------------

  - Oracle version 11gR2 (11.2.0.3) is the prefered Oracle version for castor 2.1.12.
    Oracle 10.2.0.5 is still supported for the core of CASTOR but repack will not be functional
    if Oracle 10 is used.
    Note that if you're upgrading from CASTOR 2.1.11 to CASTOR 2.1.12 under Oracle 10, you will
    end up with one invalid procedure after the upgrade : handleRepackRequest. This is not
    an issue as long as you do not use repack on the instance. If you later upgrade to
    Oracle 11, you only have to revalidate this procedure to make repack fully functionnal.


  Upgrade Instructions from 2.1.11-0
  ----------------------------------

    Before upgrading to a 2.1.12-0 instance of CASTOR2 please make sure to read
    the upgrade instructions fully before proceeding.

    Stager, DLF and Repack
    ----------------------

    The upgrade of the STAGER database to 2.1.12-0 cannot be performed online.
    As a result all daemons accessing the STAGER database MUST be stopped!
    The expected downtime for the upgrade is 30 minutes.

    Notes:
      - Prior to upgrading the STAGER database please verify with your DBA that
        you have the right to create database links. The way to add this privilege
        if it is not present is :

        GRANT CREATE DATABASE LINK TO <user>;

      - the use of LSF is not supported anymore in version 2.1.12 of CASTOR. You
        must use the new transfermanager. It is adviced to do the move from LSF
        to transfermanager while running 2.1.11 and to avoid jumping both from
        2.1.11 to 2.1.12 and from LSF to transfermanager. The following
        instructions will suppose that you are already running the transfermanager.
        Instructions for moving to the transfermanager are available in the
        upgrade instructions for version 2.1.11-0

      - the use of rtcpclientd is not supported anymore in version 2.1.12 of
        CASTOR. You must use the new tapegateway. It is adviced to do the move
        from rtcpclientd to tapegateway while running 2.1.11 and to avoid
        jumping both from 2.1.11 to 2.1.12 and from rtcpclientd to tapegateway.
        The following instructions will suppose that you are already running
        the tapegateway.
        Instructions for moving to the transfermanager are available at : 
        http://twiki.cern.ch/twiki/bin/view/DataManagement/RtcpclientdTapeGatewaySwitchover

      - the removal of the mighunter has potential consequences on the migrations
        on files that have been removed from the namespace before they are migrated.
        With the mighunter, a deletion before the mighunter acts on the file would
        not lead to any migration attempt. In 2.1.12-0, the migration will be attempted
        and will fail with ENOENT. In order to avoid looping migrations, one has to
        make sure to have a proper migrationRetry policy.

      Instructions
      ------------

       1. Stop all daemons on the stager headnodes which have direct
          connections to the STAGER database. This includes: rhd, stagerd,
          transfermanagerd, tapegatewayd, rechandlerd and rmmasterd.

          Note: It is not necessary to stop any daemons on the diskservers.

       2. Upgrade the STAGER database using the stager_2.1.11-9_to_2.1.12-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-0/dbupgrades
          Note that this script can be used for 2.1.11-8 and 2.1.11-9 databases.

       3. Upgrade the DLF database using the dlf_2.1.11-9_to_2.1.12-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-0/dbupgrades
          Note that this script can be used for 2.1.11-8 and 2.1.11-9 databases.

       4. Upgrade the software on the headnodes and diskservers to 2.1.12-0.

       5. Check configuration of the tapepools. A default configuration has been created
          taking into account the previous setup, but some complex configuration may not
          be handled properly
          - printtapepool
          - modifytapepool if needed

       6. Create the migration routes, from the old migration policies
          - entermigrationroute

       7. Run the stager_2.1.12-0_postUpgrade script on the STAGER database. It is available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-0/dbupgrades
          Note that this script prints appropriate errors if migrations couldn't be routed.
          In such a case, fix the missing routes and rerun the script.
          Repeat this until the output is clean.

       8. If you have an independent monitoring system to monitor CASTOR such
          as LEMON. Please make sure to update the monitoring configuration to
          reflect any changes in this release.

       9. Start all the daemons which were stopped in step 2.

          Note: If you have a concept of private and public request handlers
                you should start only the private ones to test/validate the
                installation without user interference.

      10. Wait 60 seconds to give time for the diskservers to send a heartbeat
          message to the rmmaster daemon.

      11. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.11-*/2.1.11-0/testsuite

      12. Start the public request handlers (if applicable)

      13. Congratulations you have successfully upgraded to the 2.1.12-0 release
          of CASTOR.


    Central services (Nameserver)
    -----------------------------

      The upgrade of the CNS databases to 2.1.12-0 can be performed online while
      the system is running.

      Instructions 
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-0/dbupgrades
         Note that this script can be used for 2.1.11-8 and 2.1.11-9 databases.

       2. Update the software to use the 2.1.12-0 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.


    Central services (CUPV, VMGR, VDQM)
    -----------------------------------

      The upgrade of the VDQM database needs a down time. We assume that VMGR and
      UPV daemons are collocated with the VDQM one, thus we give upgrade instructions
      for a non transparent upgrade of all of them. Please adapt to your needs

      Instructions
      ------------

       1. Put all the tapeservers down using tpconfig

       2. Stop all vdqm, vmgr and cupv daemons

       3. Apply the appropriate database upgrade scripts from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-0/dbupgrades
         Note that these scripts can be used for 2.1.11-8 and 2.1.11-9 databases.

       4. Update the software to use the 2.1.12-0 RPMS.

       5. Restart vdqm, vmgr and cupv daemons

       6. Put all the tapeservers up using tpconfig

       7. Upgrade complete.


    Monitoring
    ----------

      The upgrade of the MONITORING database can be performed online while the
      system is running.

      Instructions
      ------------

       1. Upgrade the MONITORING database using the mon_2.1.11-9_to_2.1.12-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.12-*/2.1.12-0/dbupgrades
          Note that this script can be used for 2.1.11-8 and 2.1.11-9 databases.

       2. Upgrade complete.



------------
- 2.1.11-0 -
------------

  CASTOR Nameserver
  -----------------

  [Bug]
    - #82015: Nsfind returns incorrect unix permissions for symbolic links

  [Features]
    - #63524: RFE: single-line nameserver log format (or unique req identifier)
    - #67763: RFE: provide missing file metadata attributes during file deletion
    - #71565: RFE: provide weighted compression rate in nslisttape --summarize
    - #75644: RFE: provide a nameserver API to atomically stat/create + change
              fileclass for a file
    - #77880: RFE: Add GRP_ADMIN support to Cns_srv_chclass
    - #79122: RFE: Cns_setsegattrs should ignore logically deleted segments when
              checking for too many copies
    - #80000: RFE: Merge the Cns_setfsizecs and Cns_dropsegs calls into
              Cns_closex

  [Code Maintenance]
    - #79320: CM: Remove VIRTUAL ID functionality to improve code
              maintainability

  CASTOR Tape
  -----------

  [Bug]
    - #76832: Buggy error-handling code in vdqmlistpriority
    - #75612: Rtcpclient should take a lock on castor-file when updating a
              migrated file to staged
    - #81291: The castor::tape::mighunter::ora::OraMigHunterSvc::reset() method
              does not reset m_invalidateTapeCopiesStatement
    - #81569: Wrong format in python tuple building in mighunter (at least)
    - #82141: RFE: Support for IBM TS1140 tape drive capacities
    - #82564: Make tape read only when there is a discrepancy between vmgr and
              NS

  [Packaging]
    - #79389: RFE: move smc and tpdump from castor-tape-server to
              castor-tape-tools
    - #80273: Remove low-level tape-tests from CASTOR test-suite

  [Features]
    - #79207: RFE: Rechandler should give the number of running recall requests
              to the rechandler policy
    - #80217: RFE: Add client and drive information to tape-bridge logs

  CASTOR Tape Gateway
  -------------------

  [Bug]
    - #17792: Tapes with unprocessed segments stuck in FAILED or unprocessed
              status
    - #36428: Tape recall candidates stuck in WAITDRIVE
    - #38818: BUSY state of tapes not reset
    - #41574: Migration of file stuck if a tape write error
    - #44315: rtcpclientd does not clean streams on restart
    - #45287: rtcpclientd might create entries in the tape table with status
              'WAITDRIVE' without a request in VDQM.
    - #45288: migrator might leave tapecopies in status 'SELECTED' in case of
              error
    - #46456: streams with no tape in WAITDRIVE
    - #47043: inconsistencies left by the recaller create problems to the
              migrator
    - #47816: RTCPCLD_MSG_INVALSEGM not handled properly by the TapeErrorHandler
    - #51131: File deletions during repack migrations leave subrequests in an
              incorrect state
    - #51472: migrator crashes in Cstager_Stream_id
    - #52469: tape in two streams at once
    - #53823: A recaller should not exit when it receives ENOENT in response to
              querying a segment in the name server
    - #69588: Block ID is not being logged correctly by the tape-gateway
    - #70759: Tape-gateway design fault - Starting a migration should only be
              done by the stager
    - #71871: Locking problem between tape gateway and mighunter
    - #80262: RFE: Too much logging with above-DEBUG level in tapegateway
    - #80894: tape gateway does not log drive information for mount transactions

  [Code Maintenace]
    - #72215: Remove the TAPEGATEWAYREQUEST table from the schema, and drop the
              triggers that populate it.

  CASTOR Miscellaneous
  --------------------

  [Features]
    - #77741: RFE: On demand generation of DLF registration messages
    - #79575: RFE: drop usage of ROWTYPE in bulk selections in the C++ framework
    - #79766: RFE: single-line CUPV and VMGR log format (or unique req
              identifier)
    - #80071: RFE: Increase default logrotation retention period to 200 days

  [Bug]
    - #61952: castor-dbtools: allow short hostnames, fail visibly
    - #78416: SLC6: callback ports in firewall configured differently ("INPUT"
              chain)
    - #82220: Incorrect exit status on rmAdminNode failures

  CASTOR Protocols
  ----------------

  [Bug]
    - #72157: bad checksums with gridFTP in case of interrupted transfers
    - #80821: Memory leaks in rfio TURL processing and HSM interface
    - #29491: Update with 'root' protocol does not update the filesize

  CASTOR Stager
  -------------
  [Bug]
    - #56042: Disable support for preset checksums in PrepareToPut, Put, PutDone
              transfers
    - #75936: In case of error, the stager API returns normalized filenames as
              opposed to the original ones
    - #76002: Stager may not reply to stagerJob in case of job cancellation
    - #76398: Stager Abort functionality does not take into account NS override
              mode
    - #77022: Issues with black and white lists on diskpoolquery
    - #77347: diskServer_qry requires fully qualified names but silently
              swallows any input string
    - #78440: lastknownfilename column should be unique and not null
    - #78826: Race condition between stageRm and disk2DiskCopyDone can result in
              NULL DiskCopy states
    - #80441: Diskcopies left in STAGEOUT after preset checksum mismatch
    - #80643: Incorrect file size and checksum information after update of files
              using the xroot protocol
    - #81247: Communication errors result in file descriptor leaks in the
              rmmaster collector thread

  [Code maintenance]
    - #80011: CM: Make use of the new Cns_openx API in the stager

  CASTOR Monitoring
  -----------------
  [Bug]
    - #77606: StatsProcessingTime does not report statistics for SRM PUT
              requests  

  [Features]
    - #78412: RFE: MON schema should use monthly partitioning as opposed to
              daily.


  Package Changes
  ---------------

  - Increased the default logrotation from 120 to 200 days.

  - Added three new packages to support the new scheduling system:
      castor-transfer-manager
      castor-transfer-manager-client
      castor-diskmanager-server

  - Moved the smc and tpdump commands from the castor-tape-server package to
    castor-tape-tools.


  Upgrade Instructions from 2.1.10-0
  ----------------------------------

    Before upgrading to a 2.1.11-0 instance of CASTOR2 please make sure to read
    the upgrade instructions fully before proceeding.

    Stager, DLF and Repack
    ----------------------

    The upgrade of the STAGER database to 2.1.11-0 cannot be performed online.
    As a result all daemons accessing the STAGER database MUST be stopped!
    The expected downtime for the upgrade is 30 minutes.

    Notes:
      - Prior to upgrading the STAGER database please verify with your DBA that
        the DBMS_ALERT package is installed on the target database and that the
        STAGER database account (e.g. castor_stager) has the rights to use it.

        GRANT EXECUTE ON DBMS_ALERT TO <user>;

      - The CASTOR project no longer distributes SL4 packages for server side
        installation. As a result all diskservers and headnodes must be running
        SL5 or a newer distribution. The recommended operating system and
        architecture is SL5 64bit.

      - Due to bug fixes:
        - #75644: RFE: provide a nameserver API to atomically stat/create +
                  change fileclass for a file
        - #80000: RFE: Merge the Cns_setfsizecs and Cns_dropsegs calls into
                  Cns_closex

        it is not possible to run a 2.1.11 stager instance against an pre
        2.1.11 name server front end. Please make sure to upgrade the name
        server before proceeding with the stager upgrade.

      - Due to changes in how CASTOR and XROOT communicate (#80643) the old
        xrootd-xcastor2fs plugin is no longer compatible with this release
        and needs to be upgraded to version 1.0.9-19 or newer.

      Instructions
      ------------

       1. Suspend all LSF activity using the `badmin qinact all` command.
          Either wait for currently running transfers to end or terminate them
          with:
            `bjobs -r -u all -w | grep RUN | awk '{ print $1 }' | xargs bkill

       2. Stop all daemons on the stager headnodes which have direct
          connections to the STAGER database. This includes: rhd, stagerd,
          jobmanagerd, rtcpclientd, mighunterd, rechandlerd, expertd and
          rmmasterd.

          Note: It is not necessary to stop any daemons on the diskservers.

       3. Upgrade the STAGER database using the stager_2.1.10-0_to_2.1.11-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.11-*/2.1.11-0/dbupgrades

       4. Upgrade the DLF database using the dlf_2.1.10-0_to_2.1.11-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.11-*/2.1.11-0/dbupgrades

       5. If applicable, upgrade the REPACK database using the repack_2.1.10-1_to_2.1.11-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.11-*/2.1.11-0/dbupgrades
       
       6. Upgrade the software on the headnodes and diskservers to 2.1.11-0.

       7. Update the file class entries in the STAGER database using the
          following command:
            for cname  in `nslistclass | grep NAME | awk '{ print $2 }'` ; do modifyFileClass --Name $cname --GetFromCns ; done

          Notes:
            - This step must be done from a machine running a 2.1.11-0 client
              and requires an operational name server front end.

       8. Make sure that the STAGER/NOTIFYHOST option in castor.conf is defined
          correctly for all servers where the request handler daemon runs.

          This option defines the hostname which the request handler should
          notify to pickup new requests for processing. If in doubt, set the
          value to the hostname where the stager daemon is running.

       At this point several options exist:
         A) You can continue to step 9 and bring up the stager instance running
            with LSF and rtcpclientd.
         B) Enable the NEW Transfer Manager daemon,
              see section 'Enabling the Transfer Manager'
         C) Enable the NEW TapeGateway daemon,
              see section 'Enabling the TapeGateway Daemon'
         D) Both B & C

       9. If you have an independent monitoring system to monitor CASTOR such
          as LEMON. Please make sure to update the monitoring configuration to
          reflect any changes in this release.

      10. Start all the daemons which were stopped in step 2. Note: Depending
          on whether you enabled the Transfer Manager or TapeGateway daemons
          some of these daemons may no longer exist!

          Note: If you have a concept of private and public request handlers
                you should start only the private ones to test/validate the
                installation without user interference. This is especially
                important if you have enabled the new Transfer Manager daemon!

      11. Wait 60 seconds to give time for the diskservers to send a heartbeat
          message to the rmmaster daemon.

      12. If still running with LSF, re-enable the LSF queues with:
            `badmin qact all`

      13. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.11-*/2.1.11-0/testsuite

          Notes:
            - The test suite now performs some tests as an unprivileged user.
              As a result it is necessary to:
               A) Update your CastorTestSuite configuration files using the
                  example provided. (See options unprivUid and unprivGid)
               B) Grant the unprivileged Uid and Gid the rights to issue the
                  following commands:
                    StagePutRequest, StageUpdateRequest and DiskPoolQuery

      14. Start the public request handlers (if applicable)

      15. Congratulations you have successfully upgraded to the 2.1.11-0 release
          of CASTOR.


    Central services (CNS, CUPV, VMGR, VDQM)
    ----------------------------------------

      The upgrade of the CNS, CUPV, VMGR and VDQM databases to 2.1.11-0 can be
      performed online while the system is running.

      Note: The upgrade of VMGR database and software must be performed from
            version 2.1.10-1 which requires downtime. If necessary please
            follow the 2.1.10-1 upgrade instructions.

      Instructions
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.10-*/2.1.11-0/dbupgrades

       2. Update the software to use the 2.1.11-0 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.


    Monitoring
    ----------

      The upgrade of the MONITORING database can take a substantial amount of
      time depending on the period of data retention. The reason for this is
      related to Bug #78412 (RFE: MON schema should use monthly partitioning as
      opposed to daily) where the table partitioning schema is changed from a
      daily schema to monthly.

      Instructions
      ------------

       1. Upgrade the MONITORING database using the mon_2.1.10-0_to_2.1.11-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.11-*/2.1.11-0/dbupgrades

       2. If you have any monitoring scripts/sensors which access the
          MONITORING database through a read account you will need to grant
          that account access to the monitoring tables using the
          grant_oracle_user script available from:
           - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.11-*/2.1.11-0/dbcreation

          The script will prompt you for the user to grant read access to, type
          in the account name and hit Enter.

       3. Upgrade complete.


    Enabling the Transfer Manager
    -----------------------------

      Instructions
      ------------

      Note: These upgrade instructions assume that the new Transfer Manager
            software will replace LSF on all machines, i.e. LSF is removed 
            completely!

       1. Stop LSF on all headnodes and diskservers.

       2. Uninstall the LSF software. Please Note: The castor-job and
          castor-rmmaster-server packages still have a dependency on the
          following LSF libraries:
            libbat.so()(64bit)
            liblsbstream.so()(64bit)
            liblsf.so()(64bit)

          As a result it is still necessary to have these libraries accessible
          on the system. At CERN this means that the LSF-GLIBC-2.3-lib package
          must remain installed, all other LSF packages can be removed.

       3. On all headnodes (machines that could be elected as LSF masters)
          install the following packages:
            - python-rpyc, python-daemon, python-lockfile
            - castor-transfer-manager
            - castor-transfer-manager-client
            - castor-dbtools

          And remove:
            - castor-lsf-plugin

       4. Uninstall the castor-jobmanager-server package from all headnodes.

       5. On all diskservers install the following packages:
            - python-rpyc, python-daemon, python-lockfile
            - castor-diskserver-manager

       6. Review the configuration options in castor.conf using the example
          found in /etc/castor.

       7. Set the RmMaster/NoLSFMode option in castor.conf to "yes" on all
          headnodes where the rmmaster daemon runs.

       8. Set the DiskManager/ServerHosts option in castor.conf to a list of
          fully qualified hostnames where the transfer manager daemon(s) run on 
          all headnodes and diskservers.

       9. Review the DiskManager/NbSlots and DiskManager/<protocol>Weight
          options in castor.conf. (Refer to section: Configuring Transfer Slots)

      10. Review the TransferManager options in castor.conf. If in doubt set 
          the values of:

            TransferManager/PendingTimeouts         to JobManager/PendingTimeouts
            TransferManager/DiskCopyPendingTimeout  to JobManager/DiskCopyPendingTimeout
            TransferManager/KillRequests            to JobManager/ResReqKill
        
          to simulate the same behaviour as the old jobmanager daemon.

      11. Connect to the STAGER database and execute the following SQL statements:

            UPDATE CastorConfig SET value = 'yes'
             WHERE class = 'RmMaster'
               AND key = 'NoLSFMode';
            UPDATE SubRequest SET status = 7 WHERE status = 14;
            COMMIT;

      12. Start the transfermanager daemon on the headnodes.

      13. Start the diskmanager daemon on all diskservers.

      14. Execute the `listtransfers -s` command on a headnode, this should
          return a list of all diskservers.

          For example:

          DISKSERVER                  NBSLOTS NBTPEND NBSPEND NBTRUN  NBSRUN
          lxc2disk11.cern.ch            60       0       0       0       0
          lxc2disk12.cern.ch            60       0       0       0       0
          lxc2disk14.cern.ch            60       0       0       0       0
          lxc2disk13.cern.ch            60       0       0       0       0

          For an explanation of the commands that can be used to administer and
          monitor the new scheduler see:
            `man killtransfers` and `man listtransfers`

      15. Installation complete. If applicable return to step 9 of the Stager
          upgrade instructions.


    Enabling the TapeGateway Daemon
    -------------------------------

    For instructions on how to enable the new TapeGateway daemon please refer
    to: 
      http://twiki.cern.ch/twiki/bin/view/DataManagement/RtcpclientdTapeGatewaySwitchover


    Configuring Transfer Slots
    --------------------------

    In previous versions of CASTOR where the scheduling was done by LSF the
    management of transfer slots was based on LSF resources. All transfers,
    regardless of the protocol were considered equal and only the number which
    could run concurrently was configurable.

    With the new scheduling subsystem the notion of slots remains unchanged.
    However, the key difference is that every transfer protocol can use a
    variable number of slots. For example, a diskserver could be configured like
    so:

      # The maximum number of slots
      DiskManager     NbSlots       60

      # The number of slots taken by each type of protocol
      DiskManager     xrootWeight   1
      DiskManager     rootWeight    2
      DiskManager     rfioWeight    3
      DiskManager     rfio3Weight   3
      DiskManager     gsiftpWeight  5
      DiskManager     d2dsrcWeight  3
      DiskManager     d2ddestWeight 3
      DiskManager     recallWeight 10
      DiskManager     migrWeight   10

    With the configuration above, the machine could:
      - run 60 xroot transfers before reaching its limit
      - run 20 rfio (v3) transfers before reaching its limit
      - run 10 root, 5 rfio and 5 gridftp transfers before reaching its limit.

    Notes:
     - The configuration of the maximum number of slots and weights per protocol
       is defined at the diskserver level in castor.conf. No central
       configuration is provided.
     - Changes to the slot configuration are automatically reloaded by the
       diskmanager daemons, there is no need perform a manual restart after
       changing castor.conf in this context.


------------
- 2.1.10-1 -
------------

  Please Note: This release is dedicated to tape and as such only includes
               tape related software and client side tools.

  Bug Fixes
  ---------

  CASTOR repack2
  --------------
  - #78347: RepackCleanup procedure returns "ORA-06502: PL/SQL: numeric or
            value error"

  CASTOR tape
  -----------
  - #75722: RFE: The VMGR should support tape capacities greater than 2TB
  - #75868: RFE: VMGR should randomize when choosing a migration tape for a
            specific tape pool and library 
  - #75990: RFE: support for new densities in taped 
  - #76440: Typos in repack man pages


  Upgrade Instructions from 2.1.10-1
  ----------------------------------

    Before upgrading to a 2.1.10-1 instance of CASTOR2 please make sure to read
    the upgrade instructions fully before proceeding.

    VMGR & VDQM plus notice for tape-servers and adminstration machines
    -------------------------------------------------------------------

    Please note the VMGR daemon must be upgraded before any of its clients are
    upgraded. The new VMGR clients of this release cannot talk to the old VMGR
    daemon. The VMGR clients include the new VMGR command-line clients, the
    rtcpd daemon and the mounttape helper process of the taped daemon. This
    means tape-servers and administration machines must not be upgraded to
    version 2.1.10-0 of CASTOR until the VMGR has been upgraded.

    Please note the new VMGR daemon is compatible with old clients.
    
    The upgrade of the VMGR database to 2.1.10-1 cannot be performed online
    while the system is running. As a result the tape subsystem must be stopped.
    The expected downtime is less than 10 minutes.

      Instructions
      ------------

       1. Move the status of all tape drives to DOWN.

       2. Stop *all* vmgr and vdqm daemons.

       3. Upgrade the VMGR database using the vmgr_2.1.10-0_to_2.1.10-01.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.10-*/2.1.10-1/dbupgrades

       4. Upgrade the vmgr and vdqm software to use the 2.1.10-1 RPMS.

       5. Start the vmgr and vdqm daemons stopped in step 2.

       6. Bring all the tape drives back up.

    Repack
    ------

    The upgrade of the REPACK databases to 2.1.10-1 can be performed online
    while the system is running.

      Instructions
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.10-*/2.1.10-1/dbupgrades

       2. Update the appropriate software to use the 2.1.10-1 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.


------------
- 2.1.10-0 -
------------

  Bug Fixes
  ---------

  CASTOR miscellaneous
  --------------------
  - #65462: RFE: Add PL/SQL constants for CASTOR magic (tragic) numbers
  - #66367: CASTOR UUID generation is not unique enough
  - #69659: CM : replacement of Kernighan and Ritchie declarations by ansi ones
  - #69665: CM : Improve Makefiles and packaging
  - #69667: CM : enable maximum level of warnings at compile time
  - #69671: CM : Port code to gcc 4.4
  - #69673: Remove non client code from libshift
  - #69677: CM : Generic cleanup of the code base
  - #69745: stripped executables and proper use of debug info in RPMs
  - #73748: CM : cleanups triggered by coverity reports
 
  CASTOR nameserver
  -----------------
  - #68212: Cns_setsegattrs should prevent the creation of too many copies
  - #74024: In the Cns_tapesum API, remove support for counting 'disabled'
            segments

  CASTOR protocols
  ----------------
  - #69661: Bad call to writerror64_v3 in the handling of first byte written in
            RFIO

  CASTOR stager
  -------------
  - #27304: RFE stager_qry -s output (or new qry option)
  - #47634: Draining diskservers remain in a DISABLED state after
            rmMasterDaemon restart
  - #62269: RFE: possible enhancements for stager_qry -M 'all:/castorpath'
  - #65238: Missing unique constraint in TYPE2OBJ
  - #67591: Cgetpwu/grgid not setting serrno properly
  - #68216: RFE: enforce a stricter check for the ability to migrate to tape
            before scheduling a write
  - #68597: ADMIN_RELEASE states in rmGetNodes
  - #68799: Error loading security modules for the rhd results in segfault
  - #69674: RFE: implement stageAbortRequest request processing
  - #69865: RFE: Implement file size consistency checks for replicated files
  - #71841: Unable to read file in case of failing replication within a diskpool
  - #72213: fixFileSize.py tool is resetting checksum of segments
  - #73677: enterPriority should verify arguments
  - #73750: Fixed deletion of services when a thread gets destroyed
  - #74392: RFE: draindiskserver - add comment
  - #74785: rmmasterd blocks boot sequence if LSF is (un-|mis-)configured

  CASTOR tape
  -----------
  - #45793: missing /etc/sysconfig/rtcpclientd.example file
  - #54597: RFE: ONHOLD explanation option
  - #67558: dumptp, readtp and writetp should have the same file permissions as
            tpdump, tpread and tpwrite
  - #67740: The SQL view used by showqueues returns duplicate rows for
            multi-dedications
  - #74087: RFE:Bad checksum errors in migrator log should include full
            disk-path

  CASTOR test suite
  -----------------
  - #66522: xroot test suite: authentication methods
  - #72092: test-suite: review tests needing two tape-backed service classes
  - #72178: testsuite: "full" service class gives confusing assertion, real
            error not shown
  - #72448: CM : made test suite reusable outside CASTOR


  Package Changes
  ---------------
  - Added an example rtcpclientd sysconfig file (/etc/sysconfig/rtcpclientd.example)
    to the castor-rtcopy-clientserver package.

  - Added a new tool, dumpSharedMemory to the castor-hsmtools package to dump
    the contents of the rmmaster's shared memory to aid in debug investigations.

  - Added a new stager_abort command line along with its associated man page to
    the castor-hsmtools package. This new command line is required to pass the
    new stager_abort test cases found in the test suite. It is not intended for
    end user deployment.

  - Removed the Csched_api.h and Csched_flags.h header files from the
    castor-devel package.

  - All manpages now have '.gz' file extensions to follow standard manpage
    distribution guidelines.

  - Removed the castor-tape-client package from the client only distribution.

  - Renamed the castor-tape-client package to castor-tape-tools to reflect that
    its contents are not for client-side distribution.

  - The castor-gridftp-dsi-xroot package is now dependent on the xrootd-server
    release (v3.0.0+).


  Upgrade Instructions from 2.1.9-10
  ----------------------------------

    Before upgrading to a 2.1.10-0 instance of CASTOR2 please make sure to read
    the upgrade instructions fully before proceeding.

    Notes:
      - Prior to running the STAGER, DLF, REPACK, MON and VDQM database
        upgrades, a DBA must first grant these accounts the privileges to
        manage scheduler jobs:

        GRANT MANAGE SCHEDULER TO <user>;


    Stager
    ------

    The upgrade of the STAGER database to 2.1.10-0 cannot be performed online
    while the system is running. All daemons accessing the database MUST be
    stopped! The expected downtime is less than 10 minutes.

      Instructions
      ------------

       1. Stop all daemons on stager headnodes which have direct connections to
          the stager database. This includes: rhd, stagerd, jobmanagerd,
          rtcpclientd, mighunterd, rechandlerd, expertd and rmmasterd.

          Note: It is not necessary to stop any services/daemons on the
                diskservers themselves.

       2. Upgrade the Stager database using the stager_2.1.9-10_to_2.1.10-0.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.10-0/dbupgrades

       3. Upgrade the software on the headnodes and diskservers to 2.1.10-0.
          Notes: All daemons involved in the upgrade will be restarted 
          automatically.

       4. Start all the daemons which were stopped in step 1.

       5. Wait 60 seconds to give time for the diskservers to send a heartbeat
          message to the rmmaster daemon.

       6. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.10-0/testsuite

       7. Upgrade complete.


    DLF, Monitoring & Repack
    ------------------------

    The upgrade of the DLF, MONITORING and REPACK databases to 2.1.10-0 can be
    performed online while the system is running.

      Instructions
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.10-0/dbupgrades

       2. Update the appropriate software to use the 2.1.10-0 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.


    Central services (CNS, CUPV, VMGR, VDQM)
    ----------------------------------------

    The upgrade of the CNS, CUPV, VMGR and VDQM databases to 2.1.10-0 can be
    performed online while the system is running.

      Instructions
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.10-0/dbupgrades

       2. Update the appropriate software to use the 2.1.10-0 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.


------------
- 2.1.9-10 -
------------

  Bug Fixes
  ---------

  CASTOR nameserver
  -----------------
  - #74163: nslisttape erroneously reports empty tapes

  CASTOR protocols
  ----------------
  - #73921: gridFTP checksums wrong on 32 bits platforms
  - #74252: RFE: GridFTP internal should support multiple service class
            attached to the same diskpool

  CASTOR stager
  -------------
  - #74382: Increase the default number of collector threads in the rmmaster
            daemon
  - #74397: BulkCheckFSBackInProd causes row lock contention
  - #73911: RFE: improve selection logic when multiple replicas are available


  Upgrade Instructions from 2.1.9-9
  ---------------------------------

    Before upgrading to a 2.1.9-10 instance of CASTOR2 please make sure to read
    the upgrade instructions fully before proceeding.

    Stager, DLF, Monitoring & Repack
    --------------------------------

    The upgrade of the Stager, DLF, Monitoring and Repack databases to 2.1.9-10
    can be performed online while the service is running. The expected upgrade
    time is less than 10 minutes.

      Instructions
      ------------

       1. Due to a bug in the rmmaster which loses the state of DRAINING
          diskservers after a restart you will need to take a note of those
          diskservers which are in a DRAINING state before the upgrade using the
          following command: (#47634)

            rmGetNodes | grep -B4 DISKSERVER_DRAINING | grep name | cut -d ':' -f 2 | awk '{ print "rmAdminNode -n "$1" -f Draining" }'

       2. Upgrade the Stager database using the stager_2.1.9-9_to_2.1.9-10.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.9-10/dbupgrades

       3. Upgrade the DLF database using the dlf_2.1.9-9_to_2.1.9-10.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.9-10/dbupgrades

       4. If applicable, upgrade the Monitoring database using the mon_2.1.9-9_to_2.1.9-10.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.9-10/dbupgrades

       5. If applicable, upgrade the Repack database using the repack_2.1.9-9_to_2.1.9-10.sql
          upgrade script available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.9-10/dbupgrades
       
       6. Upgrade the software on the headnodes and diskservers to 2.1.9-10.
          Note: All daemons involved in the upgrade will be restarted
          automatically.

       7. Wait 60 seconds to give time for the diskservers to send a heartbeat
          message to the rmmaster daemon.

       8. Apply the commands returned in step 1.

       9. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.9-10/testsuite

      10. Upgrade complete.


    Central services (CNS, CUPV, VMGR, VDQM)
    ----------------------------------------

    The upgrade of the CNS, CUPV, VMGR and VDQM databases to 2.1.9-9 can be
    performed online while the system is running.

      Instructions
      ------------

       1. Apply the appropriate database upgrade script from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.9-*/2.1.9-10/dbupgrades

       2. Update the appropriate software to use the 2.1.9-10 RPMS. Note: All
          daemons involved in the upgrade will be restarted automatically.

       3. Upgrade complete.
