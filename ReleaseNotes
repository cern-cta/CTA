-----------
- 2.1.6-7 -
-----------

  New Features and Changes
  ------------------------
  This part lists the changes since the first release of the 2.1.4 series, that is 2.1.4-5
  Some fixes were backported to later releases of the 2.1.4 series, as mentioned.

    Broad view and stager
    ---------------------

    - The central stager code was completely rewritten in order to be more modular and
      more maintainable. The logging should also be much better.
    - A synchronization of the disksevers with the nameserver has been introduced. It is
      insured by the gcDaemon of each diskserver that is checking that all the files
      present on the disk still exist in the nameserver. When not, the stager is informed
      and the file is cleaned up.
    - In order to nor accumulates files in STAGEOUT status, a timeout has been introduced
      on the time spent in this status (defaults to 3 days). After this delay, either the
      file will be dropped (if its disk size is still 0) or an automatic putDone will be issued
    - The handling of disk to disk copies has been completely reviewed. Diskcopies are now
      scheduled properly in LSF, both on the source and on the destination host. Additionally,
      internal replication within a diskpool will now be executed in background and access
      will be granted to the user to the old copy.
    - Disk checksums were introduced. This checksum is computed on the writing of the file
      on the disk (only when RFIO v3 is used) and is checked for each subsequent read
      (again only with RFIO v3). This also includes the migration, allowing for detection
      of bad files before the migration takes place.
    - The update and preapareForUpdate requests are now fully supported with RFIO (including
      read only operations). For other protocols, the situation stays unchanged and updates
      will be considered as always modifying the file.
    - Redundancy was added to the rmMasterDaemon so that it can "follow" the LSF master
      if that ones goes from one failing machine to another one.
    - The parsing of file names was improved to support properly multiple '/'s as path separator

    Migrations and recalls
    ----------------------

    - A python policy framework has been implemented that allow easy definition of policies
      in python and efficient execution of them using precompilation.
    - A stream policy policy has been introduced, using the python framework, that allows
      to influence the number of streams that will be used for migrating data to tape depending
      on the number of files and the amount of data to be migrated.
    - The existing migration policy were adapted to the new python framework, unchanged.
    - A recall policy was introduced that allows to decide whether a tape should be mounted
      for recalls depending on the number of files to recall, the amount of data to be read
      and the age of the oldest waiting recall on that tape

    Security
    --------

    - Strong authentication has been added to CASTOR on the client side, although the
      insecure protocol is still available. Kerberos 5 and grid certificates are
      supported.

    Repack
    ------

    - Repack has been enhanced to take advantage of the files already staged on disk.
      These files will be reused and won't be recalled.
    - The repack visualization was improved. See Bug #29775 for more details.
    - Checksums comparisons were added when the old segment is replaced by the new one
      in the nameserver. This allows to trap any corruption introduced during the repacking.

    Miscellaneous
    -------------

    - SQL tools are now provided to facilitate the management tools for black and white lists.
      This includes addPrivilege and removePrivilege methods and tries to keep both the black
      and the whote list as clean as possible.
    - The gridFTP plugins packaging has been improved with the creation of a third rpm called
      castor-gridftp-dsi-common holding the common code and tools. This fixes the problem of
      log rotation for concurrent installs of both the internal and external plugins.

  Bug Fixes
  ---------
  - #20052 RFE to have thresholds for the number of migrators started
  - #21431 RFE for recall policy
  - #29786 Add support for CLOB in the CASTOR code generation
  - #28994 Handling files "stuck" in STAGEOUT
  - #20652 RFE for nsrm
  - #28346 Wrong filesize with overlapping stagerUpdateRequest requests
  - #29847 files in STAGEOUT and STAGED concurrently
  - #30343 2 concurrent prepareToPut for a given file
  - #6019  Disk Resident check sums
  - #29711 Repack sometimes loses stage requests
  - #29779 inconsistent behavior between stager_rm and nsrm
  - #30391 daemons on castor should be stateless and redundant: rmMasterDaemon
  - #30404 RFE: gc daemon should delete files which exist on disk and not in nameserver
  - #31510 stop() function in /etc/init/d/rfiod kills too many process
  - #32523 Accumulation of orphaned diskfiles after failed tape recalls
  - #19413 diskpool with no filesystem should cause jobs for this diskpool to fail
  - #27311 please ensure MAXREPLICANB is respected
  - #27319 RFE Loadbalancing of VMGR pools with tapes in multiple libraries
  - #28685 Possible file loss due to inconsistent vmgr_updatetape and Cns_setsegattr calls in failing migrator
  - #28962 reporting of bad label structure from tapes
  - #29217 We forget disk to disk copies after recalls
  - #29482 Missing cleanup of tapecopy+segment when GC'ing an INVALID diskcopy
  - #29775 RFE: Repack2 message and error visualization
  - #29776 RFE: checksum verification in Repack2
  - #29805 [xrootd] dangling "vmstat" processes
  - #30190 scheduling on server running xrootd
  - #30228 MigHunter gets free(): invalid pointer ...
  - #30346 RFE: stager_rm could be allowed to remove non-existing castor files
  - #30383 No factory found for object type 3 after upgrade to CASTOR 2.1.4
  - #30384 Prevent writing too many files when labeltype='al'
  - #30559 RFE improvements in Repack when diskcopies are available
  - #30825 diskcopies with status=null
  - #30826 CHECKFSEQ check in migrator incorrectly rejects new tapes
  - #30917 Timed out locate leaves zombie posovl processes behind
  - #31055 ForcedFileClass: constraint and check
  - #31684 RFE: Stager modification for repack
  - #31683 Repack long delay to submit requests
  - #28010 regular shrink of tables in ORACLE
  - #30852 add a -f (force option) to tplabel and do not overwrite labels by default
  - #31246 NS, VMGR and CUPV should reconnect to their databases after disconnection
  - #24106 Please replace multiple '/' on the file path with single '/'
  - #26029 disk to disk copy scheduling
  - #23218 File update/append does not update the size in nameserver
  - #21821 stuck file in case of nbStreams = 0
  Already in 2.1.4-10:
  - Fix bad processing after ENOENT in Cns_lastfseq() in the migrator
  - Fix for unique constraint violation caused by -insert tape- in OraCommonSvc
  - Fix in Repack to be able to start a new RepackSubrequest, even if there is a RepackSubrequest in Failed, Archived or Failed status, with the same files.
  - Fix in Repack to start the different RepackSubrequests associated to a RepackRequest even if one of them is stopped.
  Already in 2.1.4-9:
  - #30392 Cleanup for STAGED files coming back after FS was disabled incomplete
  - #30324 PutDones stuck waiting on failed puts
  - #30275 Subrequests stuck in WAITSUBREQ after bkill
  - #30223 rfiod does not write all data to disk upon rclose64_v3
  - #30125 socket leak when BaseClient objects are reused
  - Better SubRequest partitioning
  - gridFTP v2 now uses a port range
  - Improvements in gridFTP v2 packaging, especially around the handling of certificates.
  - Fix for more tapepool associated to a single svcclass
  Already in 2.1.4-8:
  - #30125 socket leak when BaseClient objects are reused
  - #30127 Failed transfers using root protocol
  - gridFTP v2 packages are now build as part of the releases
  Already in 2.1.4-7:
  - #29133 Stagein file size problem
  - #28705 Inconsistent increment of file counter in VMGR
  - #29775 Repack2 message and error visualization (updated)
  - Added recaller retry policies.
  - Fixed the wrong mode being displayed for queued requests when using showqueues 
    with the -x option.
  - Numerous modifications to the PLSQL code of the stager database to improve the error
    handling of recall failures observed during repack testing.
  - Added support for retrying files which have failed to be repacked at the end of a 
    repack run. The maximum number of attempts to make can be specified on the command 
    line with the -m option of the repack command.
  - Fixed the nameserver daemon on 64 bit operating systems. Prior to this fix any attempt
    from a daemon to update the segments on a tape would fail.
  Already in 2.1.4-6:
  - #29815 jobs submitted in wrong svcClass by JobManager
  - #20052 RFE to have thresholds for the number of migrators started (Experimental)
  - Fixes to the recaller. In the 2.1.4-5 release a situation could occur whereby 
    reading a set of files from the same tape could result in the mounting and 
    dismounting of the tape after every read of a single file.
  - Fixes to the stager so that StagePrepareToGet and StagePrepareToUpdate requests
    are correctly scheduled.
  - Added a second MIR check after a repair. The result will be logged and is to check 
    whether it's possible to "see" the MIR repair (avoid waiting for the next mount). 
    This info can then be used to deal with tapes where the SCSI repair commands 
    succeeded, but the MIR is invalid on the next mount.

  Upgrade from 2.1.4-*
  --------------------
    - You basically have to update to 2.1.4-10 and then follow the instructions for updating from 2.1.4-10.
    - You can however avoid to update the RPMs

  Upgrade from 2.1.4-10
  ---------------------
    - Before upgrading the CASTOR 2 head nodes, one should first upgrade the nameserver
       - This is actually mandatory if you want to have synchronization of diskservers with the nameserver
       - It is a pure RPM upgrade, no DB update
       - The 2.1.6 nameserver can be ran concurrently to an older one in case you have
         DNS load balanced clusters of nameservers, allowing an upgrade with no service interruption
    - Make sure that no LSF jobs stay in the queue. bkill all the remaining ones
    - Update the configuration of LSF
       - Modify the ego.cluster file and add a diskcopy resource to all diskservers.
       - Modify the ego.shared file and add the entry:
           diskcopy   Boolean ()       ()          (Disk Copy Resource, implies RFIO)
       - Add 'PARALLEL_SCHED_BY_SLOT = y' to the parameters section of the lsb.params file
       - Verify that the schmod_parallel module is enabled in lsb.modules
    - Stop all daemons and services (rhserver, stager, jobManager, rtcpclientd, cleaningDaemon, rmMasterDaemon, dlfserver, expertd, rmNodeDaemon, gcdaemon, LSF)
    - Optionally backup your stager DB (one never knows...)
    - Upgrade the stager database
       - Use script 2.1.4-10_to_2.1.6-7 (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.6-7/upgrades)
    - Update DLF DB using script dlf_2.1.4_to_2.1.6-7
       - (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.6-7/upgrades)
    - Update the rpms on the different nodes. Some changes were made in the list of packages to install :
       - The castor-gridftp-dsi-ext and castor-gridftp-dsi-int packages are now depending on castor-gridftp-dsi-common
       - There are 2 new packages to be installed on the rtcpcld head node : castor-lib-policy and castor-rechandler-server.
       - The mighunter package was renamed from castor-rtcopy-mighunter to castor-mighunter-server
       - The cleaningDaemon is now in its own package called castor-cleaning-server
    - Update your monitoring tools according to the daemon changes :
       - The stager became stagerDaemon
       - The MigHunter became migHunterDaemon
       - A recHandlerDaemon was added
    - Update the CASTOR configuration files on all machines
       - Update castor.conf using the template giving in castor.conf.example
       - Move the scheduling policy file on the scheduler machine :
           mv /etc/castor/policies.py /etc/castor/policies/scheduler.py
       - Move or create the policies that you need on the rtcpcld machine : /etc/castor/policies/stream.py, /etc/castor/policies/migration.py and /etc/castor/policies/rechandler.py
    - Restart all daemons
       - Don't forget the new ones : stagerDaemon, recHandlerDaemon (which is not mandatory if the recall policy is not used), migHunterDaemon 


-----------
- 2.1.5-2 -
-----------

  New Features and Changes
  ------------------------
  This CASTOR version is an internal one dedicated to the testing of the new stager.
  Full release notes will be provided with the 2.1.6 version.
  Note that this version is compatible with 2.1.4 in order to run the
  new stager on top of a standard 2.1.4 install. Hence, the release field in the db
  remains at 2.1.4-x.

  Upgrade from 2.1.4:
  -------------------
  - There's no need to stop the service
  - Run the 2.1.4_to_2.1.5-2.sqlplus script on the stager database
  - Install only the stager RPM and its dependencies on a dedicated node


-----------
- 2.1.4-5 -
-----------

  New Features and Changes
  ------------------------
  This part lists the changes since the first release of the 2.1.3 series, that is 2.1.3-14.
  Many of the fixes were backported to later releases of the 2.1.3 series, as mentioned.

    Broad view and stager
    ---------------------

    - Official support for head nodes on SLC4 32 and 64 bit.
    - rmmaster has been dropped and replace by a jobManager dealing with LSF and the job scheduling
    - initd scripts and sysconfig files have been cleaned up and improved
    - Introduced hooks for recall policies. Policies themselves are not implemented yet
    - Implemented stageRm per service class
    - Added ability to declare a svcClass as disk only. In this case, requests that need to write 
      to the diskpool (that is puts but also recalls and disk to disk copy) will fail if no space 
      is available instead of filling the LSF queue
    - Added ability to force the fileClass of new files in the disk only pool
    - 2 new status codes were added for SubRequest SUBREQUEST_BEINGSCHED and READYFORSCHED. These 
      are used by the jobManager
    - Handle properly cases where STAGEOUT diskcopies are unavailable (disable hardware) or in 
      another fileclass.
    - Added 'ENABLE ROW MOVEMENT' to all tables in order to be able to shrink them online
    - Better naming of version and releases within the stager DB
    - Introduced partitioning of the subrequest table. This allows to avoid function based indexes
      and thus to shrink tables dynamically. This was actually backported to 2.1.3-17.
    - Added support for white and blacklist in the request handler.
    - Added stage_version entry point in the stager API to get the stager version
    - Added support in the init scripts for running 2 request handlers on a single node, on for 
      users and one for admins

    DiskServer
    ----------

    - Added proper cleaning after disk to disk copy failure
    - Fail the job if the localhost does not match the hostname requested by the plugin (#28398)
    - Fixed interrupted RFIO transfers not being correctly committed to the stager (#28372)
    - When a diskserver is brought back into production all recalls for files available on the 
      diskserver are now cancelled and the subrequest is restarted (#20832) 

    Central Services
    ----------------
    - rfiod :
      + dropping secondary groups as well when doing setuid/setgid
    - nameserver :
      + allow nschclass to change the class for regular files, but only if the user has Cupv admin 
        privilege
      + Added missing man pages
    - rtcopy, MigHunter
      + Added /var/lib/castor as new character device directory for rtcopyd
      + getNumFilesByStream function was added to the TapeSvc and gives the number of files to be 
        migrated in a given stream. This will allow to have thresholds for the number of migrators 
        started (#20052). They will be implemented via streamPolicies attached to service class. 
        Not there yet.
    - DLF
      + added separate stager link to ns file id results column in the DLF GUI
      + added automatic creation of partition tablespaces
      + support for large password lengths in connect string
      + Added new loggin severity LOG_STANDARD. This is masking a standard set of rules, logging
        everything but the DEBUG level locally and everything but DEBUG and USAGE level remotely
      + Added dlf_isinitialized api call to determine whether the DLF api has been initialized or not

    Tape related
    ------------
    - Added load balancing of VMGR pools with tapes in multiple libraries (#27319)
    - Extension of rtcpd to log into DLF (using the tplogger interface of the tape part).
    - Removal of message daemon from the tape code. The "ask operator" actions in case of failures 
      have been replaced by RTB_NORETRY and RBT_FAST_RETRY for mount and unmounts, respectively.
    - Refinement of 'Volume in use' fix: re-check the mount status of the drive if a 'Volume in use' 
      is encountered. Despite the first (negative) check after EBUSY, the tape may still be mounted
      (due to a flawed SMC response or a too early check).
    - Refinement of EBUSY handling: an EBUSY error reported by the SCSI layer on a mount does not 
      necessarily mean that the mount failed. A simple retry will hence cause in some cases 'volume
      in use' errors, which lead to disabled tapes. If receiving an EBUSY, the code will now check 
      if the requested tape is mounted in our drive and retry only if appropriate.
    - Streamlined logging output in case of the new 'Bad MIR' handling in order to ease parsing.

    Bug Fixes
    ---------

    Only in 2.1.4 :
    - #15841 jobs that have 'no candidate' in LSF loop
    - #18777 rhserver has no man page
    - #19413 diskpool with no filesystem should cause jobs for this diskpool to fail
    - #20832 Optimization of tape recalls after DiskServer problem
    - #22178 closing a castorinstance to the users
    - #22403 In the DLF gui, config.php has to be "world" readable
    - #22969 rhserver doesn't reply if castorversion is the wrong one
    - #23268 nsfind gives you paths with double slashes this makes stager_qry screw
    - #23293 Impossible to only target one service class with stager_rm
    - #23303 Required improvements in CASTOR2 for proper handling of Disk1TapeX
    - #23710 Error information feedback from the repackserver
    - #24746 ORA-01480 error in nameserver
    - #25159 need for constraints of filesystem table to avoid orphaned filesystems
    - #25468 nsls with big directories fails in 64 bit architectures
    - #25997 Proper handling of castorVersion in DbCnvSvc
    - #26134 no interface to fail a transfer request in a proper manner
    - #26534 GC policies are not taking gcWeight attribute into account
    - #26711 stager_qry provides responses while it shouldn't
    - #26842 cleaning daemon connection to db is not closed
    - #27311 please ensure MAXREPLICANB is respected
    - #27319 RFE Loadbalancing of VMGR pools with tapes in multiple libraries
    - #27320 RFE Get rid of msgdaemon
    - #27331 Command line option to specify location of config files
    - #27660 File with two concurrent tape recalls that get stuck.
    - #27720 showqueues shows wrong queuing times on x86_64
    - #28009 relocation of rtcopy work directory
    - #28097 migration not optimized if nb of diskservers < N * nb of migrators
    - #28168 putDone may wait on puts when the puts are already over
    - #28182 CASTOR files can still end up with wrong size
    - #28308 missing man-pages in castor-ns-client package
    - #28336 A D2D copy running while a file is being migrated can cause inconsistent status
    - #28398 New variant of files being scheduled to wrong filesystem
    - #28411 rmAdminNode does not respect the deleted status of a diskserver when using the -f option
    - #28438 rfiod may get stuck waiting for data connection in ropen(64)_v3() 
    - #28564 rfiod not resetting secondary group ID's
    - #28619 Two concurrent stagerGetRequests may result in two DiskCopies in status WAITTAPERECALL
    - #28679 Core dump when user or group are not in passwd or group file
    - #28685 Possible file loss due to inconsistent vmgr_updatetape and Cns_setsegattr calls in 
             failing migrator
    - #28678 # of castor threads can go -ve
    - #28962 reporting of bad label structure from tapes
    - #28986 stager_qry -s output not always in synch with reality.
    - #29482 Missing cleanup of tapecopy+segment when GC'ing an INVALID diskcopy
    - #29499 showqueues drive status display filter
    - Fixed creation of the rtcopy work directory
    - Fixed incorrectly logged checksum with DLF_MSG_PARAM_INT64 leading to segfault for old 
      castorfiles without checksum
    Already in 2.1.3-24:
    - #28372 A file created in an interrupted RFIO transfer for a stagerPutRequest is not properly 
             committed to the stager
    - #27871 rfcp timeout creates "Device or resource busy"
    - #27196 request waiting on D2D copy
    - #26789 streamsToDo returns streams with nothing to do
    - #27206 bad decision to STAGEIN if diskcopy in CANBEMIGR in disabled filesystem
    - #27302 spacetobefreed has negative values
    - #27564 MigHunter startup does not check for duplicates
    - #27130 rmNodeDaemon problems on start-up
    - #27507 recaller crash for old castor files without checksum
    - 'volume in use' after EBUSY leading to disabled tapes
    - Streamlined logging output in case of the new 'Bad MIR'
    - getHostByName not thread safe, as well as the CASTOR clients using it
    - #27794 cleaning daemon too slow to cope with heavy loads
    - internalPutDoneFunc suffered from bad execution plans from ORACLE
    - corrupted ns hostname logging in repack
    - broken http downloads in stagerJob due to unintialised variable
    - fixed incorrect return code of stager_put in case of success.
    Already in 2.1.3-17:
    - Fixed updating of the stager database for monitoring information when a diskserver was deleted 
      via rmAdminNode
    - Fix garbage collector in case of FileSystems with no GC policy
    - fixed handling of command line options in modifyFileClass
    - flag tape as unused in resetStream when stream is in WAITDRIVER
    - fixed signal handling in rtcpclientd and reset streams with no active process on shutdown.
    - Added an automatic repair of corrupted tape directories (MIR). The repair is done by SPACE to 
      EOD and REWIND SCSI commands. The handling of bad MIRs is controlled via the new option 
      BADMIR_HANDLING.
    Already in 2.1.3-15:
    - Fixed in stager_qry for the case of a file with no diskcopy and no subrequest but which still 
      has a CASTOR file. We were answering STAGED instead of File Not Found in previous releases.
    - Added more retries on system and tape errors
    - Fixed iptable rules for the range of open ports on a Castor client (port 30101 was open by 
      mistake)
    - Use new error code ERTWRONGSIZE to flag tape recalls with wrong filesize
    - Added support for separate stager trace and rfiod log files
    - Avoided that segments stay in SEGMENT_SELECTED if filesize is wrong on a tape recall
    - Avoided deadlock with castor2 clients after errors in Ctape_reserve() or Ctape_mount()
    - Expanded the list of customised pending errors to help in operational debugging
    - Fixed missing Python 2.3 symbols at runtime
    - Changed signal handling to prevent mount/dismount race in case of a killed mount request.
      This should fix at least some of the 'volume in use' cases and the 'Destination Element Full' 
      problems.

  Upgrade from 2.1.3-*
  --------------------
    - you basically have to update to 2.1.3-24 and then follow the instructions for updating from 2.1.3-24.
    - you can however avoid to update the RPMs

  Upgrade from 2.1.3-24
  ---------------------
    - before upgrading the CASTOR 2 head nodes, one should first upgrade the nameserver
       - this is actually mandatory if you want to use disk only service classes
       - it is a pure RPM upgrade, no DB update
       - the 2.1.4 nameserver can be ran concurrently to an older one in case you have
         DNS load balanced clusters of nameservers, allowing an upgrade with no service interruption
    - make sure that no LSF jobs stay in the queue. bkill all the remaining ones
    - stop all daemons and services (rhserver, stager, rtcpclientd, cleaningDaemon, rmmaster, dlfserver, expertd, rmnode, gcdaemon, LSF)
    - optionally backup your stager DB (one never knows...)
    - upgrade the stager database
       - use script 2.1.3-24_to_2.1.4-5 (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.4-5/upgrades)
    - update DLF DB using script dlf_2.1.3-24_to_2.1.4-x
       (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.4-5/upgrades)
    - update the rpms on the different nodes. Some changes were made in the list of packages to install :
       - the castor-rmmaster-server should not be installed anymore on the rmmaster node (now called jobManager node)
       - the castor-jobmanager-server package should be installed on the jobManager node (former rmmaster node)
    - update your monitoring tools according to the daemon changes :
       - on the jobManager (ex rmmaster) machine, jobManager replace rmmaster
    - update the CASTOR configuration files on all machines
       - update castor.conf using the template giving in castor.conf.example
       - take care that the RmMaster/SharedLSFResource target changed to JobManager/SharedLSFResource
    - restart all daemons
       - don't forget that rmmaster is replaced by jobManager
    - for the dlf web interface move the login configuration to its new location
       - mv /var/www/html/dlf/login.php.rpmsave /var/www/conf/dlf/login.conf
       - chown root:apache /var/www/conf/dlf/login.conf
       - chmod 640 /var/www/conf/dlf/login.conf

------------
- 2.1.3-24 -
------------

  Bug Fixes
  ---------
  - #27196 request waiting on D2D copy
  - #26789 streamsToDo returns streams with nothing to do
  - #27206 bad decision to STAGEIN if diskcopy in CANBEMIGR in disabled filesystem
  - #27302 spacetobefreed has negative values
  - #27564 MigHunter startup does not check for duplicates
  - #27130 rmNodeDaemon problems on start-up
  - #27507 recaller crash for old castor files without checksum
  - 'volume in use' after EBUSY leading to disabled tapes
  - Streamlined logging output in case of the new 'Bad MIR'
  - getHostByName not thread safe, as well as the CASTOR clients using it
  - #27794 cleaning daemon too slow to cope with heavy loads
  - internalPutDoneFunc suffered from bad execution plans from ORACLE
  - corrupted ns hostname logging in repack
  - broken http downloads in stagerJob due to unintialised variable

  Upgrade from 2.1.3-17
  ---------------------
  - GRANT new privileges on the DLF DB. Here is an example of what to run,
    has to be modified for each site :
      alter system set db_create_file_dest='/ORA/dbs03/oradata/CASTORDLF/' scope=both ;
      grant create tablespace to castor_dlf;
      grant unlimited tablespace to castor_dlf;
      grant alter tablespace to castor_dlf;
  - Apply the SQL upgrade script to the dlf DB
  - Stop all daemons
  - Apply the SQL upgrade script to the stager DB
  - Upgrade all nodes
  - Restart all daemons

------------
- 2.1.3-17 -
------------

  New Features and Changes
  ------------------------
  - Better naming of version and releases within the stager DB
  - introduced partitionning the subrequest table. This allows to avoid
    function based indexes and thus to shrink tables dynamically
  - Fixed updating of the stager database for monitoring information when
    a diskserver was deleted via rmAdminNode
  - Fix garbage collector in case of FileSystems with no GC policy
  - Added check of the version of the dlf schema at startup of the dlf server
  - fixed handling of command line options in modifyFileClass
  - flag tape as unused in resetStream when stream is in WAITDRIVER
  - fixed signal handling in rtcpclientd and reset streams with no active
    process on shutdown.
  - Added an automatic repair of corrupted tape directories (MIR).
    The repair is done by SPACE to EOD and REWIND SCSI commands.
    The handling of bad MIRs is controlled via the new option BADMIR_HANDLING.
  - Adapted repack to castor 2.1.3

  Upgrade from 2.1.3-15
  ---------------------
  - Stop all daemons
  - Apply the SQL upgrade script to the stager DB
  - Upgrade all nodes
  - Restart all daemons

------------
- 2.1.3-15 -
------------

  New Features and Changes
  ------------------------
  * Fixed in stager_qry for the case of a file with no diskcopy and no subrequest but which still has a CASTOR file.
    We were answering STAGED instead of File Not Found in previous releases.
  * Added more retries on system and tape errors
  * Fixed iptable rules for the range of open ports on a Castor client (port 30101 was open by mistake)
  * Use new error code ERTWRONGSIZE to flag tape recalls with wrong filesize
  * Added support for separate stager trace and rfiod log files
  * Avoided that segments stay in SEGMENT_SELECTED if filesize is wrong on a tape recall
  * Avoided deadlock with castor2 clients after errors in Ctape_reserve() or Ctape_mount()
  * Expanded the list of customised pending errors to help in operational debugging
  * Fixed missing Python 2.3 symbols at runtime
  * Changed signal handling to prevent mount/dismount race in case of a killed mount request.
    This should fix at least some of the 'volume in use' cases and the 'Destination Element Full' problems.

  Upgrade from 2.1.3-14
  ---------------------
  - There is no need to stop the service for this upgrade
    and the order in which the update steps are taken does
    not matter
  - Upgrade all head nodes and tapeServers
  - Apply the SQL upgrade script

------------
- 2.1.3-14 -
------------

  New Features and Changes
  ------------------------
  They are so numerous here that they were divided into categories

    Broad view and transverse items
    -------------------------------
    - the monitoring has been fully rewritten, see details below
    - the LSF plugin has been fully rewritten, see details below
    - clips was dropped, python replaces it
    - fully qualified domain names are used everywhere
    - all executables and libraries have a new symbol storing the CASTOR version number.
      just do an "nm <exe or lib file> | grep CastorVersion" to get it
    - putDones are not scheduled anymore
    - the maximum file name length in the nameserver was changed from 231 to 255 characters
    - garbage collection on diskservers was "desynchronized". The point is to avoid that all GCs
      start concurrently both for load and smoothing reasons

    Monitoring
    ----------
    - rmmaster daemon is now only doing job scheduling and is not used anymore for monitoring
      although it is still active. As a consequence, rmgetnodes won't give anything
    - as a corrolate, rmnode is no more distributed and does not run anymore on the diskservers.
      rmadminnode should not be used anymore and the fileSystem service of the stager was dropped.
    - the new monitoring daemon is called rmMasterDaemon and should run on the scheduler node.
      It is distributed in the same package castor-rmmaster as rmmaster so this package should be
      installed on both the scheduler node and the rmmaster node even if both daemons are not
      needed in both places.
    - rmMasterDaemon uses shared memory to store the data. This set of memory will survive it
      in case of crash and be reused when restarting. In case of reboot, it can be reinitialized from
      the data contained in the stager DB, which are constantly updated.
    - the new command lines are rmAdminNode and rmGetNodes (the second one being only available on
      the scheduler node).
    - the new monitoring daemon is called rmNodeDaemon and is distributed in castor-rmnode. On top
      of monitoring the node and sending data to the rmMasterDaemon, it also keeps the current status
      of the node in the daemon in /etc/castor/status (default). It initializes the shared memory when
      starting by reseting the metrics, setting statuses to Down and Admin statuses to what they are
      in the stager DB.
    - the new monitoring monitors :
      - for each diskserver : ram(total+free), memory(total+free), swap(total+free),
        nbRead/ReadWrite/Write/Recaller/MigratorStreams, read/writeRate, status and adminStatus
      - for each filesystem : space(total+free), nbRead/ReadWrite/Write/Recaller/MigratorStreams,
        read/writeRate, status and adminstatus
    - the status can be Production, Down, Draining.
    - the admin status can be None, Force or Deleted. If None, the status in the master is updated according
      to monitoring. It Force, it is not updated. If Deleted, the diskServer/FileSystem is dropped from the
      stager database and will disappear at the next reset of the shared memory (e.g. reboot).
    - the new monitoring system is able to disable diskservers that did not send data for too long

    Around LSF
    ----------
    - there is a new LSF plugin with same name and same interface as the old one.
    - it uses shared memory to get data from the rmMasterDaemon and thus has to run on the same machine
    - this shared memory is also accessed by rmGetNodes that basically dumps it
    - In case of corruption, the way to reset the shared memory is to stop rmMasterDaemon and LSF and
      use ipcrm. the key used is 0x946
    - scheduling policies are now defined in python and can be defined per svcclass.
      The python code is precompiled at plugin start in order to speed up the execution.
      Policies can be updated with no service interuption by reloading the plugin (badmin reconfig).
      Default policy takes only the number of streams per filesystem into account.
    - CASTOR now uses multiple queues in LSF, namely one queue per svcclass
    - we don't use message boxes anymore inside LSF. The first one was dropped while the second one is
      replaced by file sharing, either via distributed filesystem or via a web server running on the
      scheduler machine. This also implies that jobs are no longer suspended and resumed.

    Around the stage DB
    -------------------
    - all indexes and constraints in the stager DB schema are now properly named
    - reservedSpace has been dropped from the filesystem table. It is replaced by loose heuristics
      leading to imprecisions on the actual and expected free space but avoiding any accumulation
      problems.
    - the nullGCPolicy does not exist anymore since "INVALID" diskcopies are cleaned up independently.
      In order to not use any garbage collection policy, an empty name should be used.
    - the way to choose the best filesystem to be used for migration (migration policy) has changed
      to favor the reuse of filesystems. Namely, filesystems will be reused during 15 minutes by
      migrators (as long as there are candidates and the filesystems are in Production). This allows
      the scheduler plugin to adapt its behavior by reducing the number of concurrent reads and has
      very good effects on the migration speed.
    - many schema changes took place. Here is an overview of the biggest ones :
       - min/max/minAllowedFreeSpace were dropped. They now reside in the monitoring shared memory
         of the new rmMasterDaemon and are defined in the castor.conf of the diskServers
       - the load of diskServers and the weight of fileSystems have been dropped. They are replaced by
         nbRead/ReadWrite/Write/Recaller/MigratorStreams and read/writeRate. The values stored in the
         database are actually only copies from the mnitoring shared memory that is the primary source.
       - procedure bestFilesystemForJob is gone, replaced by the python policies of the scheduler plugin.
       - reservedSpace disappeared from the filesystem table as already mentionned.
       - the stream table has new columns : lastFileSystemChange, lastFileSystemUsed and lastButOneFileSystemUsed.
         This allows the migration policies to be more clever (see above).

    Tape related
    ------------
    - Extended tpdaemon to use proper DFL logging
    - Concerning bad MIRs, take the operator out of the loop and rely on the healing capabilities of the
      drive during the next access. This can be controlled by new option TAPE CANCEL_ON_BADMIR.
    - a restriction was added on the repack server to dismiss users with 0 id
    - the repack -a has been modified. "repack -a Vid" is used to archive a specific finished tape
      while "repack -A" archive all the finished one.
    - repack -x has been extended to give information about the different segments that were on a
      repacked tape querying the name server.
    - A new couter has been added to repack with the STAGED files (properly migrated) and it will be
      updated by the monitoring. At the end the last snapshot is taken before deleting the subrequest.

    Bug Fixes
    ---------
    - #26534  GC policies are not taking gcWeight attribute into account
    - #26489  Allow results to be displayed in reverse chronological order
    - #26177  putDone does not to update the filesize in the nameserver?
    - #26176  Accumulation of orphaned subrequests in SUBREQUEST_WAITSUBREQ (5)
    - #26135  Customized pending reason number 20003
    - #26060  enterSvcClass should fail if it detects illegal options
    - #25344  misleading error message from rfcp
    - #25029  reservedSpace leak for SRM 'put' request cycles
    - #24864  Tape recalls do not work properly on 2.1.2.6 (itdc)
    - #24407  errcode == 32102 doesn't resurrect
    - #23961  Tape removed from vmgr makes rtcpclientd loop
    - #23795  UID zero checking 
    - #23794  FAILING file count
    - #23793  repack -a
    - #23709  Kill and terminate a repack session 
    - #23705  repack -s status information
    - #21953  problem with subdirectory creation on diskservers 
    - #21823  putDone failure not well handled if file does not exist on diskserver
    - #21745  RFE: CASTOR to support FQDN internally for disk servers
    - #20353  The 'no valid copy found on tape' instead of 'Internal error' is still not fixed!
    - #20079  filesystems not being updated are not taken out of production!
    - #17153  LSF job can remain in PSUSP after a timeout between stager - rmmaster
    - #15832  RMMASTER does not keep node states
    - #11550  Leak in stager
    - #1288   rfmkdir -p
    - the default values for the cleaning daemon time intervals were fixed
    - a PutDone is now accepted on a disabled diskserver
    - read only jobs go through whatever the space situation is. However "false" read jobs
      (read jobs that would trigger disk2diskCopy or recalls) don't
    - fixed automatic renewal of the database connection for all hand-written services
    - regular retries on disabled or archived tapes
    - jump over files with bad size in tape recalls. This avoids looping blocking other correct files.
      There will still be an alert log for the wrong file.
    - fixed tape stuck in START problem. The code ignores consistency problems with the status of a tape in VMGR.
    - fix for the device or resource busy problem leading to put all drives down. A simple retry worked here.
    - fix tape stuck in UNKNOWN status in case of tape alert. The new parameter DOWN_ON_TPALERT allows
      to configure whether to put the drive down or to ignore the alert.
    - in case of time out in contacting the stager the  repack subrequest is not ending up into "FAILED" status.

  Upgrade from 2.1.1-*
  --------------------
    - you basically have to update to 2.1.2-x (with x >= 4) and then follow the instructions for updating from 2.1.2-*.
    - you can avoid to update the RPMs
    - concerning the stager database updates :
       - if you start from a 2.1.1-4, first run 2.1.1-4_to_2.1.1-9
         http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.1-9/upgrades/
       - from a 2.1.1-9, update to 2.1.2-4 using 2.1.1-9_to_2.1.2-4
         http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.2-*/2.1.2-4/upgrades/
    - concerning the DLF database :
       - if starting from a 2.1.1, drop and recreate the DLF DB.
         The easiest is to use the 2.1.3 drop script (a generic one that can be applied to any version)
         and recreate the DLF schema using the 2.1.2-4 script
         (both can be found on http://castor.web.cern.ch/castor/software.htm)

  Upgrade from 2.1.2-*
  --------------------
    - first make sure that no LSF jobs stay in the queue. bkill all the remaining ones
    - stop all daemons and services (rhserver, stager, rtcpclientd, cleaningDaemon, rmmaster, dlfserver, expertd, rmnode, gcdaemon, LSF)
    - optionnaly backup your stager DB (one never knows...)
    - upgrade the stager database
       - first save somewhere the values of min/max/minAllowedFreeSpace for all filesystems. They will needed
         when upgrading the diskservers.
       - use script 2.1.2-*_to_2.1.3-14 (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.3-14/upgrades)
    - upgrade the diskServer names in the stager database to include the fully qualified domain :
         UPDATE DiskServer SET name = name || '.cern.ch';
         COMMIT;
    - update DLF DB using script dlf_2.1.2-x_to_2.1.3-x
       (http://castor.web.cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.3-14/upgrades)
    - update the rpms on the different nodes. Some changes were made in the list of packages to install :
       - on the scheduler machine, castor-rmmaster and castor-lib-monitor should be added
       - on the diskservers, castor-lib-monitor should be added
       - clips can be dropped from the expert machine
    - update your monitoring tools according to the daemon changes :
       - on the diskServer, rmnode was replaced by rmNodeDaemon
       - on the scheduler machine, rmMasterDaemon was added
    - upgrade to LSF7 and modify the LSF configuration files :
       - use fully qualified machine names :
          - update machine names accordingly in all config files
          - remove LSF_STRIP_DOMAIN from lsf.conf on all machines
       - define one queue per svcclass in lsb.queues
       - remove or increase the 1000 jobs limitation in lsb.params
          - e.g. CERN has a 30000 limit on the number of jobs
       - allow for extsched option in job submission :
          - add LSF_ENABLE_EXTSCHEDULER in lsf.conf of the scheduler and rmmaster machines
    - define python scheduling policies :
       - create the policy python file from the .example one (/etc/castor/policies.py.example)
       - optionnaly modify it to adapt policies
       - optionnaly define different policies for different service classes
    - update the CASTOR configuration files on all machines
       - update castor.conf using the template giving in castor.conf.example
       - define the way to share files between the scheduler machine and the diskservers
         - on the scheduler machine, define the Sched/SharedLSFResource option giving the place where to put files
           Note that this ca be either a shared filesystem or a local directory accessible through a web server
           In both cases, lsfadmin needs write access to the directory
         - on the rmmaster machine, define the Job/SharedLSFResource option giving the place where diskservers
           can find the files. Again, this can be either a shared filesystem or an http location (e.g.
           dir:/var/lsf/sharedfiles or http://castorscheduler.cern.ch/lsf)
       - on diskservers :
         - edit castor.conf to define the proper name for the RM/HOST option (scheduler machine)
         - edit castor.conf to define proper mountPoints in the RmNode/MountPoints option. e.g.
           RmNode MountPoints /srv/castor/01/ /srv/castor/02/ /srv/castor/03/
         - optionnaly define option RmNode/StatusFile in castor.conf. The given file will contain the filesystem
           and node status as seen by the central servers, constantly updated
         - if needed, edit castor.conf to define the min/max/minAllowedFreeSpace according to the values
           saved from the stager DB. Default values are .10, .15 and .05
    - restart all daemons
       - on diskservers : rmNodeDaemon, gcDaemon
       - on scheduler : rmMasterDaemon, LSF
       - on other nodes, nothing changed (including on rmmaster, the old rmmaster must still run)
    - enable diskservers one by one using rmAdminNode :
         rmAdminNode -n <fully qualified node name> -r -R

-----------
- 2.1.2-10-
-----------

  New Features and Changes
  ------------------------
  This release brings selected fixes and no new functionality with respect to the previous 2.1.2:
  - Fix the back signal handler modifications from 2.1.3

  Upgrade from 2.1.2-9
  --------------------
  - Upgrade all head nodes (the order is not important)

-----------
- 2.1.2-9 -
-----------

  New Features and Changes
  ------------------------
  This release brings selected fixes and no new functionality with respect to the previous 2.1.2:
  - Fixes for reservedSpace accumulation in case of failed Puts and Puts from SRM
  - Fixes in the stager to prevent segmentation faults in case of exception handling for tape recalls
  - Fix in the scheduling of disk-to-disk copies
  - Fix for the stagemap handling when only the user gid is mapped in stagemap.conf

  Upgrade from 2.1.2-6/7
  ----------------------
  - Upgrade the database 
  - Upgrade all head nodes (the order is not important)


-----------
- 2.1.2-7 -
-----------

  This release is broken in disk-to-disk copy handling.


-----------
- 2.1.2-6 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes:
   - Fixed potential double/invalid frees in the database access layer
   - Fixed error checking in the PrepareToGet in case of tape recalls
   - Bug Fixed in repack worker and monitoring now is able to deal with the stager timeout.
   - Fixed the socket leak in DLF client when the DLF server is not reachable
   - Fixed the stagerJob for xroot. We were calling prepareForMigration while xroot does it for us.
   - Optimized version of stage_qry
   - Fixes for the processing of update requests
   - Fixed the reservedSpace accumulation problem
   - Fixed garbage collector job
  
  Upgrade from 2.1.2-4
  --------------------
  - Upgrade the database 
  - Upgrade all head nodes (the order is not important)


-----------
- 2.1.2-4 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes and improvements:
  - A PrepareToGet request is not scheduled anymore if the requested file is to be recalled from tape
  - Bugfixes and finalization of the Repack system
  - Implemented Repack check and Repack undo
  - Fixes around the DLF and the web interface
  - Fixed all known database deadlocks to date
  - Reconnection to the db more robust in case of transient failures, many more are recognized now
  - Cleaned NS clients and CUPV code for 64bit
  - Many optimizations in the PL/SQL code
  - Minor fixes to be able to compile the C++ part with gcc-3.4 and gcc-4.1 without warnings
  - Fixed a problem in TapeErrorHandler to allow tape retries upon error
  - Fixed query used by the cleaningDaemon to delete old failed subRequests
  - Bugfix for #22620: Uncaught C++ exception causes TapeErrorHandler to Abort
  - Bugfix for #22382: rtcpclientd does not stop gracefully upon signal
  - Bugfix for #21824: internalPutDoneFunc tries to delete the prepareTo request too hard
  - Bugfix for #21541: change location of RTCOPY
  - Bugfix for #21482: bad values on defaultFileSize when using modifySvcClass
  
  Upgrade from 2.1.1-9
  --------------------
  - Stop all the CASTOR daemons
  - Upgrade the database; new database code does not work with old stagers
  - Upgrade all nodes (the order is not important) and restart the service


-----------
- 2.1.1-9 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes and improvements:
  - Improvements around the Repack system, added Repack multi-stager ability
  - Bugfix for #21093: strange files created on local disk, when DLF is not able to log
  - Bugfix for #20069: if user has the wrong (non existent) svcclass there is no intelligible error reported
  - Bugfix for very large files
  - Improved selection of migration candidates via PL/SQL procedure
  - Improved port handling in clients in case of many concurrent callbacks on a small port range
  - Improved stager_qry in case of castorFiles with many diskCopies
  - Bugfix for stager_qry (#19044, #19836)
  - Improved logging in the Request Handler
  - Support for 700Gb tapes

  Upgrade from 2.1.1-4
  --------------------
  - Upgrade the database
  - Upgrade all nodes (the order is not important).


-----------
- 2.1.1-4 -
-----------

  New Features and Changes
  ------------------------
  This release brings a number of bug fixes:
  - Fixes around the Repack system
  - Improved cleaning procedures and config for the cleaningDaemon, now it shares castor.conf
  - Fixed Oracle deadlock which was leading to many diskcopies left in status 9
  - Fixed Oracle deadlock in streams leading to inconsistencies in NbTapeCopiesInFS
  - Fixes in the migrator
  - Improved GC trigger to resurrect the GC job if it previously died
  - Improved stagerJob to prevent it to loop forever for a missing msgbox 4 from LSF

  Upgrade from 2.1.1-0
  --------------------
  - Upgrade the database
  - Upgrade the head nodes and the diskservers (the order is not important).


-----------
- 2.1.1-3 -
-----------

  New Features and Changes
  ------------------------
  This release brings the new Repack for CASTOR2 system, and a number of bug fixes:
  - Fixed leak of stager_rm subrequests in status ready
  - Fixed deadlock in the updateFsFileOpen/Close SQL methods
  - Fixed stager_qry for prepareToPut requests
  - Fix concerning retrieval of castorversion from stagemap if the env variable is not set
  - Take '*' as a wildcard for the svcClass in a stager_qry
  - Changed compression factor for IBM3592 in order to activate NVC bit
  - Included modifications for xrootd

  Upgrade from 2.1.1-0
  --------------------
  - Upgrade the head nodes
  - Then for the other nodes no special order is required.


-----------
- 2.1.1-2 -
-----------

  - This release is broken in the rtcpclientd.


-----------
- 2.1.1-1 -
-----------

  New Features and Changes
  ------------------------
  This release is exactly the same as the 2.1.1-0 except for a minor fix on the DLF server.
  It has been rebuilt to widely deploy it at CERN, as the first attempt for the 2.1.1-0 had
  a major problem leading to backward incompatibilities on the client.
  Note that the RPMs in the castor download area are correct for both 2.1.1-0 and 2.1.1-1.

  Upgrade from 2.1.1-0
  --------------------
  Not needed. If you're running an older version, please follow upgrade instructions below.


-----------
- 2.1.1-0 -
-----------

  New Features and Changes
  ------------------------
  This is a major release with many fixes and features. See the changelog for all the details.
  The most important new features are:
  - New rewritten version of DLF
  - Resurrection of the db connection when it's lost
  - Many fixes to the SQL code
  - Fixed some deadlock between the GC and the updateFsFileClosed PL/SQL function
  - Fixed the random port issue on the client
  - Changed default nb of threads to 20 for DB and JOB (optimal value according to Olof's tests)
  - Modifications to be ready with RFIO merge
  - Increased the number of test suites
  
  Upgrade from 2.1.0-x
  --------------------
  The upgrade to be done concerns the whole system (database, stager, clients, tape servers),
  and backward compatibility is ensured only if clients are uprgraded BEFORE servers.
  - Upgrade all clients (they can still connect to an OLD stager).
  - Upgrade the stager database (you don't need to stop the services nor daemons for that).
  - Drop and recreate the DLF database.
  - Then upgrade the head nodes.

-----------
- 2.1.0-8 -
-----------

  New Features and Changes (from 2.1.0-7)
  ---------------------------------------
  - Fixed temporaly the problems with rfcp command for CASTOR1

  Upgrade from 2.1.0-7
  --------------------
  Only CLIENTS using CASTOR1 should upgrade. For CASTOR2 there are no changes
    
-----------
- 2.1.0-7 -
-----------
  
  New Features and Changes (from 2.1.0-6)
  ---------------------------------------
  - Fixed a problem with the memory leak in RFIO
  - Temporary fix, the port used in the bind is now a pseudo-random value that change for each request.To reduce the probability that the answer of an aborted request is returned to a new one.

  Upgrade from 2.1.0-6
  --------------------
  Non special order is required.
  
-----------
- 2.1.0-6 -
-----------
  New Features and Changes (from 2.1.0-5)
  ---------------------------------------
  - Made the output of stager_qry -s less wide
  - Added CF.lastKnownFileName as a result of diskServer_qry -i 
  - Mir valid/invalid checking on IBM3592E05 tape drives, new IBM3592E05 firmware release (172E) fixed problem seen before.
  - Connection reset messages from rmc are just sent to msgdaemon after 2 retries.
  - Rmcdaemon now checks if tape is in the storage slot before returning dismount OK
  - Fixed the cleaning of the DiskCopy in case of put failure. This was leading to orphaned DiskCopies
  - Fixed a bug in updateFiles2Delete
  - Handle properly exceptions in sendRequest and especially release memory and close the socket
  - Fixed missing filling of the svcclass of a castorfile in memory.The consequence was that the svcclass could be overwritten in case  of reuse of a castorfile in 2 different service classes


  Upgrade from 2.1.0-5
  --------------------

  - Upgrade the Database
  - Upgrade the head nodes
  - Then for the other components there are no restrictions in that release.

-----------
- 2.1.0-5 -
-----------
  New Features and Changes (from 2.1.0-5)
  ---------------------------------------
  - Fixed some deadlock between the GC and the updateFsFileClosed PL/SQL function
  - Fixed a missing chkconfig line in debian/cleaning.init
  - RFIO_USE_CASTOR_V2 NOT case sensitive.
  - Fixed the bug of permissions when creating a file (stager_put)
  - Remove check for already migrated files preventing recreated files from being migrated
  - Fixed stage_open, broken since the integration of O_LARGEFILE
  - Fixed tpdump for toot usage

  Upgrade from 2.1.0-4
  --------------------

  - Upgrade the Database
  - Upgrade the head nodes
  - Then for the other components there are no restrictions in that release.

-----------
- 2.1.0-4 -
-----------
  New Features and Changes (from 2.1.0-3)
  ---------------------------------------
  This is a bug fix release, strictly identical to 2.1.0-3 with some exceptions:
  - The name server client commands(nsln, nsgetacl and nssetacl) for manipulating the ACLs are included in castor-ns-client package
  - Fixed TURL parsing in rfcp
  - Updated the man pages with the new turl
  - Added some changes in the stager and disk server to support xrootd protocol
  - Fixed bug #17720, backport fix in rtcpcld_putFailed from 1.143.0.2
  - Added message when skipping not-staged files
  - Stager client command line is CASTOR2 by default (RFIO_USE_CASTOR_V2 doesn't need to be set with this client version)
  - Check of tapealerts hard error, media error, write failure, read failure at release time. 
  - Removal of MIR validity check, on IBM3592EO5 in some situation gives false positive results.
  

  Upgrade from 2.1.0-3
  --------------------

   - The upgrade to be done concerns to the whole system but the database( stager, client, disk servers and tape servers)
   - There are no specific modifications of the database
   - If your are upgrading 2.1.0-3 no order is required, but if you are upgrading from 2.1.0-2 or previous versions please follow the order specified in "2.1.0-3 upgrade from 2.1.0-2"  
   



-----------
- 2.1.0-3 -
-----------
  New Features and Changes (from 2.1.0-2)
  ---------------------------------------
  This is a bug fix release, strictly identical to 2.1.0-2 with some exceptions:
  - Fixes concerning the TURL parsing in RFIO
  - The garbage collection was optimized 
  - Added the check of MIR validity at mount time for SUN/STK T10000, LTO and IBM3592
  - Fixes concerning the deadlocks in PL/SQL and avoiding replication of STAGEOUT files
  - Added dependency on castor_lib_compact 2.0.2_1 to avoid relinking.
  - Fixed the inicialitation of the file systemd for minAllowedFreeSpace
  

  Upgrade from 2.1.0-2
  --------------------

    - The upgrade to be done concerns the whole system (database, stager, client, tape servers)
    - Upgrade the database (you don't need to stop the services nor daemons for that).
    - Then upgrade the rest of the system. 


  

-----------
- 2.1.0-2 -
-----------

  New Features and Changes (from 2.1.0-0)
  ---------------------------------------
  
  This is a bug fix release, strictly identical to 2.1.0-0 except
  for the tape part.
  On previous releases of Castor, the report of compression statistics
  from SUN T10000 tape drives was wrong. This behaviour was due to the
  wrong handling of the T10000 device when decoding the sequential log page.
  Castor will report wrong compression statistics on IBM3592 tape drives
  if the tape drive is set with NVC mode on. The tape drive will only post
  correct statistics after the flush of the NVC buffer, this behaviour is
  particularly seen when files are smaller than the NVC buffer size
  (~300MBytes).

  Upgrade from 2.1.0-0
  --------------------

    - The only upgrade to be done concerns the tape servers.


-----------
- 2.1.0-1 -
-----------

  New Features and Changes (from 2.1.0-0)
  ---------------------------------------
  This is not really a release but modifications at the rpm level
  
-----------
- 2.1.0.0 -
-----------

  New Features and Changes (from 2.0.4-1)
  ---------------------------------------

  This version is strictly identical to 2.0.4-1.
  It is however prefered to 2.0.4-* since the soname of the libraries changed
  which was needed after we introduced some non backward compatible change in
  the interface.

  Upgrade from 2.0.4-1
  --------------------

    - The only upgrade to be done concerns the castor.conf: it has to be
      upgraded everywhere (head nodes and diskservers) with the new library
      name (ends with 2.1 instead of 2.0).
    - Note that for the client part, we advice to let castor-lib 2.0.* live
      aside the new one, so that applications linked with libshift.2.0 don't
      break suddenly. Once all users have relinked there applications with
      the new version, it can be removed.

-----------
- 2.0.4-1 -
-----------

  New Features and Changes (from 2.0.4-0)
  ---------------------------------------
    
    - Fixes concerning the TURL parsing in RFIO
    - Implementation of a port range for the ports used by the stager clients.
      Default range in [30000,30100], it can be overwritten in castor.conf
    - The file permission checks were fixed in the stager
    - The getNext functionnalities were made available to update requests
    - The cleaning daemon now has an init script
    - The flags given to RFIO when opening a file are now correctly handled
    - Queries on directories are now properly handled
    - SQL scripts are now also provided for sqlplus
    - The garbage collection was fixed (it was broken in 2.0.4-0 due to the new archiving of subrequests)
    - stager_qry -s was fixed (ordering was wrong)

  Upgrade from 2.0.4-0
  --------------------

    - The database schema has not evolved. However, some fixes in the PL/SQL code
      were added. Use upgrades/2.0.4-0_to_2.0.4-1.sql to upgrade.
    - Clients can be updated completely independently (before or after the server
      update).

-----------
- 2.0.4-0 -
-----------

  New Features and Changes (from 2.0.3-0)
  ---------------------------------------

    - Fixes concerning the IBM 3592 tapes
    - Support for preallocation and direct i/o were added to RFIO
    - A Cleaning service was added that cleans up old requests
      and errors after some (configurable) time
    - stager_qry support for regexp was improved. Regexp is
      no more the default but can be activated using options.
      Directory listing is also supported and several fixes were applied.
    - stager_qry -s was implemented for getting statistics on diskpools
    - (prepareTo)Get/Put requests now detect directory names
    - RFIO was modified for supporting extended TURLs needed for
      SRM : rfio://stager:port//castor/.../file?svcclass=...&castorversion=...
    - /etc/castor/ORASTAGERCONFIG can now contain several lines in order
      to be able to switch between several CASTOR2 instances. The environment
      variable CASTOR_INSTANCE defines which line to use
    - VMGR now updates correctly the estimated free space, even for full tapes
    - rmnode has no more default for the RMMASTER_HOST. It raises an error
      if nothing is set.
    - Better logging of rmmaster to DLF

  Upgrade from 2.0.3-*
  --------------------

    - The database schema has evolved. Use upgrades/2.0.3_to_2.0.4.sql to upgrade.
      This can be done anytime before the software upgrade and does not imply any
      service interruption.
    - The server code can be updated after the DB, still without service interruption
      if you play with several processes.
    - Clients can be updated completely independently (before or after the server
      update). However, new functionnalities will lead to strange errors in case
      servers were not updated, typically :
          > stager_qry -s
          Error: Invalid argument
          stage_diskpoolsquery: Server Error 22 :
          Unable to read Request object from socket.
          No factory found for object type 103 and representation type 1

-----------
- 2.0.3-5 -
-----------

  Prerelease of future 2.0.4-0. To be tested.

-----------
- 2.0.3-4 -
-----------

  Bug fix release. Should again only be used for IBM 3592 taperservers

-----------
- 2.0.3-3 -
-----------

  Bug fix release. This release should be used in place of v2_0_3_0. It fixes
  a packaging bug that was introducing a symbol main in the library libshift.
  It is otherwise fully identical to v2_0_3_0.

-----------
- 2.0.3-2 -
-----------

  Bug fix release. Should again only be used for IBM 3592 taperservers

-----------
- 2.0.3-1 -
-----------

  Bug fix release. Should only be used for IBM 3592 taperservers

-----------
- 2.0.3-0 -
-----------

  New Features and Changes
  ------------------------

    - Support for ACLs in the nameserver
    - Regular expression support in stager_qry. The feature is
      limited on the server side via the FILEQUERY/MAXNBRESPONSES
      entry in the castor.conf.
    - LSF plugin is using new fixes coming with LSF 6.1-24. This
      allows to remove an internal fork. This change is incompatible
      with any old version of LSF
    - Interface for passing an authorization id to the stager on top
      of a user id.
    - Framework for multithreaded applications was ported to C++
    - Fixed compilation for x86_64 architecture
    - New package castor-dlf-gui
    - tpdaemon uses sg0 device instead of sga
    - Write filemarks with sg driver and immediate bit set

  Upgrade from 2.0.2-*
  --------------------

    - LSF 6.1-24 or higher must be used
    - The database schema has evolved. Use upgrades/2.0.2_to_2.0.3.sql to upgrade
    - Once the database is upgraded, the content of the lastKnownFileName column
      of the CastorFile table is still empty. It will be updated when the files
      are accessed but you can update it by hand using upgrades/2.0.2_to_2.0.3.pl
      Note that the only disadvantage of not updating is that stager_qry -M will
      not return files that were not updated.
    - For the nameserver, the database must be updated first, using the script
      ns/Cns_oracle_add_ACL.sql. Then the daemons can be updated and finally the
      clients. Note that each step is backward compatible, so that an old daemon
      can run on a new database and an old client can contact a new daemon.

