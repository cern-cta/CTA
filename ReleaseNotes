-----------
- 2.1.8-3 -
-----------

  Bug Fixes
  ---------
  #2589:  Name server command(s)
  #6019:  Disk Resident check sums
  #20356: An interrupted put followed by another put for same file may be processed in wrong
          order
  #21783: listSvcClass gives oracle errors
  #22102: 'stager_qry -s' output wrong in case of a disk pool shared among more svc classes
  #23005: stage_rm return code
  #27311: please ensure MAXREPLICANB is respected
  #27587: nsls truncates user/group names
  #29078: cleanup query request
  #31772: CLI interface to DISABLE and un-DISABLE segments
  #31867: stager_qry to check the name server
  #33034: stager_qry loose track of renamed files in 2.1.6 (and earlier)
  #33220: RFE VDQM should be stateless and redundant
  #33866: putDone is allowed for (root,root)
  #33910: The name server should always record absolute filenames 
  #40215: Failed to open file: /etc/castor/policies/migration.py
  #40364: listSvcClass gives perl errors when database connection fails
  #40365: Stager tries to recall from disabled tape even if 2nd copy available
  #41513: repacking 2nd copy fails if 1st copy already staged
  #42577: Document scheduler customized pending reasons
  #42614: Updates not working correctly with internal replication enabled
  #42631: Corrupted output in nsls --comment
  #42801: Missing OwnerUID and ownerGID
  #43042: RFE : nsrmdir
  #43055: tplabel cannot label empty tapes
  #43056: Drive not put down on I/O errors during locate
  #43068: no need to use DUAL table in DB script creation
  #43217: duplicated filter condition for WHERE stmt in SELECTFILES2DELETE
  #43254: nsrename with no arguments seg faults
  #43456: stager_qry -sd bypasses CUPV checks
  #43521: cross stager file consistency (Updated)
  #43527: Missing table aliases invalidate optimizer hint in defaultMigrSelPolicy
  #43727: Missing bind variable in db job  HOUSEKEEPINGJOB
  #43902: Add support for OFFLINE libraries
  #43904: vmgrdeletelibrary does not work
  #43906: The VDQM fails with a schema mismatch error

  - Modified the logging of the garbage collector to include the name of the service class
    that a deleted file belongs to. If a file belongs to multiple service classes a comma
    separated list will be given.

  - Added a log message in rtcpclientd to show when rtcpcld disables a tape.

  - Improved the logging when files are replicated by the diskCopyTransfer mover to include
    the filepath of the source diskcopy being replicated. 
    E.g. lxc2disk11.cern.ch:/srv/castor/09/78/131078@lxcastorsrv101.156812

  - Fixed Oracle deadlocks between the prepareForMigration and disk2DiskCopyDone procedures
    and the new disk space accounting triggers. This problem was identified during large
    scale replication on close tests.

  - Fixed a bug which could cause a file being replicated to become corrupted mid transfer.
    Due to the way replications are handled, a bug was identified which could allow the
    source diskcopy involved in a replication to be updated while also being replicated to
    another machine. As the contents of the file are being modified at the same time as
    being read there was no guarantee that the contents of the duplicated file matched that
    of the original. As a consequence of this it was possible to have multiple copies of a 
    file online but with different contents.

    Now if an update starts for a file, all ongoing replications for that file will be
    cancelled.

  - Fixed a bug in the createRecallCandidate logic which did not set the owneruid and 
    ownergid of the diskcopy waiting to be recalled. As a consequence the space taken on disk
    from files which were recalled from tape were not attributed to any user in the 
    Accounting table. (#42801)

  - Fixed the ownership of the gcDaemon and repackserver log files. In the 2.1.8 series of
    CASTOR2 these two daemons were modified to run under the stage:st account. As a 
    consequence of this the log files which were previously written under the root:root 
    account could no longer be updated. Now on upgrade of the castor-gc-server and 
    castor-repack-server RPM's the permissions for their corresponding log files 
    will be changed and local logging of messages from these daemons will resume.

  - Dropped the replicationPolicy attribute from the service class. The reason for this was
    because the logic for replication on close, which is pure PL/SQL cannot call the expert
    daemon in order to execute the policy.

  - Fixed the VDQM commands: vdqmsetpriority, vdqmlistrequest, vdqmdeletepriority,
    vdqmlistpirority and vdqmDBInit which could not connect to the VDQM database due to a
    schema and client version mismatch.

  - Fixed the vmgrlistdenmap command which segfaults when display capacities greater than
    999.99Gi.

  - Fixed a bug which could prevent the migration of a file when two copies of the file
    exist in a DxT0 and DxT1 service class and the DxT1 file is stager_rm'd. Deleting the
    DxT1 copy would leave the DxT0 as the only available file that could be migrated. As
    DxT0 service classes have no tape backend the file will never go to tape. Now a stager_rm
    is denied on DxT1 service class if a replica of the file is not entitled for migration.

  - Added a new command, 'nsrmdir' to the castor-ns-client package. This command is aimed at
    allowing users to remove directories safely if they are empty. (#43042)

  - Added explicit RPM dependencies to vdt_globus_essentials on the following packages:
      castor-rh-server
      castor-rfio-server
      castor-ns-server

  - Improved the testing of update cases in the test suite.

  - Fixed a bug in the repackserver which prevented a 2.1.8 version of the server from being
    run against a 2.1.8 schema.

  - The logic for restarting stuck tape recalls which used to be run every 12 hours is now
    run on a hourly basis.

  - Fixed a bug in the stager which prevented internal replication from working correctly.
    Although users were still able to access their files no additional copies of the files
    could be created, resulting in the following error being seen in the stager log file:

     "Error caught in createDiskCopyReplicaRequest. ORA-06550: line 1, column 7: PLS-00306:
      wrong number or types of arguments in call to 'CREATEDISKCOPYREPLICAREQUEST"

  - Modified the rfmkdir command to ignore errors if the target directory already exists and
    the -p option is being used to create non-existent parent directories first. This now
    makes the return code and error reporting of this command consistent with `mkdir -p` and
    `nsmkdir -p`.

  - Added a new command, 'nssetsegment' to the castor-ns-client package. This command is
    aimed at allowing ADMIN's to alter the checksum and status of segments on tape (#31772).
    Note: it is no longer possible to use the nssetchecksum command to alter the checksum of
    segment. The nssetchecksum now only targets regular files.

  - Added support for end-to-end file check summing. End users now have the ability to preset
    a checksum for a file in the nameserver prior to writing the data to disk. Once the data
    is written to disk using the RFIO protocol the checksum generated during the transfer
    will be crosschecked against the predefined value and an error returned to the client if
    a mismatch is detected. Furthermore, the checksum of the file on disk will also be
    checked at migration time to make sure that the data recorded on disk is consistent with
    the data that was written to tape. (#6019)

    Example:
      [lxcastordev10] nstouch /castor/cern.ch/dev/w/waldron/checksumtest

      [lxcastordev10] nssetchecksum -n adler32 -k 0xf /castor/cern.ch/dev/w/waldron/checksumtest
      [lxcastordev10] rfcp /etc/group /castor/cern.ch/dev/w/waldron/checksumtest

      [lxcastordev10] ~ > rfcp /etc/group /castor/cern.ch/dev/w/waldron/checksumtest
      close target : Bad checksum (error 1037 on lxc2disk21.cern.ch)

  - Modified VMGR to support OFFLINE libraries. Each tape library can now have the status
    ONLINE or OFFLINE and VMGR will only select tapes from ONLINE libraries when asked for
    a tape for migration. The commands vmgrenterlibrary and vmgrmodifylibrary have been
    modified accordingly to offer ADMIN's the ability to change the status. (#43902)
  
  - Fixed a bug in the vmgrdeletelibrary command which would incorrectly report an error to
    the client even though it succeeded in deleting the target library. (#43904)

  - The VDQM server now supports the running of multiple concurrent servers.

  - Fixed a bug in vmgrlistpool which incorrectly displays the capacity of a tape when using
    the -s option.

  - Fixed a bug in the vmgrmodifytape command which when called with no arguments returned:
    'vmgrmodifytape (null): Bad address'. Now the command returns an error indicating that
    the mandatory VID option is missing.

  - Fixed stager_qry by file id.

  - Added long options, including --help to all name server commands.


  Package changes
  ---------------
  - Dropped the dlfserver manpage from the castor-devel package.

  - Added the new nsrmdir and nssetsegment commands to the castor-ns-client package.

  - Added the schmod_castor manpage to the castor-lsf-plugin package.

  - Changed the mode bits on the /usr/bin/rmGetNodes command to be executable by all users.

  - The installation of the:
      castor-stager-client package now obsoletes castor-commands
      castor-vdqm2-client package now obsoletes castor-vdqm-client
      castor-vdqm2-server package now obsoletes castor-vdqm-server
    this was necessary to fix software upgrade conflicts when using YUM.

  - Dropped the tapetohsm command from the castor-hsmtools package.


  SQL script changes
  ------------------
  - Note: From this release onwards database upgrade and creation scripts will only be 
          distributed using the sqlplus syntax and with the standard .sql file extension.


  Upgrading from 2.1.8-2
  ----------------------

    Central services (CNS, CUPV, VMGR)
    ----------------------------------

    Due to changes in the Name Server API it is mandatory to upgrade the Name Server to
    2.1.8-3 prior to upgrading any stager instances. It is not however mandatory to upgrade
    the following packages:
 
      - castor-upv-server
      - castor-vmgr-server

    to insure the correct running and operation of 2.1.8-3 stager instances of CASTOR2.    
    However, if you wish to benefit from enhancements in VMGR to be able to change the status
    of tape libraries then an upgrade of the castor-vmgr-server package will be necessary.

    Notes:
      - This upgrade does not require stopping of the central services and should be
        transparent to the end users.
      - The 2.1.8-3 version of the Name Server supports backwards compatibility with 2.1.[7|8]
        clients and therefore it is not necessary to upgrade the Name Server, Clients and
        Stagers at the same time.
 
      Instructions
      ------------

      1. Upgrade the CNS database using the correct upgrade script from
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-3/dbupgrades

      2. Update the appropriate software to use the 2.1.8-3 RPMS. Note: the affected services
         will be restarted automatically.

      3. Upgrade complete.


    VDQM
    ----

    The following twiki page gives full instructions on how to install version 2.1.8-3 of
    the VDQM for all sites. Please note that external sites are recommended to check with
    CERN before installing version 2.1.8-3 of the VDQM in to production:

      - http://twiki.cern.ch/twiki/bin/view/FIOgroup/InstallVDQM218

    To upgrade from 2.1.7-20 to 2.1.8-3 please refer to:

      - http://twiki.cern.ch/twiki/bin/view/FIOgroup/VDQM2Upgrade21720to2183


    Repack
    ------

    To upgrade the repack server please follow the instructions below:

      Instructions
      ------------

      1. Upgrade the Repack database using the correct upgrade script from
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-3/dbupgrades

      2. Upgrade the software to use the 2.1.8-3 RPMS. Note: the repackserver will be 
         restarted automatically.

      3. Upgrade complete.


    Stager
    ------

    Before upgrading to a 2.1.8-3 instance of CASTOR2 please make sure to read the upgrade
    instructions fully before proceeding. It is highly recommended to have an Oracle DBA on
    standby while upgrading the stager database just in case a database restoration is
    required. 

    Notes: 
      - It is highly recommended to test the upgrade of the database on an exported copy
        prior to performing the upgrade live (maybe several days before). There are many 
        constraints added in this release and as a consequence it may not be possible to
        apply the upgrade script if the database has lots of orphaned entries. If you
        encounter any error upgrading please contact the CASTOR development team.

      - This upgrade is intrusive and will require the stager to be stopped!  

      Instructions
      ------------

       1. Stop all daemons on stager headnodes which have direct connections to the stager
          databases. This includes: rhserver, stager, jobManager, rtcpclientd, mighunter
          rechandler and the rmMasterDaemon. Note: it is not necessary to stop any 
          services/daemons on the diskservers themselves.
 
       2. Remove any cron scripts used to restart stuck tape recalls. This logic is now part
          of database cleaning jobs which run ever hour.

       3. Verify that all sessions from CASTOR2 related daemons to the stager database are
          terminated. This step is advised to make sure that modifications are no longer
          being made to the database which may interfere or prolong the upgrade process.

       4. Ask your DBA to create a backup of the database.

       5. Upgrade the stager and DLF databases using the correct upgrade scripts from
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-3/dbupgrades

            Note: At the end of the upgrade process the upgrade script will recompile all the
                  invalid objects in the database schema. This may fail if you have objects
                  which do not belong to the castor schema or left over objects from past
                  workarounds. It does not mean that the upgrade failed but you should 
                  cleanup the database schema. If in doubt please ask the CASTOR development
                  team.

                  If a recompilation error occurs an ORA-24344: success with compilation 
                  error will be observed.

       6. Upgrade the software on the headnodes and diskservers to 2.1.8-3. All daemons will
          be automatically restarted.

       7. Restart the service

       8. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-3/fastTestSuite

       9. Start the public rhserver (if applicable)

      10. Congratulations you have successfully upgraded to the 2.1.8-3 release of CASTOR2!!


-----------
- 2.1.8-2 -
-----------

  Highlights and BugFixes
  -----------------------
  - #29840: replication after put/recall
  - #38590: Two tape drive deletion bugs in the VDQM2
  - #35388: error messages in DLF (updated)
  - #39552: Documentation errors
  - #40578: Formatting of data volumes in repack and others should use uppercase K
  - #42055: Should be able to delete a drive in the UNKNOWN state with no pending request
  - #42575: disk pool 'has no tape backend' decision

  - Added support for replication on close. Now it is possible to have CASTOR automatically
    trigger the replication of a file after it has been closed by the user. The number of
    replicas created is dependent on the value of the maxReplicaNb defined on the service
    class. You can enable replication on close using the following command against a 
    targeted service class:
     `modifySvcClass --Name <svcclass> --ReplicateOnClose yes --MaxReplicaNb X`

    Notes:
     - Make sure that the maxReplicaNb is not equal to 1. This defies the purpose of 
       replication on close if you only permit one copy of the file to be present on disk!
     - The rebalancing of the number of copies of a file when a diskserver goes offline is
       not supported.
     - When allocating resources for ReplicateOnClose based service classes do not assume
       that the only requirement is disk capacity. It is a key factor but network throughput
       and the number of job slots needed are also critical. Remember disk2disk copy 
       transfers require two slots. So if you want to allow 100 concurrent transfers on a
       D2TX service class you need a minimum of 300 slots.

       Please make sure to monitor services classes that have replication on close enabled
       to avoid an accumulation due to lack of resources.
       
  - The gcDaemon and repackserver now run under the stage:st account not root:root.

  - Improved logging of the garbage collection process. Now when files are deleted the
    LastAccessTime, NbAccesses, GcWeight and GcType will be logged for every file removed
    by the garbage collector. The GcType can have two values:
      Automatic:      The file was deleted in order to liberate space on the filesystem.
      User Requested: The file deletion was triggered by the user with an explicit stager_rm
                      call or the transfer failed due to some user error and the file is
                      invalid.

  - Removed regular expression support from the stager's query service. It is now no longer
    possible to perform stager_qry commands with the -E option. Note: since 2.1.7-7 this
    functionality was disabled by default with an option provided in castor.conf to enable
    it. In this release the support for this functionality has been removed completely.

  - Fixed a deadlock caused by a race condition in the request replier of the stager that
    could cause the stager to become unresponsive.

  - Modified the contents of the /etc/castor/status file and changed the labeling of
    filesystems. Prior to this modification filesystems were labeled as FS1=, FS2=, FS3=,
    etc... Now there are labeled using the name of the mountpoint as defined by the
    RmNode/MountPoints option in castor.conf. E.g. /src/castor/01, /srv/castor02/, etc...

  - Added checks to stagerJob to prevent a file being transferred to an incorrect service
    class due to a misconfiguration between LSF and the stager database.

  - Added support for defining the maximum amount of data that can be read by a client or
    server. The default value is 20MB and can be altered by changing the
    CLIENT/MAX_NETDATA_SIZE option in castor.conf. For more information please refer to
    savannah bug #38984

  - Added support for the automatic determination of the LSF master when downloading a jobs
    notification file using the http:// access method. Now the JobManager/SharedLSFResource
    configuration option can be specified for example as http://LSB_MASTERNAME/lsf. When the
    job starts it will automatically substitute the LSB_MASTERNAME variable with the name of
    the current LSF master. This allows for jobs which do not use a shared filesystem to fail
    over to another webserver when LSF goes down in a redundant setup (i.e. multiple LSF 
    masters)

  - Added basic user space accounting. It is now possible to see in the Accounting table of 
    the stager database the amount of data that every user has on disk on a per service class
    basis. Note: In this release no tools are provided for end-users to see this information.

  - New implementation of stagerJob with improved logging and error reporting (#35388).

  - Modified stagerJob to support a new xroot interface. Please note: this is a backwards
    incompatible change. When upgrading to 2.1.8 old installations of xrootd must also be
    upgraded. For installation instructions of xrootd please refer to:
      https://twiki.cern.ch/twiki/bin/view/DataManagement/X2CASTOR#Deployment_Configuration_Model

    The interface was modified to support T3 analysis use-cases.

  - Consolidated the service class parameters used to define the retention policy and access
    latency of the underlying disk pools. The flag hasDiskOnlyBehavior has been renamed to
    failJobsWhenNoSpace, as this is reflects its true meaning; a flag disk1Behavior 
    supersedes gcEnabled; and forcedFileClass defines whether files go to tape. All those 
    parameters enable full support of any DiskN TapeM storage class combinations. See also the
    enterSvcClass man page for further details.

  - Enabled checksuming support in RFIOD by default. Now all files written to disk using 
    RFIO as a protocol will have the checksum of the file recorded in the files extended
    attributes.

  - Added role based initd scripts for the rhserver and nsdaemon. This now provides support
    for controlling multiple rhservers and nsdaemons from their respective initd scripts. 
    (Refer to point 8 of the upgrade instructions)

  - Improved the initd script for the mighunter daemon. Now you can stop the mighunters on a
    per service class basis as well as querying their status. Prior to this it was not 
    possible to stop a service class based mighunter without stopping all of them.

  - Official release of VDQM2 providing a stateless backend and support for tape priorities.


  Repack Related:
  ---------------
  - #38419: Post FINISHED repack actions such as reclaim or move pool
  - #23711: Maxdrives - input and output
  - #38418: Limit parallel ONGOING repacks for bulk repack
  - #40680: repack of a non-full tape gives 'No Tape Given'
  - #40489: repack command exits with 0 if running as root and errors to stdout


  Package changes
  ---------------
  - With the discontinuation of VDQM1 the castor-vdqm-client and castor-vdqm-server packages
    are no longer provided.
  - The commands that use to be found in the castor-vdqm-client package have been moved to a
    new package called castor-vdqm2-client.
  - The vdqm_admin command has been moved from castor-hsmtools to the castor-vdqm2-client
    package.


  Upgrading from 2.1.7-19
  -----------------------

    Central services (CNS, CUPV and VMGR)
    -------------------------------------

    Although there are modifications to the Castor Central Services in this release. It is
    not mandatory to upgrade the software for the correct operation and running of a 2.1.8
    stager instance of CASTOR2. As a result the upgrade of the following packages:

      - castor-upv-server
      - castor-vmgr-server
      - castor-ns-server

    are not prerequisites for deploying a 2.1.8 stager.

    Note: If you do wish to upgrade the central services described please follow the
    instructions below. Note: this upgrade does not require stopping of the central services
    and should be transparent to the end users

      Instructions
      ------------

      1. Upgrade the CNS database using the correct upgrade script from
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-2/dbupgrades

      2. Install the vdt_globus_essentials RPM on the Castor Name Server headnodes. This is
         necessary as the name server now has a dependency on globus in order to provide 
         support for strong authentication through GSI and KRB5. The recommended version for 
         vdt_globus_essentials is VDT1.6.1{x86|x86_64}_rhas_4-7 for SLC4.
 
      3. Update the software to use the 2.1.8-2 RPMS. Note: the affected services will be
         restarted automatically.

      4. Upgrade complete.


    Stager
    ------

    Before upgrading to a 2.1.8 stager instance of CASTOR2 please make sure to read the 
    upgrade instructions fully before proceeding. It is highly recommended to have an 
    Oracle DBA on standby while upgrading the stager database just in case a database
    restoration is required.
    
    Notes: 
      - To upgrade to this version of CASTOR2 all jobs PEND'ing in the LSF queue will need
        to be terminated. In order to reduce the visible impact of the upgrade to the users
        in terms of failed transfers it is recommended to suspend any new user activity 
        from entering the instance several hours before the upgrade.

      - This upgrade is intrusive and will require the stager to be stopped!  

      Instructions
      ------------

       1. Stop all daemons on stager headnodes which have direct connections to the stager
          databases. This includes: rhserver, stager, jobManager, rtcpclientd, mighunter
          rechandler and the rmMasterDaemon. Note: it is not necessary to stop any 
          services/daemons on the diskservers themselves.

       2. Verify that all sessions from CASTOR2 related daemons to the stager database are
          terminated. This step is advised to make sure that modifications are no longer
          being made to  the database which may interfere or prolong the upgrade process.

       3. Ask your DBA to create a backup of the database.

       4. Upgrade the stager and DLF databases using the correct upgrade scripts from
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-2/dbupgrades

          Note: At the end of the upgrade process the upgrade script will recompile all the
                invalid objects in the database schema. This may fail if you have objects
                which do not belong to the castor schema or left over objects from past
                workarounds. It does not mean that the upgrade failed but you should cleanup
                the database schema. If in doubt please ask the CASTOR development team.

                If a recompilation error occurs an ORA-24344: success with compilation error
                will be observed.

       5. Install the vdt_globus_essentials RPM on all machines which run the rhserver and
          rfiod daemons. This is necessary as these two daemons now have a dependency on 
          globus in order to provide support for strong authentication through GSI and KRB5.
          The recommended version for vdt_globus_essentials is VDT1.6.1{x86|x86_64}_rhas_4-7
          for SLC4.

       6. Upgrade the software on the headnodes and diskservers to 2.1.8-2. Note: the
          castor-vdqm-client package has been replaced by castor-vdqm2-client.

       7. Update the castor.conf configuration file on all machines based on the
          castor.conf.example file in /etc/castor/. Please note that the options which are
          commented out are the default values for those options unless explicitly stated
          otherwise.

          Of particular note: the job/LOGSTANDARD option has been replaced by 
          Job/LOGSTANDARD. A change in the capitalization was necessary to distinguish the
          difference between the old and new implementations of stagerJob.

       8. The rhserver now uses a role based initd script (like the mighunter) which allows
          for multiple rhservers to be controlled via a single initd script. If you are
          running multiple rhservers on the same machine or using the Black and White list
          support you will need to modify the rhserver sysconfig file(s) accordingly.

          Example 1: Running a private and public rhserver on the same machine
          ---------
            - Create a rhserver sysconfig file:
                `cp /etc/sysconfig/rhserver.example /etc/sysconfig/rhserver`
            - Edit the new file and set the ROLES option to:
                 ROLES="public private"
            - Create an additional sysconfig file for the private rhserver:
                 `cp /etc/sysconfig/rhserver.example /etc/sysconfig/rhserver.private`
            - Edit the rhserver.public file and set the RHSERVER_OPTIONS value to reflect the
              port you want the private rhserver to run on. E.g:
                 RHSERVER_OPTIONS="-p 5002"

          Once completed you now have the ability to start, stop and query the status of the
          individual rhserver roles. The syntax for the initd script is:

          Usage: /etc/init.d/rhserver {start|stop|status|restart|condrestart} [role]

          If no role is specified then all listed roles will be targeted  E.g.

          [root@c2itdcsrv102 ~]# service rhserver status
          rhserver for role: public is stopped
          rhserver for role: private is stopped

          [root@c2itdcsrv102 ~]# service rhserver start public
          Starting rhserver:
          Starting rhserver for role: public                         [  OK  ]

          [root@c2itdcsrv102 ~]# service rhserver status
          rhserver for role: public (pid 12115) is running...
          rhserver for role: private is stopped


          Example 2: Enabling Black and White list support.
          ---------
            - Create a rhserver sysconfig file:
                `cp /etc/sysconfig/rhserver.example /etc/sysconfig/rhserver`
            - Edit the new file and set the RHSERVER_OPTIONS value to enable Black and White
              list support:
                RHSERVER_OPTIONS="-b"

          Note: The ONLY way to turn on Black and White list support is on the command line
          to the rhserver. The RH/USEACCESSLISTS option in castor.conf is no longer
          respected.

       8. If you have an independent monitoring system to monitor the CASTOR2 related daemons
          such as LEMON. Please make sure to update the monitoring configuration to reflect
          the fact the the gcDaemon no longer runs as root:root but as stage:st.

       9. Verify that all services classes have the correct attributes enabled. In particular,
          please pay close attention to the failJobsWhenNoSpace and disk1Behavior flags.

      10. Restart the service

          Notes:
            - If you have a concept of private and public rhserver's you should start the
              private one only so that you can test/validate the installation without user 
              interference.
            - Unlike previous upgrades it is no longer necessary to cleanup the stager
              database when restarting rtcpclientd. This is now done automatically when the
              daemon starts.

      11. Test the instance by running the test suite available from:
          - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-2/fastTestSuite

      12. Start the public rhserver (if applicable)

      13. Congratulations you have successfully upgraded to the 2.1.8 release of CASTOR2!!


    Repack
    ------

    1. Stop the repackserver

    2. Upgrade the repack database using the correct upgrade scripts from
       - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.8-*/2.1.8-2/dbupgrades

    3. If you have an independent monitoring system to monitor the CASTOR2 related daemons
       such as LEMON. Please make sure to update the monitoring configuration to reflect
       the fact the the repackserver no longer runs as root:root but as stage:st.
 
    4. Grant the stage:st user TP_OPER and ADMIN rights in CUPV from the repackserver 
       machine to the central name servers. These privileges are need so that repack can
       issue the vmgrmodifytape and reclaim commands.

    5. Start the repackserver


  Installation of VDQM
  --------------------

  For installation instructions on VDQM2 please refer to:
    http://twiki.cern.ch/twiki/bin/view/FIOgroup/InstallVDQM218

  Note: The installation instructions provided will not be updated for the 2.1.8 version of
        VDQM2 until that version is installed at CERN. External sites are advised not to 
        install the 2.1.8 version of the VDQM2 until CERN has successfully used it in 
        production for at least several days.


------------
- 2.1.7-19 -
------------

  Bug Fixes
  ---------
  - #36804: Inconsistency in permission handling between stager_rm and nsrm (Updated)
  - #38611: modifySvcClass vs printSvcClass: setting and displaying parameters
  - #39604: updates on diskonly pools may create CANBIMIG files
  - #39795: stager_addprivilege: Invalid argument
  - #41500: stager_removeprivilege returns ORA-01722: invalid number
  - #41943: STAGED diskcopy with castorfile=0
  - #42034: A recent zero-size file case created by 2.1.7-17 stager (Updated)

  - Fixed a bug in enterSvcClass which did not respect the GcEnabled option supplied on the
    command line. (#38611)

  - Fixed a bug in the logic for adding and removing entries from the Black and White list
    which did not check to see if the service class being targeted existed. Now if the user
    specifies an non-existent service class using the stager_[add|remove]privilege commands
    they will get the following error: `stage_addPrivilege: Invalid service class 'xxxx'`

  - Fix to the castorBW PL/SQL package to allow users to issue the stager_removeprilege 
    command to manage the Black and White list. Prior to this fix it was not possible to 
    remove privileges. This was due to an error in the upgrade script when upgrading from 
    2.1.6 to 2.1.7. (#41500)

  - Fixed a bug in the stager client API which caused a segmentation fault because of an
    attempt to free memory twice (*** glibc detected *** double free or corruption). This
    occurred when STAGE_HOST was defined but set to an empty value.

  - Added missing checks for memory allocation failures in the automatically generated 
    database access code.

  - Added a cancelRecallForTape procedure to the stager database which accepts a tape vid
    as input and cancels all files waiting to be recalled from that tape. Note: there is no
    external interface to this procedure, it must be called directly from a SQL database
    session.

  - Fixed a bug which allowed files to be left in a CANBEMIGR state indefinitely after being
    updated in a disk only pool. Now if a user attempts to update a file which has a copy on
    tape in a disk only pool they will get EBUSY with rfcp and the following error with a 
    PrepareToPut: `EINVAL/"File recreation canceled since this service class doesn't provide
    tape backend"`. (#39604)

  - Added a new implementation of the selectFiles2Delete procedure pack ported from 2.1.8 to
    optimise the selection of files to be deleted when there are many files to be garbage 
    collected. Prior to this optimisation the stager DB would consume far more resources then
    necessary and cause the general castor instance to become degraded. Furthermore, the 
    garbage collection process stops working completely as the stager cannot answer clients
    (GC daemons) fast enough before they timeout waiting for a response.

  - Fixed handling of symbolic links in the white list of RFIOD.

  - Added support for disabling tape recalls in the Black and White list. During the upgrade
    process for this release all users will be granted the ability to recall from tape. So,
    from the perspective of the user nothing will have changed after the upgrade. In order to
    manipulate who can recall from tape. one must:

     A) Remove the line granting all users access:

        stager_removeprivilege -R TapeRecall -U ':' -S '*'

     B) Add the grants for who can recall from tape:

        stager_addprivilege -R TapeRecall -U ':c3' -S 'default'
        stager_addprivilege -R TapeRecall -U ':st' -S 'diskonly'
        ...
        ...

    Note: Clients must be running 2.1.7-19 if they wish to manipulate the TapeRecall request
          type through the stager_[add|remove]privilege commands.

    Clients who are prevented from recalling from tape will get the following error message:
      SUBREQUEST_FAILED 13 Insufficient user privileges to trigger a tape recall to the
      'xxxxx' service class

  - Fixed segmentation fault in the jobManager.

  - Added support for defining the maximum runtime (wallclock) of a disk2disk copy transfer.
    This helps to protect against cases where slots used by a disk2disk copy transfer are
    not released after a diskserver crashes mid transfer. The default value is 5 hours, after
    which time the transfer will be killed by LSF.

  - Added a startup hook into rtcpclientd which resets all the streams, tapecopies and tape
    statuses. As a result of this fix it is no longer necessary to cleanup the stager database
    when restarting rtcpclientd. Furthermore, rtcpclientd is now automatically restarted
    upon RPM updates.

  - Fixed bad check allowing migration of the two copies of a file to the same tape.

  - Backports to allow compilation of the CASTOR2 software on SLC5. Note: although SLC5 RPMS
    can be built they are only provided on demand, are not certified nor do they come with 
    any support.

  - Added support for disabling the GcDaemon's synchronization functionality by setting the
    GC/SyncInterval configuration option in castor.conf to 0.

  - Fixed incorrect logging of the NSFILEID in the "File to be unlinked since it dissapeared 
    from the stager" messaged logged by the stager when a file is on disk but is not know to
    to the stager database. Prior to this fix the NSFILEID was actually the diskcopy id. Now
    the NSFILEID will be 0 and a separate parameter will be logged giving the diskcopy id.

  - Added support to repack database to cleanup ARCHIVED Repack SubRequests after a 
    configurable amount of time.

  Tape
  ----
  - Added support to abort when positioning took suspiciously long. The timeout value is 
    controlled by the new parameter POSITION_TIMEOUT. Default value is 900 seconds.
 
  - Added a new parameter DOWN_ON_UNLOAD_FAILURE to control the behaviour if an unload failure
    occurs. Default (and old behaviour): the drive is put down. Since SUN drives do not need
    the unload as the unmount will be retried with the force option, this option may help to
    avoid unnecessary down time of drives.
  
  Upgrading from 2.1.7-17
  -----------------------

  Before upgrading to a 2.1.7-17 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Instructions
  ------------
  1. Upgrade the stager and dlf databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-19/dbupgrades

  2. Upgrade the software on the headnodes and diskservers to 2.1.7-19. All daemons will be
     automatically restarted.

  3. Announce completion of the upgrade.


------------
- 2.1.7-17 -
------------

  Bug Fixes
  ---------
  - Fixed possible memory corruption when using setDataBufferArray.

  - Fixed migrations to several tapepools when not using migration policies. Only one pool
    was used leading to using only a fraction of the streams.

  - Added support to be able to use username and groupname as input on the listPriority,
    deletePriority and enterPriority commands

  Upgrading from 2.1.7-16
  -----------------------

  Before upgrading to a 2.1.7-16 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Instructions
  ------------
  1. Upgrade the stager, dlf and repack databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-17/dbupgrades

  2. Stop rtcpclientd.

  3. Upgrade the software on the headnodes and diskservers to 2.1.7-16. All daemons apart
     from rtcpclientd will be restarted automatically.

  4. Log onto the stager database and execute:

     TRUNCATE TABLE Stream2TapeCopy;
     UPDATE NbTapeCopiesInFS SET NbTapeCopies = 0;
     UPDATE TapeCopy SET status = 1 WHERE status IN (2, 3, 7);
     UPDATE Stream SET status = 0;
     UPDATE tape SET status = 1 WHERE status IN (2, 3, 4);
     UPDATE tape SET stream = 0 WHERE stream != 0;
     UPDATE Stream SET tape = 0 WHERE tape != 0;
     COMMIT;

  5. Start rtcpclientd.

  6. Announce completion of the upgrade.


------------
- 2.1.7-16 -
------------

  Bug Fixes
  ---------
  - #40147: Stager segfaults on non-existent service classes

  - Added support for the CASTOR_INSTANCE environment variable in the database connection 
    handling code of the C and C++ related applications/utilities. Now, just like listSvcClass,
    listFileClass, etc... it is possible to use commands from a single machine but connect to
    different CASTOR2 instances.

    For this to work correctly the /etc/castor/ORASTAGERCONFIG file must contain all the
    passwords for the instances you wish to connect too. E.g

      DbCnvSvc_castoratlas  user     castoruser
      DbCnvSvc_castoratlas  passwd   castorpass
      DbCnvSvc_castoratlas  dbName   castoratlas

      DbCnvSvc_castorcms  user	     castoruser
      DbCnvSvc_castorcms  passwd     castorpass
      DbCnvSvc_castorcms  dnName     castorcms

    By setting CASTOR_INSTANCE to castoratlas or castorcms you can switch between the stager
    instances.

  - Fixed the StagePrepareToGetRequest IOResponse for the xrootd protocol so that it now 
    correctly returns the physical location of a file. Previously the response was empty.

  - Fixed a bug in the scheduling logic of the schmod_python LSF plugin which did not take
    into consideration the scheduling coefficients as defined in castor.conf. As a
    consequence the selection of filesystems was biased and jobs were not spread evenly
    across diskservers.

  - Fixed a misconfiguration of the FIFO and LRU garbage collection policies which would
    call a nonexistent function to update the gc weight of a file after a recall. As a
    consequence this would cause the following error in the recaller log:

      DB_ERROR="Error caught in fileRecalled. ORA-06550: line 1, column 27: PLS-00302: 
      component 'CREATIONTIMERECALLSWEIGHT' must be declared ORA-06550: line 1, column 7: 
      PL/SQL: Statement ignored ORA-06512: at "CASTOR_STAGER.FILERECALLED", line 49  

    Which in turn would cause the recall to fail.


  Upgrading from 2.1.7-15
  -----------------------

  Before upgrading to a 2.1.7-16 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Instructions
  ------------
  1. Upgrade the stager, dlf and repack databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-16/dbupgrades

  2. Stop rtcpclientd.

  3. Upgrade the software on the headnodes and diskservers to 2.1.7-16. All daemons apart
     from rtcpclientd will be restarted automatically.

  4. Log onto the stager database and execute:

     TRUNCATE TABLE Stream2TapeCopy;
     UPDATE TapeCopy SET status = 1 WHERE status IN (2, 3, 7);
     UPDATE Stream SET status = 0;
     UPDATE tape SET status = 1 WHERE status IN (2, 3, 4);
     UPDATE tape SET stream = 0 WHERE stream != 0;
     UPDATE Stream SET tape = 0 WHERE tape != 0;
     COMMIT;

  5. Start rtcpclientd.

  6. Reload the LSF plugin. `badmin reconfig`

  7. Announce completion of the upgrade.


------------
- 2.1.7-15 -
------------

  Bug Fixes
  ---------

  - Added support to GridFTPv2 external for handling diskpools shared across multiple service
    classes. Prior to this release transfers would fail for diskpools which were shared
    across service classes under certain circumstances. For example, user A could issue a
    file transfer request from service class Z and when the transfer actually took place, be
    using service class Y. The reason for this is that the user is not mapped using their
    uid and gid to an appropriate service classes once on the diskserver. So, GridFTPv2 
    selects the default service class as defined in the castor.conf (STAGER/SVCCLASS) or if 
    not defined uses 'default'.

    Now like GridFTPv1, GridFTPv2 supports the stagemap.conf file concept.

  - Fixed an 'ORA-10632: Invalid rowid' error, in the subRequestToDo and subRequestFailedToDo
    procedures of the Stager database. This error, under certain conditions was typically
    seen when the table shrink job was running in the database.


  Upgrading from 2.1.7-14
  -----------------------

  Before upgrading to a 2.1.7-15 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Installing GridFTPv2 External
  -----------------------------

  1. On all diskservers that you wish to be GridFTP enabled you will need to install the 
     following packages. Note: Some of these may already be installed by the previous
     GridFTPv1 installation:

     For i386:
       vdt_globus_essentials-VDT1.6.1x86_rhas_4-5
       vdt_globus_data_server-VDT1.6.1x86_rhas_4-5
     For x86_64:
       vdt_globus_essentials-VDT1.6.1x86_64_rhas_4-5
       vdt_globus_data_server-VDT1.6.1x86_64_rhas_4-5

     In all cases:
       castor-gridftp-dsi-ext-2.1.7-15
       castor-gridftp-dsi-common-2.1.7-15
  
  2. Copy the host certificate of the machine to the correct place. As user root on all
     diskservers:

     mkdir /etc/grid-security/castor-gridftp-dsi-ext
     cd /etc/grid-security/
     cp hostcert.pem castor-gridftp-dsi-ext/castor-gridftp-dsi-ext-cert.pem
     cp hostkey.pem castor-gridftp-dsi-ext/castor-gridftp-dsi-ext-key.pem
     chmod 644 castor-gridftp-dsi-ext/castor-gridftp-dsi-ext-cert.pem
     chmod 400 castor-gridftp-dsi-ext/castor-gridftp-dsi-ext-key.pem

  3. For diskpools which are mapped directly to a service class 1-to-1 and are NOT shared
     across service classes there is no need to have a stagemap.conf file on all the
     diskservers and it is advised to simply set the STAGER/SVCCLASS option in castor.conf
     to the appropriate value.

     For diskpools which are shared across service classes a stagemap.conf will be
     necessary to resolve the uid and gid of the user to a service class. This is the same
     as used for GridFTPv1.

  4. Restart xinetd.


  Instructions
  ------------
  1. Upgrade the stager, dlf and repack databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-15/dbupgrades

  2. Stop rtcpclientd.

  3. Upgrade the software on the headnodes and diskservers to 2.1.7-15. All daemons apart
     from rtcpclientd will be restarted automatically.

  4. Log onto the stager database and execute:

     TRUNCATE TABLE Stream2TapeCopy;
     UPDATE TapeCopy SET status = 1 WHERE status IN (2, 3, 7);
     UPDATE Stream SET status = 0;
     UPDATE tape SET status = 1 WHERE status IN (2, 3, 4);
     UPDATE tape SET stream = 0 WHERE stream != 0;
     UPDATE Stream SET tape = 0 WHERE tape != 0;
     COMMIT;

  5. Start rtcpclientd.

  6. Announce completion of the upgrade.


------------
- 2.1.7-14 -
------------

  Bug Fixes
  ---------

  - Fixed a 32 bit overflow problem when using id2type values which exceeded the value of 
    INT_MAX (2147483647). When this number is exceeded migrations and recalls to tapes,
    where the id of the tape is greater than INT_MAX will fail and the following error 
    messages are observed:

    In /var/spool/rtcpclientd/rtcpcld:
      DB_ERROR="Unable to select tape by vid, side and tpmode : ORA-01455: converting column 
                overflows integer datatype"   

    In /var/spool/stager/log:
      Error_Message="Unable to select tape by vid, side and tpmode : ORA-01455: converting 
                     column overflows integer datatype"

  - Fixed a security problem in rfiod which allowed root users (uid=0) to write files to 
    a remote host as root using the update v2 (non-streaming) protocol.

  - Fixed a bug in the startChosenStreams procedure which incorrectly contained the wrong
    content.

  - Fixed a bug in the cleaning logic which automatically calls PutDone for files which
    remain in a STAGEOUT/WAITFS state after X number of hours (default: 72). Under
    curtain circumstances the previous code was triggering the file to be migrated to tape
    and creating new tapecopies when actually it should have been invalidating the 
    diskcopy for garbage collection.

  Upgrading from 2.1.7-12
  -----------------------

  Before upgrading to a 2.1.7-14 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Instructions
  ------------
  1. Upgrade the stager, dlf and repack databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-14/dbupgrades

  2. Stop rtcpclientd.

  3. Upgrade the software on the headnodes and diskservers to 2.1.7-14. All daemons apart
     from rtcpclientd will be restarted automatically.

  4. Log onto the stager database and execute:

     TRUNCATE TABLE Stream2TapeCopy;
     UPDATE TapeCopy SET status = 1 WHERE status IN (2, 3, 7);
     UPDATE Stream SET status = 0;
     UPDATE tape SET status = 1 WHERE status IN (2, 3, 4);
     UPDATE tape SET stream = 0 WHERE stream != 0;
     UPDATE Stream SET tape = 0 WHERE tape != 0;
     COMMIT;

  5. Start rtcpclientd.

  6. Announce completion of the upgrade.


------------
- 2.1.7-12 -
------------

  Bug Fixes
  ---------
  - #35428: GC stager SYNC causes high DB load on 2.1.7
  - #38611: modifySvcClass vs printSvcClass: setting and displaying parameters
  - #38825: CUPV does not support multiple domains
  - #38858: Default STAGE_HOST for 2.1.7 old clients is no longer set for CASTOR1
  - #38832: GCweight values loosing precision during conversions between C++ and ORACLE
  - #38984: castor client doesn't allow repack to process tapes with more than ~74 000 file

  - Fixed a segmentation fault in rtcpclientd caused by attempting to log a NULL tape 
    VID using the DLF API.

  - Fixed the init.d scripts for the stager and rhserver. In the case of the stager, its
    sysconfig file (/etc/sysconfig/stager) was not sourced correctly and for the rhserver,
    the RHSERVER_OPTIONS value in the sysconfig file was not passed through to the rhserver
    binary.

  - Integrated modifications from RAL into the test suite so that it is less CERN specific.

  - Fixed support for the EXCLUDE_HOSTS option on LSF jobs which was ignored for disk2disk
    copy replication requests, the only request for which the option is actually needed.

  - Added support for bulk selection to reduce the stress on the rhserver and stager for
    large requests. #35428

  - Fixed a memory leak in the jobManager

  Upgrading from 2.1.7-10
  -----------------------

  Before upgrading to a 2.1.7-12 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Warning
  -------
  Do not upgrade CUPV to run 2.1.7-12 unless your CUPV entries are fully qualified!

  Instructions
  ------------
  1. Upgrade the stager, dlf and repack databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-12/dbupgrades

  2. Stop rtcpclientd.

  3. Upgrade the software on the headnodes and diskservers to 2.1.7-12. All daemons apart
     from rtcpclientd will be restarted automatically.

  4. Log onto the stager database and execute:

     TRUNCATE TABLE Stream2TapeCopy;
     UPDATE TapeCopy SET status = 1 WHERE status IN (2, 3, 7);
     UPDATE Stream SET status = 0;
     UPDATE tape SET status = 1 WHERE status IN (2, 3, 4);
     UPDATE tape SET stream = 0 WHERE stream != 0;
     UPDATE Stream SET tape = 0 WHERE tape != 0;
     COMMIT;

  5. Start rtcpclientd.

  6. Reload the LSF plugin. `badmin reconfig`

  7. Upgrade the CUPV and CNS to 2.1.7-12.

  8. Now that CUPV supports multiple network domains (Bug: #38825) it is necessary to update 
     the contents of the CUPV database so that all source and target entries are fully 
     qualified. If you know that your database already contains fully qualified domain names
     then you can skip this step. Otherwise,

     If you are operating in a single domain network, then execute the following 2 SQL 
     statements on the CUPV database:

        UPDATE user_privilege 
           SET src_host = substr(src_host, 0, length(src_host) - 1) || '.DOMAIN.COM$'
         WHERE src_host NOT LIKE '%.DOMAIN.COM$';
 
        UPDATE user_privilege 
           SET tgt_host = substr(tgt_host, 0, length(tgt_host) - 1) || '.DOMAIN.COM$'
         WHERE tgt_host NOT LIKE '%.DOMAIN.COM$';

        COMMIT;
 
        Where DOMAIN.COM represents the name of the network domain. Note: the domain name
        is case sensitive.

     For network's with multiple domains you will need to modify the entries by hand. The
     recommended procedure for minimising downtime for this case is detailed below:

     - Connect to the CUPV database and create a copy of the user_privilege table:
       
         CREATE TABLE user_privilege_copy AS SELECT * FROM user_privilege WHERE '1' = '1';

     - Instruct users which have UPV_ADMIN rights that making changes to CUPV while the
       migration is taking place will be lost.

     - Modify the entries in the user_privilege_copy table so that both the source and target
       entries are fully qualified.

     - Stop CUPV. Due to the nature of the CUPV API clients will just wait until the service
       returns.

     - Backup the production table
    
         ALTER TABLE user_privilege RENAME TO user_privilege_bak;

     - Rename the copy as the production table

         ALTER TABLE user_privilege_copy RENAME TO user_privilege;

     - Start the 2.1.7-12 CUPV


------------
- 2.1.7-10 -
------------

  Bug Fixes
  ---------
  - #18778: -f option to stager_get to pass a file containing the list of files to prestage
  - #32902: RFE: schedule separately for rfio and gridftp jobs
  - #33207: Need admin tools to manage black/white lists for stager ACLs
  - #34639: consider DISABLED tapes as not available
  - #34766: stager error messages corrupted?
  - #34800: stager: request processed missing fileid?
  - #35084: concurrent repack of two tapecopies of the same file
  - #35085: minor repack enhancement
  - #35086: multiple vid given by command line
  - #36804: Inconsistency in permission handling between stager_rm and nsrm (Updated)
  - #35870: Temporarily move vdqm_admin to another rpm
  - #36713: RFE: garbage collection
  - #37286: Default logrotate should be 90 days
  - #37374: stager: setGC does not log the pin value
  - #37691: RFIO preseek/preread fails on repeated blocks.
  - #37823: stage_filequery returns empty file name
  - #37893: test suite not up-to-date after 'rfio filter' in 2.1.7-8
  - #37936: Write manual page for the VDQM2 server: vdqmserver
  - #37968: Order VDQM2 showqueues requests by DGN as well as by time
  - #38027: b/w check of non-existent svcclass returns 'Insufficient user privileges'
  - #38149: New wrong size problem
  - #38835: stager_qry -r -n broken when waiting for tape recalls

  - In this release of CASTOR2 the cleaning daemon has been removed and replaced by a pure
    PL/SQL database job. This removes the danger of running two cleaning daemons for reasons
    of redundancy which would cause deadlocks inside the stager database.

  - Modified RFIOD to support white listing of directories/paths. By default, RFIOD will now
    only allow files to be read or written too if the path of the file is listed as a
    mountpoint in CASTOR (ie. appears in RmNode/MountPoints) or is explicitly defined in the
    RFIOD/PathWhiteList option in castor.conf. Note: this functionality can not be disabled.

  - Added black and white list tools for manipulating the access control lists of the stager.
    These new tools require the user to have GRP_ADMIN privileges or above against the stager.

  - Fixed the defaults used by the repack server in castor.conf.example.

  - The follow stager commands: stager_qry, stager_get, stager_putdone, stager_rm and
    stager_update now support a -f option. The -f option or --filelist specifies a file
    which can contain a list of CASTOR HSM file(s) to be processed.

  - Modified the way in which the stager checks to see if a user is authorized to see the
    diskserver related information when issuing a stager_qry -s. In this release of CASROR2
    the user must have ADMIN privileges from the machine executing the command to the stager
    machine in CUPV. Prior to this release the check did not take into consideration the
    machine executing the command.

  - Modified the HTML output of the DLF web interface to provide meta instructions to robots
    and web crawlers to stop them following links and indexing the DLF site.

  - Fixed SQL injection and XSS (Cross-site scripting) vulnerabilities in the DLF web
    interface.

  - Fixed a segmentation fault in rfiod caused by free'ing invalid memory.

  - Fixed the parsing of physical filenames inside rfiod to support the parsing of fully
    qualified domain names. Prior to this the logging of the NS host was not fully
    qualified.

  - Fixed segmentation fault in the request handler when network/communication errors with
    clients occurred.

  - Modified the way in which jobs are submitted into LSF by the jobManager to support the
    usage of LSF shared resources. This now allows operations teams to specify through LSF
    configuration how many jobs slots for each mover/protocol are allowed per diskserver
    or service class. Therefore, it is now possible to say for example that a diskserver
    which has 15 slots is allowed to run a maximum of 5 rfio transfers, 2 rfio3 (streaming)
    transfers, 3 xrootd transfers and 15 gridftp transfers at any one time.

  - Fixed the partitioning of statistic tables in the DLF database. Prior to this
    modification the statistics tables where not partitioned and where part of the main
    tablespaces.

  - Fixed `stager_qry -s` returning the wrong error message where there were no diskservers
    attached to a diskpool

  - Fixed a bug in the rfio code which caused updates using the rfio v2 (non streaming)
    protocol to have the wrong file size in the nameserver. This prevented the files from
    being migrated to tape.

  - Fixed incorrect recording of the errno and serrno values in the 'Job exiting' message
    of stagerJob. Prior to this fix stagerJob would report errno=10 (No child processes) and
    serrno=1016 (Operation now in progress) for all transfers even if they were successful.
  
  - Added support for enabling coredumps in rtcpclientd.

  - Fixed a bug in Disk2Disk copy scheduling which allowed a file to be replicated to a
    diskserver which already held a copy of the file. Note: this was only possible in the past
    on service classes where the max replica number was greater than one.

  - Enhanced the GC logic to allow for policy based decision making. Provided with this 
    release are 3 policies:
    - default: gcweight based on file size, but with a one day of bonus before the first 
      access, subsequent accesses after that will only get an extra 30 minutes of weight and
      setFileGcWeight is capped to 5 hours.
    - FIFO: first in first out.
    - LRU: least recently used.

  - Added support for using different types of migration policies when migrating files to
    tape. In this release 3 policies are defined:
    - defaultMigrSelPolicy: Equivalent to bestTapeCopyForStream in previous releases where
      the stager will pick the best filesystem to use based on monitoring information and
      try to keep a migrator on a given filesystem for at least 15 minutes if there are
      files to migrate.
    - drainDiskMigrSelPolicy (RAL): Same as defaultMigrSelPolicy with the exception that
      the migrator will remain on the box for at least 30 minutes if there is data to 
      migrate and new migration streams will favour diskservers where migration streams
      are already running.
    - repackMigrSelPolicy: A repack orientated policy, no filesystem pinning.

  Package changes
  ---------------
  - Moved the vdqm_admin command from the castor-vdqm1-server to the castor-hsmtools
    package.
  - Added the commands enterPriority, deletePriority, listPriority, vdqmsetpriority,
    vdqmdeletepriority, vdqmlistpriority and vdqmlistrequest commands to the castor-dbtools 
    package.
  - Added the commands stager_addprivilege, stager_removeprivilege and stager_listprivileges
    to the castor-stager-client package.
  - The castor-cleaning-server and castor-db-config packages have been removed.

  Upgrading from 2.1.7-7
  ----------------------

  Before upgrading to a 2.1.7-7 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. It is highly recommended to have an Oracle DBA on 
  standby while upgrading the stager database just in case a database restoration is required.

  Note: 
    - This upgrade is intrusive and will require the stager and repack instances to be 
      stopped.
    - It is not mandatory to upgrade the CNS or CUPV for this release. However, it is always
      recommended to keep your software up to date.

  Stager
  ------

    1. Stop all daemons on stager headnodes which have direct connections to the stager or
       DLF databases. This includes: rhserver, stager, cleaning, jobManager, rtcpclientd,
       mighunter, rechandler, rmMasterDaemon, expertd and dlfserver daemons. Note: it is not
       necessary to stop any services/daemons on the diskservers themselves.

    2. Remove any cron scripts used to restart stuck tape recalls. This logic is now part
       of database cleaning jobs.

    3. Verify that all sessions from CASTOR2 related daemons to the database are gone. This
       step is advised to make sure that modifications are no longer being made to the 
       database which may interfere or prolong the upgrade process.

    4. Ask your DBA to create a backup of the database.

    5. Upgrade the stager and DLF databases using the correct upgrade scripts from
       - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-10/dbupgrades

    6. On the STAGER database execute the following sql statement:

       UPDATE CastorConfig SET value = 'XXXXX' WHERE key = 'instance';
       COMMIT;

       Where XXXX represents the name of the Castor2 instance e.g. castorcms, castoratlas ...

    7. Update the rechandler.py python policy file for the recall handler based on the
       rechandler.py.example file in /etc/castor/. The major difference here is that an
       additional attribute is now passed into each function called 'priority'.

       Note: For this particular release the priority field should be ignored!! It is only
       meaningful if you are running VDQM2 in production.

    8. Upgrade the software on the headnodes and diskservers to 2.1.7-10. Daemons will
       automatically be restarted on upgrade.

    9. Start all the daemons which were stopped in step 1. Note: the cleaning daemon no
       longer exists!

   10. Test the instance by running the test suite available from:
       - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-10/fastTestSuite 

   11. Congratulations you have successfully upgraded to the 2.1.7-10 release of CASTOR2!!

  Repack
  ------

    1. Stop the repackserver

    2. Upgrade the repack database using the correct upgrade scripts from
       - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-10/dbupgrades

    3. Start the repackserver


-----------
- 2.1.7-7 -
-----------

  Bug Fixes
  ---------
  - #22408: Disk only file not removed from name server after a stager_rm
  - #34758: ClientGetManyFiles error: loosing responses in call back to client
  - #35443: tapecopies left orphan when nbDrives < number of tapepools
  - #35954: name server threads pools
  - #36468: rtcpd does not clean up its children 
  - #36543: StageRmRequest: 'Unable to perform request, notifying user' is missing NSFILEID
  - #36591: Diskserver to name server synchronization does not support FQDN's

  - Modified the DiskCacheEfficiency statistics generated by DLF so that the values are
    grouped by request type and service class.

  - Modified the 'Triggering tape recall' message in the stager daemon to include the Tape
    VID, Tape status and File Size of the tape and castor file involved in the recall. These
    new logging attributes are later used in the TapeRecalled statistics generated by DLF.

  - Added a ProcessingTime statistic table to DLF to provide statistical information on how
    fast the stager and request handler process requests broken down by request type.

  - Fixed a double delete in the mighunter code which caused it to crash when an oracle
    connection was dropped and recreated.

  - Optimised a query to the database used during stage_rm requests which caused full table
    scan of the CastorfFile table to be performed.

  - Changed the GC/ChunkInterval from 60 to 120 seconds.

  - Added rtcpclientd and cleaning daemon example sysconfig files.

  - Removed regular expression support (-E option) from the stager_qry command. For now the
    query service within the stager still supports it however it is disabled by default
    unless the FILEQUERY/ENABLEREGEXP option is set to YES in castor.conf

  - Fixes to the cleanLostFiles utility which was leaving the castorfile in the stager
    database by mistake.

  - The castor client now supports callbacks from multiple stagers.

  Upgrading from 2.1.7-6
  ----------------------

  Before upgrading to a 2.1.7-7 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Instructions
  ------------
  1. Upgrade the stager and dlf databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-7/upgrades

  2. On the DLF database execute the following sql statement:

       UPDATE dlf_config SET value = 'XXXXX' WHERE name = 'instance';
       COMMIT;

     Where XXXX represents the name of the Castor2 instance e.g. castorcms, castoratlas ...

  3. Upgrade the software on the headnodes and diskservers to 2.1.7-7. Daemons will
     automatically be restarted on upgrade.

  4. Upgrade the CNS to 2.1.7-7 (This step is not mandatory if you do not wish to benefit
     from #35954)


-----------
- 2.1.7-6 -
-----------

  Bug Fixes
  ---------
  - #35568: stager_qry -s without service class should respect black/white list authorization
  - #35851: mighunter memory leak in 2.1.7
  - #35960: rfcp with wrong cns host fails...with 'success'!?
  - #35977: Disk2Disk copy failures between service classes using shared diskpools
  - #36013: stager_qry -s
  - #36011: stager_qry -s: SI units
  - #36060: Client gets no reply in case of 'Invalid user'
  - #36068: stagerDaemon has (too?) many file descriptors
  - #36255: RFE: Core dumping to be enabled by default
  - #36296: reclaim gives send2nsd error

  - Fixed a bug in the LSF plugins which prevented jobs from being scheduled even though
    resources/slots were available.

  - Modified the size2gcweight function in the PL/SQL code to improve the garbage 
    collection decisions made with respect to large files.

  - Added support for checking the black and white list for `stager_qry -s` with no
    service class defined. If the user issuing the command does not have DiskPoolQuery
    rights (103) on the service class to be listed it will not be visible to them. (#35568)

  - Added support for preventing non ADMIN users from viewing diskserver and filesystem
    information in `stager_qry -s`. (#36013) To be able to view this information you will
    need to have ADMIN privileges in CUPV from the stager machine to the stager machine.

  - Fixed `stager_qry -s` with an explicit diskpool defined (-d option) not respecting the
    service class attribute.
 
  - Fixed `stager_qry -s` returning the wrong error message in cases where no diskservers
    are attached to a diskpool.

  - Added replication statistics to DLF to provide a high level summary of how many files
    are being replicated both within a service class and across service classes.

  - Improved the QueueTime statistics so that the high level summary is broken down by the
    type of request queued and the service class it belongs too.

  - Modified the GcDaemon to including the FileSize and FileAge of files been deleted by
    the stager and name server synchronizations. These fields are later used to create a
    summary of the garbage collection process broken down by type of Gc'ing being
    performed. E.g normal, stager and name server sych.

  - Added an elapsed time statistic to the CNS and CUPV daemons to provide an indication of
    how long each request has taken to process.


  Upgrading from 2.1.7-4
  ----------------------

  Before upgrading to a 2.1.7-6 instance of CASTOR2 please make sure to read the upgrade
  instructions fully before proceeding. Note: this upgrade does not require stopping of the
  instance and should be transparent to the end users.

  Instructions
  ------------
  1. Stop the stager and jobManager daemons. This is because of a schema change that is
     non backwards compatible.

  2. Upgrade the stager and dlf databases using the correct upgrade scripts from
     - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-6/upgrades

  3. Upgrade the software on the headnodes and diskservers to 2.1.7-6. Daemons will
     automatically be restarted on upgrade.

  4. Start the stager and jobManager daemons

  5. Reload the LSF plugin. `badmin reconfig`

  6. Upgrade the CNS and CUPV services to 2.1.7-6 (This step is not mandatory)


-----------
- 2.1.7-4 -
-----------

  Bug Fixes
  ---------
  - #25996: Inconsistently linked DLF records
  - #29786: Add support for CLOB in the CASTOR code generation
  - #29787: Improve overview of disk-cache use
  - #31342: Cupv GRP_ADMIN privileges for the stager are not respected
  - #34607: RFE: "STAGER SVCCLASS" directive to added to castor.conf example file
  - #35398: stager_setFileGCWeight should be protected with GRP_ADMIN Cupv rights
  - #35414: mighunter crashes in 2.1.7
  - #35494: Too many files in a directory - Cns_update_fmd_entry: UPDATE error: ORA-01438
  - #35498: Name server database schema does not respect CA_MAXNAMELEN - 
            Cns_insert_fmd_entry: INSERT error: ORA-12899
  - #35500: Transfers remain PEND'ing for full disk only service classes
  - #35520: Oracle deadlock updating filesystem/diskserver related tables
  - #35647: Syslog message too large, rtcpd crashes, drive left in UNKNOWN 
  - #35835: Stager crashes with empty filenames

  - Added support for checking the black and white list for both the source and destination
    service classes of a disk2disk replication request. Likewise when triggering a tape 
    recall verify that the user has access rights to create a file in the target service 
    class. Refer to hotfix: 2.1.7-3-1

  - Fixed the subRequestToDo procedure which was incorrectly generating uuid's every time 
    a subrequest was picked up for processing by the stager. Prior to this modification it
    was not possible to trace the history of a subrequest if it had been restarted.

  - Fixed the caching logic of the LSF master name within the rmMasterDaemon. Prior to this
    modification every metric update from a diskserver was triggering a call to LSF. As a 
    consequence of this, the processing time for updating information in the shared memory 
    increased and client connections were timing out.
  
  - Fixed the post install script of the castor-gridftp-dsi-int package which was trying 
    to change the ownership of a file that did not exist.
 
  - Fixed the totalWaitTime statistic for disk2disk copy transfers which was offset by one
    hour.

  - Fixed the update of the GcWeight of a diskcopy when opening a file for read.

  - Changed the permissions on the /var/spool/job directory from 777 to 755.

  - Changed the default value for the heartbeat timeout of diskservers in castor.conf from
    20 to 60 seconds. Prior to this change a single lost message from a diskserver to the 
    rmMasterDaemon was enough to cause the diskserver to be disabled in Castor.


  Upgrading from 2.1.7-3
  ----------------------

    Central services
    ----------------

    It is not mandatory to upgrade the central services in this release. However, if you
    wish to benefit from the following bug fixes as documented above: #31342, #35494 and
    #35498 you will need to upgrade the CNS (Castor Name Server).

      Instructions
      ------------
      1. Upgrade the castor name server database using the correct upgrade scripts from
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-4/upgrades
         - Note: the schema modification which is made in this script does not require
           any downtime.

      2. Upgrade the software on the headnodes and diskservers to 2.1.7-4. Daemons will
         automatically be restarted on upgrade.
 
    Stager
    ------

    Before upgrading to a 2.1.7-4 stager instance of CASTOR2 please make sure to read the 
    upgrade instructions fully before proceeding. Note: this upgrade does not require
    stopping of the instance and should be transparent to the end users.

      Instructions
      ------------
      1. Upgrade the stager and dlf databases using the correct upgrade scripts from
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-4/upgrades

      2. Upgrade the software on the headnodes and diskservers to 2.1.7-4. Daemons will
         automatically be restarted on upgrade.

      3. If you are using the dlfserver.sysconfig file, please regenerate it using the
         example in /etc/sysconfig/dlfserver.example. In particular the -D option has
         been removed.

      4. Change the permissions on the /var/spool/job directory to 755.


-----------
- 2.1.7-3 -
-----------

  Highlights
  ----------

  - A synchronization between the diskservers and the stager database has now been 
    introduced. By default all gcDaemon's will now verify the contents of the diskservers
    filesystems with the stager database and flag files for deletion if the files are not
    meant to be on the diskcache. Furthermore, improved synchronization logic between the
    diskserver and name server has been developed to reduce the load and number of queries
    per second to the name server on large Castor2 installations.

  - In this release of Castor2 the configuration of LSF has changed. In the past resources
    where defined in LSF to represent A) which service class a diskserver belonged to and
    B) the protocols that the diskserver supported. In 2.1.7 only protocols are defined as
    resources and service classes are defined as host groups.

  - A new protocol called 'rfio3' has been introduced. This is to support the distinction
    between those clients which open a file in streaming mode (rfio3) vs non-streaming 
    mode (rfio). This allows the number of streaming vs non-streaming resources on a 
    diskserver to be configured independently. Note: only 2.1.7 clients and above make this
    distinction, 2.1.6 clients will continue to use the rfio resource for both streaming 
    and non-streaming transfer modes.

  - At the recommendation of Platform (LSF), Castor2 now uses two plugins (schmod_shmem
    and schmod_python) when making decisions where to schedule jobs. This is done for 
    performance and scalability reasons. 

  - The scheduler logic has been enhanced to properly schedule access for disk replication
    jobs. In the previous release of Castor2, policies were only applied to the destination
    end of a transfer. Now both destination and source policies are taken into 
    consideration.

  - CTRL-C propagation between clients and the rhserver. In previous releases of Castor2
    if a client issuing a command issued a CTRL-C or a network failure occurred the 
    requested actions of the client up to that point were committed to the stager database
    and processed. Now these events are correctly detected by the rhserver and all actions
    are cancelled accordingly. Note: This is a server side change only and does not apply
    to RFIO transfers between the client and the diskserver.

  - The rmMasterDaemon has been certified to work in setups where multiple LSF masters
    exist. I.e. in a failover/redundancy deployment model.

  - The garbage collection logic has been completely written to address issues in
    scalability. As a consequence the spaceToBeFreed column in the filesystem table no 
    longer exists.

  - The logging of the stager has been improved so that the fileid and nshost attributes
    are recorded in all messages which are job centric.

  - The jobManager now has post job checking functionality in order to address operational
    problems with LSF. In particular it has been observed that LSF will sometimes fail to
    start a job on the remote execution host and just EXIT the job from the queue. When
    this happens the status of the diskcopy and subrequest within the stager database is
    left in a bad status which is never cleaned up. In response to this problem the
    jobManager now runs checks against the stager database whenever a job exits to verify
    that the status of the diskcopy and subrequest are correct. If not, an error will be
    logged and the jobManager will cleanup after the job.

  - Miscellaneous
    - All initialization scripts now use the same logic (inc. tape related daemons)
    - Improved logging output has been added to several daemons to recording statistical
      related information. E.g QueueTime for the scheduler, TotalWaitTime on jobs and
      FileAge in the GC.
    - Several bug fixes to the name server have been back ported from the DPM name 
      server to the Castor Name Server.
    - The infamous stagein stuck problem (#31616) has been fixed.
    - Several memory leaks have been fixed.


  Package changes
  ---------------
  - The nsGetPath command has moved from the castor-hsmtools package to castor-ns-client
    and renamed to nsgetpath.
  - The contents of the castor-commands package has been moved to the castor-stager-client
    and the castor-commands package has been discontinued.
  - The follow packages are no longer distributed: castor-scriptlets, castor-msg-client,
    castor-msg-server, castor-monitor-server and castor-commands.
  - The stager_setFileGCWeight command within the castor-stager-client package has been
    renamed to stager_setfilegcweight.


  Bug Fixes
  ---------
  - #1441:  nlinks in name server
  - #27327: Start time for tape requests not set leads to false accounting
  - #27610: vmgrmodifytape error message incorrect
  - #27622: nslisttape needs a fast 'total files' option
  - #27681: nsGetPath and related utilities
  - #27736: nslisttape information on a fseq
  - #29037: vmgrmodifytape anomaly concerning library change
  - #29809: rh server logging: incoming requests
  - #29942: nslisttape to return non-zero exit status if VID not found
  - #30391: daemons on castor should be stateless and redundant: rmMasterDaemon
  - #30993: RFE vmgrlistpool and vmgrlisttape output for scripts
  - #31028: 2.1.4.-9 rfiod segfaults
  - #31217: SubRequest in status WAITSUBREQ without a valid parent
  - #31247: ns client should retry if server not available
  - #31616: prepareToGet requests sometime doesn't fill tape segment information
  - #32901: scheduling slowdown
  - #32903: db schema change scripts have hard-coded table names
  - #33091: tpdaemon does not clean up if rlstape dumps
  - #33203: Stream policy should not be called when stream is busy
  - #33250: garbage collection not scaling
  - #33301: Continuing problem with castor filesize after concurrent StageUpdateRequests
  - #33418: Errors in BESTTAPECOPYFORSTREAM that explain tape timeouts
  - #33949: Failure of vmgrdeletepool for empty pool
  - #34027: PROCESSPREPAREREQUEST: exact fetch returns more than requested number of rows
  - #34159: stager and rh server memory leak
  - #34607: RFE: "STAGER SVCCLASS" directive to added to castor.conf example file
  - #34639: rfcp returns 'Host not known' from 2.1.6 stagers for files on DISABLED 
  - #34699: rmMasterDaemon hangs not fixed in 2.1.6-11
  - #34748: GC: Error="Operation now in progress"
  - #34767: mighunter logs fileid missplaced
  - #34957: rtcpcld_gettape() - Invalid argument
  - #35021: Cannot read nl tapes with Castor-2


  Available in 2.1.6-11
  ---------------------	
  - #33253: Error_Code=BAD ERROR NUMBER: 0
  - #33558: weird paths cause name server client to hang which cause stager to hang
  - #34025: 2.1.6-10: timeouts kill jobmanager
  - #31481: Problems with white/blacklists on c2atlas
  - #33091: tpdaemon does not clean up if rlstape dumps
  - #34292: Stager segmentation faults when making a DLF log
  - #20354: recreation of CASTOR file doesn't reset the filesize in the name server
  - #20887: creation of read only files in CASTOR
  - Fixed memory leak in the rmMasterDaemon.
  - Fixed the arguments being passed through to the recall policies.
  - Fixed the bad handling of disabled segments in the repack server which would cause the
    name server to become unresponsive.
  - Fixed the concurrent updates problem.
  - Fixed the scheduler plugin to allow disk2disk copying from a draining filesystem.
  - Fixed a bug in the gcDaemon which prevented the synchronisation of the diskservers 
    contents with the stager for files over 2GB.
  - Fixed the disk2DiskCopyFailed procedure so that it correctly restarts waiting 
    subrequests when a prior call to disk2DiskCopyStart was never issued.
  - Fixed the resource killing logic of the jobManager to respect disk2disk copy 
    replication requests and the detection of service classes with no filesystems in 
    production.
  - Optimization of the gc to fix scalability issues with stagers that have millions
    of diskcopies.
  - Optimised the processing of filesystems when they move from disabled to production.
  - Fixed the ownership of the gridftp log file for internal transfers
  - Changed the logging of gridftp to be more suitable


  Available in 2.1.6-10
  ---------------------
  - #33251: invalid cursor on FILESDELETEDPROC
  - #33419: ATLAS migrator errors => missing diskcopies
  - #33879: wrong FREE SPACE % and put jobs still accepted for full pool
  - Fixed the 'communication errors' between nameserver and gcDaemon when trying to 
    synchronise the diskserver contents with the nameserver.
  - Fixed segmentation fault in gcDaemon when trying to extract the fileid from a file on 
    disk which does not conform to the castor naming conventions.
  - Fixed random writing of zero sized files during replication
  - Fixed UDP notification problems between headnode daemons leading to poor response times.


  Available in 2.1.6-9
  ---------------------
  - Made the client timeout tunable so that repack can have longer timeouts
  - Fixed bug in stageForcedRm leading to not answering the client in all cases
  - Tuning for inputForMigrationgPolicy
  - Ignore temporary tables in tableShrink procedure
  - Changed sleeping time for the error svc thread pool
  - In case of failures explicitly call the disk2DiskCopyFailed procedure in order to 
    avoid accumulation of WAITDISK2DISKCOPY in case of timeouts.
  - After a disk to disk copy, make sure that the size of the replica is equal in size to
    the original file. This allows to detect transfer failures.
  - Fixed resurrectCandidates to avoid to resurrect candidates that are actually already
    attached to a stream.


  Upgrading from 2.1.6-11+
  ------------------------

   Software Upgrades
   -----------------

    Central services
    ----------------

    Although there are modifications to the central services (VMGR and CUPV) within this
    release, they are not mandatory for the correct operation and running of a 2.1.7 stager
    instance of CASTOR2. As a result the upgrade of the following packages:

      - castor-vmgr-server
      - castor-upv-server

    are not prerequisites for installing 2.1.7 stagers. However, if you wish to benefit
    from enhanced client-side functionality or minor bug fixes in these services you will
    need to upgrade. (Note: 2.1.7 CNS, VMGR and CUPV clients support backwards 
    compatibility with pre 2.1.7 services). The Castor Name Server (CNS) must be installed
    before deploying any 2.1.7 stagers.

      Instructions
      ------------
      1. For the CNS (Castor Name Server), VMGR and CUPV services upgrade the software
         to use the 2.1.7 RPMS. Note: the services may not come back automatically. This
         is a known bug which is fixed in this release.
         
      2. For all tape related daemons review their corresponding sysconfig file based on 
         the example file <dameon>.example in /etc/sysconfig. Changes have been made in
         this area to standardise the initialization scripts and sysconfig files with the
         rest of Castor2. For example OPTIONS becomes RMCDAEMON_OPTIONS for the rmcdaemon.
 
    Stager
    ------

    Before upgrading to a 2.1.7 stager instance of CASTOR2 please make sure to read the 
    upgrade instructions fully before proceeding. It is highly recommended to have an 
    Oracle DBA on standby while upgrading the stager database just in case a database
    restoration is required.
    
    Note: to upgrade to this version of CASTOR2 all jobs PEND'ing in the LSF queue will
    need to be terminated. In order to reduce the visible impact of the upgrade to the
    users in terms of failed transfers it is recommended to suspend any new user 
    activity from entering the instance several hours before the upgrade.

      Instructions
      ------------
      1. Stop all daemons on both the stager headnodes and diskservers. This includes: 
         rhserver, stager, cleaning, jobManager, rtcpclientd, mighunter, rechandler,
         rmMasterDaemon, rmNodeDaemon, expertd, dlfserver, rfiod and gcDaemon

      2. Stop and/or remove any cron scripts which maybe performing corrective actions
         against the database such as cleanup procedures. In this particular release of
         Castor2 the notion of spaceToBeFreed no longer exists so any cron scripts which 
         deal with periodically resetting this field should be permanently stopped.

      3. Verify that all sessions from CASTOR2 related daemons to the database are gone.
         This step is advised to make sure that modifications are no longer being made
         to the database.

      4. Ask your DBA to create a backup of the database (better to be safe then sorry)

      5. Upgrade the stager and dlf databases using the correct upgrade scripts from
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-3/upgrades
         - Note: If you are not running 2.1.6-11 or newer you must first upgrade the
                 stager database schema to 2.1.6-11 perform proceeding.

      6. Apply hotfixes where applicable.

      7. Stop the LSF master and clients

      8. Modify the lsb.hosts file to define the host groups known to LSF. The name of
         group must be of the form <svcclass>Group e.g. defaultGroup, recoveryGroup etc..

         Example:
           Begin HostGroup
           GROUP_NAME    GROUP_MEMBER      # Key words
           defaultGroup (diskserver1.domain.com diskserver2.domain.com ...)
           exportGroup (diskserver3.domain.com ...)
           End HostGroup

      9. Modify the lsb.modules file to define the CASTOR2 plugins. Note: they have
         changed name in this release! The only plugins required for a working CASTOR2 
         stager instance are:

         schmod_default                  ()                              ()
         schmod_shmem                    ()                              ()
         schmod_fcfs                     ()                              ()
         schmod_limit                    ()                              ()
         schmod_parallel                 ()                              ()
         schmod_python                   ()                              ()

         All other modules can be commented out to improve performance. The order in
         which the modules are listed is very important!! schmod_shmem must be after the
         default module and schmod_python must be the last.

     10. Modify the ego.shared file. Remove all resources which refer to service classes
         and add the resource:

         rfio3      Boolean ()       ()          (RFIO Transfer Protocol V3)

     11. Modify the ego.cluster.* file. Remove all references to resources which are
         service classes and give all diskservers the rfio3 resource.

     12. Delete the shared memory used by the rmMasterDaemon and scheduler plugins using
         the command `ipcrm -M 0x946`. The contents are no longer compatible with this
         release.

     13. Upgrade the software on the headnodes and diskservers to 2.1.7. Note: the
         castor-scriplets, castor-rtcopy-mighunter, castor-msg-daemon, castor-msg-client 
         and castor-commands RPMS are no longer provided. The old contents of the
         castor-commands RPM can now be found in castor-stager-clients.

     14. Update the castor.conf configuration file on all machines based on the
         castor.conf.example file in /etc/castor/

     15. Update the stream.py python policy file for the mighunter based on the
         stream.py.example file in /etc/castor/. The major difference here is that the
         status argument provided to each function has been removed.

     16. The information made available to the schedulers python policy file has changed.
         If you are using the defaults original provided:
           mv /etc/castor/policies/scheduler.py.example /etc/castor/policies/scheduler.py
         If not, please review the file /etc/castor/policies/scheduler.py.example and
         merge the changes into your existing version.

     17. Restart the service

         - Start the dlfserver
         - Start the rmMasterDaemon, followed by the LSF master
         - Start the rmNodeDaemon's and LSF clients.

         To verify that the LSF upgrade has been successful please run `badmin reconfig`
         and `lsadmin reconfig` if you are presented with any warnings please correct the
         LSF configuration and retry. To view the list of host groups defined in LSF use 
         the command `bmgroup`

         - Start all other daemons: rhserver (private), stager, cleaning, jobManager, 
         rtcpclientd, mighunter, rechandler, expertd, rfiod and gcDaemon.

         Note: if you have a concept of private and public rhserver's you should start the
         private one only so that you can test/validate the installation without user 
         interference.

     18. Test the instance by running the test suite available from:
         - http://cern.ch/castor/DIST/CERN/savannah/CASTOR.pkg/2.1.7-*/2.1.7-3/fastTest

     19. Start the public rhserver (if applicable)

     20. Congratulations you have successfully upgraded to the 2.1.7 release of CASTOR2!!


  Hotfixes for 2.1.7-3
  ---------------------
  - 2.1.7-3-1: Enforce black and white list checks for disk copy replication requests and 
    file recalls. For stagers which are using black and white lists please make sure that
    the access rights are setup correctly before applying this hotfix.

    If you wish users to be able to replicate between service classes they must have 
    StageDiskCopyReplicaRequest (133) rights on the source service class and StagePutRequest
    (40) rights on the destination service class. Likewise, if you wish to allow file 
    recalls from tape, the user must have StagePutRequest (40) rights on destination/target
    service class.

    By default when this hotfix is applied all users in all service classes will be granted
    access rights to read files from the source service class involved in disk2disk copy
    replication request. CAUTION: they will not by default be granted put rights!!!

    For stagers which have never used white and black lists. i.e. no entries exist in the
    BlackList table and only the default '*', NULL, NULL, NULL entry is found in the 
    WhiteList table no actions are required.

  - 2.1.7-3-2: Modification in attachTapeCopiesToStream procedure to have always a 
    stateless behaviour. 

